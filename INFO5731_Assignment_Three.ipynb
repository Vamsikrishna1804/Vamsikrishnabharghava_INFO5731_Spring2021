{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsikrishna1804/Vamsikrishnabharghava_INFO5731_Spring2021/blob/main/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016a4e89-ab84-4f6f-8aef-81571aa637d0"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import nltk\n",
        "import itertools\n",
        "from textblob import TextBlob\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('punkt')\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "df1=pd.read_csv('AbstractData.csv')\n",
        "print(df1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                                             Abstract\n",
            "0                        Abstract not found       ...\n",
            "1                         describe a method for st...\n",
            "2                        Scaling conditional rando...\n",
            "3                        The paper addresses the i...\n",
            "4                        In most natural language ...\n",
            "..                                                ...\n",
            "95                       This paper presents a wor...\n",
            "96                       Abstract—Natural Language...\n",
            "97                       ABSTRACT: After twenty ye...\n",
            "98                       Text statistics are frequ...\n",
            "99                       We summarize our experien...\n",
            "\n",
            "[100 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7eMR8FLj7HM",
        "outputId": "f39abdca-8d4e-4526-9a9f-f9439fb3a31f"
      },
      "source": [
        "df1=pd.read_csv('AbstractData.csv')\r\n",
        "df1['Special characters and Punctutation Removal'] = df1['Abstract'].str.replace('[,.~`\\?!@#$%^&\"*-/:;...()]','')\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Removing Numbers\r\n",
        "df1['Removal of numbers'] = df1['Abstract'].str.replace('\\d+', '')\r\n",
        "print(df1)\r\n",
        "\r\n",
        "# Remove stopwords by using the stopwords list.\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop=stopwords.words('english')\r\n",
        "df1['Removal of Stop words']=df1['Abstract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Lowercase all texts\r\n",
        "df1['Lower_case'] = df1['Abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "df1.to_csv(\"cleandata_list.csv\")\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Stemming.\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "st=PorterStemmer()\r\n",
        "df1['Stemming']=df1['Abstract'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Lemmatization.\r\n",
        "from textblob import Word\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "df1['Lemmitization']=df1['Abstract'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "print(df1)\r\n",
        "#Write a python program to clean the text data you collected above and save the data in a new column in the csv file\r\n",
        "cd=df1.to_csv('cleandata_list.csv')\r\n",
        "print(cd)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             Abstract        Special characters and Punctutation Removal\n",
            "0                        Abstract not found       ...                       Abstract not found       ...\n",
            "1                         describe a method for st...                        describe a method for st...\n",
            "2                        Scaling conditional rando...                       Scaling conditional rando...\n",
            "3                        The paper addresses the i...                       The paper addresses the i...\n",
            "4                        In most natural language ...                       In most natural language ...\n",
            "..                                                ...                                                ...\n",
            "95                       This paper presents a wor...                       This paper presents a wor...\n",
            "96                       Abstract—Natural Language...                       Abstract—Natural Language...\n",
            "97                       ABSTRACT: After twenty ye...                       ABSTRACT After twenty yea...\n",
            "98                       Text statistics are frequ...                       Text statistics are frequ...\n",
            "99                       We summarize our experien...                       We summarize our experien...\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "                                             Abstract  ...                                 Removal of numbers\n",
            "0                        Abstract not found       ...  ...                       Abstract not found       ...\n",
            "1                         describe a method for st...  ...                        describe a method for st...\n",
            "2                        Scaling conditional rando...  ...                       Scaling conditional rando...\n",
            "3                        The paper addresses the i...  ...                       The paper addresses the i...\n",
            "4                        In most natural language ...  ...                       In most natural language ...\n",
            "..                                                ...  ...                                                ...\n",
            "95                       This paper presents a wor...  ...                       This paper presents a wor...\n",
            "96                       Abstract—Natural Language...  ...                       Abstract—Natural Language...\n",
            "97                       ABSTRACT: After twenty ye...  ...                       ABSTRACT: After twenty ye...\n",
            "98                       Text statistics are frequ...  ...                       Text statistics are frequ...\n",
            "99                       We summarize our experien...  ...                       We summarize our experien...\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "                                             Abstract  ...                              Removal of Stop words\n",
            "0                        Abstract not found       ...  ...                                     Abstract found\n",
            "1                         describe a method for st...  ...  describe method statistical modeling based max...\n",
            "2                        Scaling conditional rando...  ...  Scaling conditional random fields natural lang...\n",
            "3                        The paper addresses the i...  ...  The paper addresses issue cooperation linguist...\n",
            "4                        In most natural language ...  ...  In natural language processing applications, D...\n",
            "..                                                ...  ...                                                ...\n",
            "95                       This paper presents a wor...  ...  This paper presents workbench built Priberam I...\n",
            "96                       Abstract—Natural Language...  ...  Abstract—Natural Language Processing (NLP) eff...\n",
            "97                       ABSTRACT: After twenty ye...  ...  ABSTRACT: After twenty years disfavor, technol...\n",
            "98                       Text statistics are frequ...  ...  Text statistics frequently used stylometry cry...\n",
            "99                       We summarize our experien...  ...  We summarize experience using FrameNet two rat...\n",
            "\n",
            "[100 rows x 4 columns]\n",
            "                                             Abstract  ...                                         Lower_case\n",
            "0                        Abstract not found       ...  ...                                 abstract not found\n",
            "1                         describe a method for st...  ...  describe a method for statistical modeling bas...\n",
            "2                        Scaling conditional rando...  ...  scaling conditional random fields for natural ...\n",
            "3                        The paper addresses the i...  ...  the paper addresses the issue of cooperation b...\n",
            "4                        In most natural language ...  ...  in most natural language processing applicatio...\n",
            "..                                                ...  ...                                                ...\n",
            "95                       This paper presents a wor...  ...  this paper presents a workbench built by pribe...\n",
            "96                       Abstract—Natural Language...  ...  abstract—natural language processing (nlp) is ...\n",
            "97                       ABSTRACT: After twenty ye...  ...  abstract: after twenty years of disfavor, a te...\n",
            "98                       Text statistics are frequ...  ...  text statistics are frequently used in stylome...\n",
            "99                       We summarize our experien...  ...  we summarize our experience using framenet in ...\n",
            "\n",
            "[100 rows x 5 columns]\n",
            "                                             Abstract  ...                                           Stemming\n",
            "0                        Abstract not found       ...  ...                                 abstract not found\n",
            "1                         describe a method for st...  ...  describ a method for statist model base on max...\n",
            "2                        Scaling conditional rando...  ...  scale condit random field for natur languag pr...\n",
            "3                        The paper addresses the i...  ...  the paper address the issu of cooper between l...\n",
            "4                        In most natural language ...  ...  In most natur languag process applications, de...\n",
            "..                                                ...  ...                                                ...\n",
            "95                       This paper presents a wor...  ...                                                NaN\n",
            "96                       Abstract—Natural Language...  ...                                                NaN\n",
            "97                       ABSTRACT: After twenty ye...  ...                                                NaN\n",
            "98                       Text statistics are frequ...  ...                                                NaN\n",
            "99                       We summarize our experien...  ...                                                NaN\n",
            "\n",
            "[100 rows x 6 columns]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "                                             Abstract  ...                                      Lemmitization\n",
            "0                        Abstract not found       ...  ...                                 Abstract not found\n",
            "1                         describe a method for st...  ...  describe a method for statistical modeling bas...\n",
            "2                        Scaling conditional rando...  ...  Scaling conditional random field for natural l...\n",
            "3                        The paper addresses the i...  ...  The paper address the issue of cooperation bet...\n",
            "4                        In most natural language ...  ...  In most natural language processing applicatio...\n",
            "..                                                ...  ...                                                ...\n",
            "95                       This paper presents a wor...  ...  This paper present a workbench built by Priber...\n",
            "96                       Abstract—Natural Language...  ...  Abstract—Natural Language Processing (NLP) is ...\n",
            "97                       ABSTRACT: After twenty ye...  ...  ABSTRACT: After twenty year of disfavor, a tec...\n",
            "98                       Text statistics are frequ...  ...  Text statistic are frequently used in stylomet...\n",
            "99                       We summarize our experien...  ...  We summarize our experience using FrameNet in ...\n",
            "\n",
            "[100 rows x 7 columns]\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzvB-EkxkAjh",
        "outputId": "10c85439-5aec-40b2-80af-91060f120ec9"
      },
      "source": [
        "words = []\r\n",
        "for i in df1['Lemmitization']:\r\n",
        "  words.append(nltk.tokenize.word_tokenize(i))\r\n",
        "data = [x for x in words if x != []]\r\n",
        "print(data)\r\n",
        "final_list=[]\r\n",
        "for item in data:\r\n",
        "  final_list.extend(item)\r\n",
        "print(final_list)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Abstract', 'not', 'found'], ['describe', 'a', 'method', 'for', 'statistical', 'modeling', 'based', 'on', 'maximum', 'entropy', '.', 'We', 'present', 'a', 'maximum-likelihood', 'approach', 'for', 'automatically', 'constructing', 'maximum', 'entropy', 'model', 'and', 'describe', 'how', 'to', 'implement', 'this', 'approach', 'efficiently', ',', 'using', 'a', 'example', 'several', 'problem', 'in', 'natural', 'language', 'processing', '.'], ['Scaling', 'conditional', 'random', 'field', 'for', 'natural', 'language', 'processing', 'Terms', 'and', 'Conditions', ':', 'Terms', 'and', 'Conditions', ':', 'Copyright', 'in', 'work', 'deposited', 'in', 'Minerva', 'Access', 'is', 'retained', 'by', 'the'], ['The', 'paper', 'address', 'the', 'issue', 'of', 'cooperation', 'between', 'linguistics', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'in', 'general', ',', 'and', 'between', 'linguistics', 'and', 'machine', 'translation', '(', 'MT', ')', ',', 'in', 'particular', '.', 'It', 'focus', 'on', 'just', 'one', 'direction', 'of', 'such', 'cooperation', ',', 'namely', 'application', 'of', 'linguistics', 'to', 'NLP', ',', 'virtually'], ['In', 'most', 'natural', 'language', 'processing', 'applications', ',', 'Description', 'Logics', 'have', 'been', 'used', 'to', 'encode', 'in', 'a', 'knowledge', 'base', 'some', 'syntactic', ',', 'semantic', ',', 'and', 'pragmatic', 'element', 'needed', 'to', 'drive', 'the', 'semantic', 'interpretation', 'and', 'the', 'natural', 'language', 'generation', 'processes', '.', 'More', 'recently', ',', 'Description', 'Logics', 'have', 'been'], ['We', 'propose', 'a', 'unified', 'neural', 'network', 'architecture', 'and', 'learning', 'algorithm', 'that', 'can', 'be', 'applied', 'to', 'various', 'natural', 'language', 'processing', 'task', 'including', 'part-of-speech', 'tagging', ',', 'chunking', ',', 'named', 'entity', 'recognition', ',', 'and', 'semantic', 'role', 'labeling', '.', 'This', 'versatility', 'is', 'achieved', 'by', 'trying', 'to', 'avoid', 'task'], ['Natural', 'Language', 'Processing', 'The', 'subject', 'of', 'Natural', 'Language', 'Processing', 'can', 'be', 'considered', 'in', 'both', 'broad', 'and', 'narrow', 'senses', '.', 'In', 'the', 'broad', 'sense', ',', 'it', 'cover', 'processing', 'issue', 'at', 'all', 'level', 'of', 'natural', 'language', 'understanding', ',', 'including', 'speech', 'recognition', ',', 'syntactic', 'and', 'semantic', 'analysis', 'of', 'sentence'], ['Robots', 'that', 'interact', 'with', 'human', 'face-to-face', 'using', 'natural', 'language', 'need', 'to', 'be', 'responsive', 'to', 'the', 'way', 'human', 'use', 'language', 'in', 'those', 'situations', '.', 'We', 'propose', 'a', 'psychologicallyinspired', 'natural', 'language', 'processing', 'system', 'for', 'robot', 'which', 'performs', 'incremental', 'semantic', 'interpretation', 'of', 'spoken', 'utterance'], ['Natural', 'language', 'are', 'language', 'spoken', 'by', 'humans', '.', 'Currently', 'we', 'are', 'not', 'yet', 'at', 'the', 'point', 'where', 'these', 'language', 'in', 'all', 'of', 'their', 'unprocessed', 'form', 'can', 'be', 'understood', 'by', 'computers', '.', 'Natural', 'language', 'processing', 'is', 'the', 'collection', 'of', 'technique', 'employed', 'to', 'try', 'and', 'accomplish', 'that', 'goal', '.', 'The', 'field', 'of', 'natural'], ['ABSTRACT', ':', 'Ambiguity', 'can', 'be', 'referred', 'a', 'the', 'ability', 'of', 'having', 'more', 'than', 'one', 'meaning', 'or', 'being', 'understood', 'in', 'more', 'than', 'one', 'way', '.', 'Natural', 'language', 'are', 'ambiguous', ',', 'so', 'computer', 'are', 'not', 'able', 'to', 'understand', 'language', 'the', 'way', 'people', 'do', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'concerned', 'with', 'the', 'development'], ['Introduction', 'Statistical', 'natural', 'language', 'processing', '(', 'SNLP', ')', 'is', 'a', 'field', 'lying', 'in', 'the', 'intersection', 'of', 'natural', 'language', 'processing', 'and', 'machine', 'learning', '.', 'SNLP', 'di', '#', 'ers', 'from', 'traditional', 'natural', 'language', 'processing', 'in', 'that', 'instead', 'of', 'having', 'a', 'linguist', 'manually', 'construct', 'some', 'model', 'of', 'a', 'given', 'linguistic'], ['text', 'directly', '(', 'rather', 'than', 'e.g', '.', 'title', 'and', 'abstracts', ')', ',', 'and', 'suggests', 'appropriate', 'approach', 'to', 'doing', 'this', ',', 'with', 'a', 'focus', 'on', 'the', 'role', 'of', 'natural', 'language', 'processing', '.', 'The', 'paper', 'also', 'comment', 'on', 'possible', 'connection', 'with', 'data', 'and', 'knowledge', 'retrieval', ',', 'and', 'concludes', 'by', 'emphasizing', 'the', 'importance', 'of', 'rigorous'], ['ABSTRACT', ':', 'Language', 'is', 'way', 'of', 'communicating', 'your', 'word', 'Language', 'help', 'in', 'understanding', 'the', 'world', ',', 'we', 'get', 'a', 'better', 'insight', 'of', 'the', 'world', '.', 'Language', 'help', 'speaker', 'to', 'be', 'a', 'vague', 'or', 'a', 'precise', 'a', 'they', 'like', '.', 'NLP', 'Stands', 'for', 'natural', 'language', 'processing..', 'Natural', 'language', 'are', 'those', 'language', 'that', 'are', 'spoken'], ['We', 'report', 'experiment', 'on', 'the', 'use', 'of', 'standard', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tool', 'for', 'the', 'analysis', 'of', 'music', 'lyrics', '.', 'A', 'significant', 'amount', 'of', 'music', 'audio', 'ha', 'lyrics', '.', 'Lyrics', 'encode', 'an', 'important', 'part', 'of', 'the', 'semantics', 'of', 'a', 'song', ',', 'therefore', 'their', 'analysis', 'complement', 'that', 'of', 'acoustic', 'and', 'cultural'], ['this', 'paper', ',', 'we', 'will', 'describe', 'a', 'simple', 'rule-based', 'approach', 'to', 'automated', 'learning', 'of', 'linguistic', 'knowledge', '.', 'This', 'approach', 'ha', 'been', 'shown', 'for', 'a', 'number', 'of', 'task', 'to', 'capture', 'information', 'in', 'a', 'clearer', 'and', 'more', 'direct', 'fashion', 'without', 'a', 'compromise', 'in', 'performance', '.', 'We', 'present', 'a', 'detailed', 'case', 'study', 'of', 'this', 'learning', 'method', 'applied', 'to', 'part', 'of', 'speech', 'tagging'], ['This', 'paper', 'focus', 'on', 'connectionist', 'model', 'in', 'natural', 'language', 'processing', '.', 'We', 'briefly', 'present', 'and', 'discus', 'several', 'aspect', 'of', 'high', 'level', 'task', 'which', 'recently', 'have', 'been', 'approached', 'with', 'connectionism', ',', 'either', 'with', 'localist', 'or', 'parallel', 'distributed', 'processing', 'models', '.', 'Several', 'interesting', 'architecture'], ['process', 'of', 'language', 'understanding', '.', 'This', 'is', 'a', 'new', 'approach', 'in', 'natural', 'language', 'processing', 'based', 'on', 'the', 'deterministic', 'chaotic', 'behavior', 'of', 'dynamical', 'systems', '.', '1'], ['this', 'paper', '(', 'see', '[', 'Schank', '86', ']', 'for', 'a', 'theoretical', 'discussion', 'and', '[', 'Kass', '86', ']', 'and', '[', 'Leake', 'and', 'Owens', '86', ']', 'for', 'brief', 'discussion', 'of', 'a', 'program', 'built', 'around', 'these', '.principles', ')', ';', 'the', 'goal', 'here', 'is', 'simply', 'to', 'point', 'out', 'how', 'our', 'interest', 'in', 'natural', 'language', 'processing', 'ha', 'led', 'u', 'naturally', ',', 'and', 'indeed', 'inevitably'], ['Objectives', 'To', 'provide', 'an', 'overview', 'and', 'tutorial', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'modern', 'NLP-system', 'design', '.', 'Target', 'audience', 'This', 'tutorial', 'target', 'the', 'medical', 'informatics', 'generalist', 'who', 'ha', 'limited', 'acquaintance', 'with', 'the', 'principle', 'behind', 'NLP', 'and/or', 'limited', 'knowledge', 'of', 'the', 'current', 'state'], ['This', 'paper', 'briefly', 'describes', 'the', 'current', 'implementation', 'status', 'of', 'an', 'intelligent', 'information', 'retrieval', 'system', ',', 'MARIE', ',', 'that', 'employ', 'natural', 'language', 'processing', 'techniques', '.', 'Descriptive', 'caption', 'are', 'used', 'to', 'iden-', 'tify', 'photographic', 'image', 'concerning', 'various', 'military', 'projects', '.', 'The', 'caption', 'are', 'parsed'], ['based', 'and', 'literature', 'resources', '.', 'We', 'describe', 'here', 'a', 'system', 'for', 'agent', 'directed', 'natural', 'language', 'processing', 'to', 'extract', 'information', 'from', 'journal', 'articles', '.', 'An', 'interface', 'wa', 'developed', 'to', 'permit', 'curation', 'of', 'the', 'NLP', 'result', 'and', 'deposition', 'of', 'accepted', 'result', 'into', 'a', 'knowledge', 'base', '.', 'Motivation', ':', 'The', 'advent', 'of', 'high'], ['to', 'evaluation', 'in', 'speech', 'processing', '.', 'Part', '2', 'survey', 'significant', 'evaluation', 'work', 'done', 'so', 'far', ',', 'for', 'instance', 'in', 'machine', 'translation', ',', 'and', 'discus', 'the', 'particular', 'problem', 'of', 'generic', 'system', 'evaluation', '.', 'The', 'conclusion', 'is', 'that', 'evaluation', 'strategy', 'and', 'technique', 'for', 'NLP', 'need', 'much', 'more', 'development', ',', 'in', 'particular'], ['similar', 'to', 'the', 'way', 'human', 'intuitively', 'do', 'in', 'order', 'to', 'eliminate', 'noisy', 'content', '.', 'In', 'this', 'paper', ',', 'we', 'describe', 'a', 'combination', 'of', 'HTML', 'DOM', 'analysis', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'technique', 'for', 'automated', 'extraction', 'of', 'main', 'article', 'with', 'associated', 'image', 'from', 'web', 'pages', '.'], ['Abstract', '--', 'Natural', 'Language', 'Processing', 'is', 'a', 'theoretically', 'motivated', 'range', 'of', 'computational', 'technique', 'for', 'analysing', 'and', 'representing', 'naturally', 'occurring', 'text', 'at', 'one', 'or', 'more', 'level', 'of', 'linguistic', 'analysis', 'for', 'the', 'purpose', 'of', 'achieving', 'human-like', 'language', 'processing', 'for', 'a', 'range', 'of', 'task'], ['This', 'paper', 'review', 'the', 'process', 'involved', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'It', 'then', 'demonstrates', 'the', 'various', 'kind', 'of', 'choice', 'that', 'need', 'be', 'taken', 'during', 'the', 'execution', 'of', 'the', 'word', 'morphology', ',', 'the', 'syntactic', 'text', 'analysis', ',', 'or', 'text', 'generation', 'components', '.', 'It', 'compare', 'the', 'time', 'complexity'], ['This', 'article', 'focus', 'on', 'the', 'derivation', 'of', 'large', 'lexicon', 'for', 'natural', 'language', 'processing', '.', 'We', 'describe', 'the', 'development', 'of', 'a', 'dictionary', 'support', 'environment', 'linking', 'a', 'restructured', 'version', 'of', 'the', 'Longman', 'Dictionary', 'of', 'Contemporary', 'English', 'to', 'natural', 'language', 'processing', 'systems', '.', 'The', 'process'], ['We', 'introduce', 'a', 'method', 'for', 'analyzing', 'the', 'complexity', 'of', 'natural', 'language', 'processing', 'tasks', ',', 'and', 'for', 'predicting', 'the', 'difficulty', 'new', 'NLP', 'tasks', '.', 'Our', 'complexity', 'measure', 'are', 'derived', 'from', 'the', 'Kolmogorov', 'complexity', 'of', 'a', 'class', 'of', 'automaton', '—', 'meaning', 'automata', ',', 'whose', 'purpose', 'is', 'to', 'extract', 'relevant', 'piece'], [',', 'sounds', ',', 'text', 'and', 'motion', '.', 'The', 'technique', 'developed', 'from', 'deep', 'learning', 'research', 'have', 'already', 'been', 'impacting', 'the', 'research', 'of', 'natural', 'language', 'process', '.', 'This', 'paper', 'review', 'the', 'recent', 'research', 'on', 'deep', 'learning', ',', 'it', 'application', 'and', 'recent', 'development', 'in', 'natural', 'language', 'processing', '.', '1'], ['This', 'is', 'an', 'author-produced', 'version', 'of', 'a', 'paper', 'published', 'in', 'The'], ['Abstract—Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'the', 'application', 'of', 'automated', 'parsing', 'and', 'machine', 'learning', 'technique', 'to', 'analyze', 'standard', 'text', '.', 'Applications', 'of', 'NLP', 'to', 'requirement', 'engineering', 'include', 'extraction', 'of', 'ontology', 'from', 'a', 'requirement', 'specification', ',', 'and', 'use', 'of', 'NLP', 'to', 'verify', 'the', 'consistency'], ['statistical', 'baseline', 'including', ':', 'the', 'forgiving', 'nature', 'but', 'broad', 'coverage', 'of', 'the', 'typical', 'retrieval', 'task', ';', 'the', 'lack', 'of', 'good', 'weighting', 'scheme', 'for', 'compound', 'index', 'terms', ';', 'and', 'the', 'implicit', 'linguistic', 'processing', 'inherent', 'in', 'the', 'statistical', 'methods', '.', 'Natural', 'language', 'processing', 'technique', 'may', 'be', 'more', 'important'], ['Work', 'in', 'computational', 'linguistics', 'began', 'very', 'soon', 'after', 'the', 'development', 'of', 'the', 'first', 'computer', '(', 'Booth', ',', 'Brandwood', 'and', 'Cleave', '1958', ')', ',', 'yet', 'in', 'the', 'intervening', 'four', 'decade', 'there', 'ha', 'been', 'a', 'pervasive', 'feeling', 'that', 'progress', 'in', 'computer', 'understanding', 'of', 'natural', 'language', 'ha', 'not', 'been', 'commensurate'], ['the', 'voice', 'recognition', 'for', 'a', 'natural', 'language', '(', 'Tamil', ')', 'by', 'combining', 'the', 'digital', 'and', 'mathematical', 'knowledge', 'using', 'MFCC', 'and', 'DTW', 'to', 'extract', 'and', 'match', 'the', 'feature', 'to', 'improve', 'the', 'accuracy', 'for', 'better', 'performance', '.'], ['Abstract', ':', 'Testing', 'against', 'natural', 'language', 'requirement', 'is', 'the', 'standard', 'approach', 'for', 'system', 'and', 'acceptance', 'testing', '.', 'This', 'test', 'is', 'often', 'performed', 'by', 'an', 'independent', 'test', 'organization', 'unfamiliar', 'with', 'the', 'application', 'area', '.', 'The', 'only', 'thing', 'the', 'tester', 'have', 'to', 'go', 'by', 'are', 'the', 'written', 'requirements', '.', 'So'], ['Abstract', 'not', 'found'], ['conversational', 'partners', '.', 'But', 'it', 'also', 'provides', 'u', 'with', 'information', 'about', 'being', 'creative', ',', 'making', 'associations', ',', 'storytelling', 'and', 'language', 'use', '.', 'Many', 'more', 'subtlety', 'in', 'face-to-face', 'and', 'multiparty', 'interaction', 'can', 'be', 'added', ',', 'such', 'a', 'using', 'humor', 'to', 'persuade', 'and', 'dominate', ',', 'to', 'soften', 'or', 'avoid', 'a', 'face', 'threatening', 'act'], ['Abstract', 'not', 'found'], ['In', 'recent', 'years', ',', 'machine', 'learning', '(', 'ML', ')', 'ha', 'been', 'used', 'more', 'and', 'more', 'to', 'solve', 'complex', 'task', 'in', 'different', 'disciplines', ',', 'ranging', 'from', 'Data', 'Mining', 'to', 'Information'], ['We', 'argue', 'that', 'manual', 'and', 'automatic', 'thesaurus', 'are', 'alternative', 'resource', 'for', 'the', 'same', 'NLP', 'tasks', '.', 'This', 'involves', 'the', 'radical', 'step', 'of', 'interpreting', 'manual', 'thesaurus', 'a', 'classification', 'of', 'word', 'rather', 'than', 'word', 'senses', ':', 'the', 'case', 'for', 'this', 'is', 'made', '.', 'The', 'range', 'of', 'role', 'for', 'thesaurus', 'within', 'NLP', 'is', 'briefly', 'presented', 'and', 'the', 'WASPS', 'thesaurus', 'is', 'introduced', '.', 'Thesaurus', 'evaluation', 'is', 'now', 'becoming', 'urgent', '.', 'A', 'range', 'of', 'evaluation', 'strategies', ',', 'all', 'embedded', 'within', 'NLP', 'tasks', ',', 'is', 'proposed', '.'], ['Introduction', 'Patterns', 'in', 'music', 'have', 'been', 'the', 'object', 'of', 'intensive', 'study', 'in', 'the', 'past', 'years', '.', '\\\\One', 'of', 'the', 'purpose', 'of', 'analyzing', 'musical', 'structure', 'and', 'form', 'is', 'to', 'discover', 'the', 'pattern', 'that', 'are', 'explicit', 'or', 'implicit', 'in', 'musical', 'works', \"''\", 'Simon', '[', '13', ']', '.', 'Patterns', 'comprise', 'periodicity', ',', 'make', 'use', 'of', 'alphabets', ',', 'can', 'be', 'compound', '(', 'made', 'up', 'of', 'subpatterns', ')', 'and', 'posse', 'phrase', 'structure', 'with', 'various', 'form', 'of', 'punctuation', '.', 'Traditionally', ',', 'composer', 'have', 'employed', 'pattern', 'propagation', 'intuitively', ',', 'but', 'algorithmic', 'composition', 'technique', 'allow', 'the', 'pattern', 'propagation', 'to', 'be', 'formalized', ',', 'albeit', 'a', 'high', 'level', '.', 'During', 'composition', ',', 'all', 'the', 'musical', 'pattern', 'evolve', 'according', 'to', 'the', 'rule', 'and', 'constraint', 'specied', 'at', 'the', 'design', 'stage', '.', 'In', 'jazz', 'improvisation', ',', 'the', 'musician', 'invents', 'a', 'solo', 'guided', 'by', 'a', 'progression', 'of', 'chord', '(', 'the', 'changes', ')', '.', 'One', 'approach', '[', '1', ']', 'to', 'learn', 'improvising', 'is', 'to', 'memorize', 'pattern', '(', 'short', 'chunk', 'of', 'music', ')', 'that', 't', 'sub-progressions', ',', 'and', 'to', 'concatenate', 'them', 'to', 'form', 'a', 'whole', 'solo', 'that', 't', 'a', 'whole', 'progression', '.', 'One'], ['Abstract', 'Many', 'information', 'retrieval', '(', 'IR', ')', 'system', 'retrieve', 'relevant', 'document', 'based', 'on', 'exact', 'matching', 'of', 'keywords', 'between', 'a', 'query', 'and', 'documents', '.', 'This', 'method', 'degrades', 'precision', 'rate', '.', 'In', 'order', 'to', 'solve', 'the', 'problem', ',', 'we', 'collected', 'semantically', 'related', 'word', 'and', 'assigned', 'semantic', 'relationship', 'used', 'in', 'general', 'thesaurus', 'and', 'a', 'special', 'relationship', 'called', 'keyfact', 'term', '(', 'FT', ')', 'manually', '.', 'In', 'addition', 'to', 'the', 'semantic', 'knowledge', ',', 'we', 'automatically', 'constructed', 'statistic', 'knowledge', 'based', 'on', 'the', 'concept', 'of', 'mutual', 'information', '.', 'Keyfact', 'is', 'an', 'extended', 'concept', 'of', 'keyword', 'represented', 'by', 'noun', 'and', 'compound', 'noun', '.', 'Keyfact', 'can', 'be', 'a', 'verb', 'and', 'an', 'adjective', 'including', 'subject', 'or', 'object', 'term', '.', 'We', 'first', 'retrieved', 'relevant', 'document', 'with', 'original', 'query', 'using', 'tf', '*', 'idf', 'weighting', 'formula', 'and', 'then', 'an', 'expanded', 'query', 'including', 'keyfacts', 'is', 'used', 'in', 'both', 'second', 'document', 'ranking', 'and', 'word', 'sense', 'disambiguating', '.', 'So', 'we', 'made', 'an', 'improvement', 'in', 'precision', 'rate', 'using', 'keyfact', 'network', '.', '1'], ['this', 'paper', 'we', 'argue', 'that', 'questionanswering', '(', 'QA', ')', 'over', 'technical', 'domain', 'is', 'distinctly', 'different', 'from', 'TREC-based', 'QA', 'or', 'Web-based', 'QA', 'and', 'it', 'can', 'not', 'benefit', 'lom', 'data-intensive', 'approach'], ['Universit', '&', 'quot', ';', 'at', 'de', 'Saarlandes'], ['Proceedings', 'of', 'the', 'Workshop', 'on'], ['uni-hamburg.de'], ['Abstract', 'not', 'found'], ['Abstract', 'not', 'found'], ['SRI', 'ha', 'developed', 'a', 'new', 'architecture', 'for', 'integrating', 'speech', 'and', 'natural-language', 'processing', 'that', 'applies', 'linguistic', 'constraint', 'during', 'recognition', 'by', 'incrementally', 'expanding', 'the', 'state-transition', 'network', 'embodied', 'in', 'a', 'unification', 'grammar', '.', 'We', 'compare', 'this', 'dynamic-gralnlnar-network', '(', 'DGN', ')', 'approach'], ['This', 'chapter', 'considers', 'the', 'revolution', 'that', 'ha', 'taken', 'place', 'in', 'natural', 'language', 'processing', 'research', 'over', 'the', 'last', 'five', 'years', '.', 'It', 'begin', 'by', 'providing', 'a', 'brief', 'guide', 'to', 'the', 'structure', 'of', 'the', 'field', 'and', 'then', 'present', 'a', 'caricature', 'of', 'two', 'competing', 'paradigm', 'of', '1980s', 'NLP', 'research', 'and', 'indicates', 'the', 'reason'], ['visual', 'development', 'environment', 'to', 'support', 'the', 'visual', 'assembly', ',', 'execution', 'and', 'analysis', 'of', 'modular', 'natural', 'language', 'processing', 'systems', '.', 'The', 'visual', 'model', 'is', 'an', 'executable', 'data', 'flow', 'program', 'graph', ',', 'automatically', 'synthesised', 'from', 'data', 'dependency', 'declaration', 'of', 'language', 'processing', 'modules', '.', 'The', 'graph'], ['In', 'this', 'Chapter', 'the', 'basic', 'us', 'of', 'Description', 'Logics', 'for', 'Natural', 'Language', 'Processing', 'will', 'be', 'analysed', ',', 'together', 'with', 'a', 'little', 'bit', 'of', 'history', ',', 'and', 'the', 'role', 'of', 'Description', 'Logics', 'in', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'computational', 'linguistics', 'will', 'be', 'pointed', 'out', '.', '18.1', 'Introduction', 'Since', 'the', 'early', 'day'], ['We', 'applied', 'a', 'structure', 'learning', 'model', ',', 'Max-Margin', 'Structure', '(', 'MMS', ')', ',', 'to', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', ',', 'where', 'the', 'aim', 'is', 'to', 'capture', 'the', 'latent', 'relationship', 'within', 'the', 'output', 'language', 'domain', '.', 'We', 'formulate', 'this', 'model', 'a', 'an', 'extension', 'of', 'multi–class', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', 'and', 'present', 'a'], ['-mation', 'Infrastructure', ',', 'digital', 'libraries', ',', 'networked', 'services', ',', 'digital', 'convergence', 'or', 'intelligent', 'agents', '.', 'This', 'attention', 'is', 'moving', 'natural', 'language', 'processing', 'along', 'the', 'critical', 'path', 'for', 'all', 'kind', 'of', 'novel', 'applications', '.', 'This', 'article', 'will', 'mention', 'a', 'number', 'of', 'successful', 'application', 'of', 'natural', 'language', 'processing', '(', 'NLP'], ['Over', 'the', 'last', 'few', 'years', ',', 'a', 'number', 'of', 'area', 'of', 'natural', 'language', 'processing', 'have', 'begun', 'applying', 'graph-based', 'techniques', '.', 'These', 'include', ',', 'among', 'others', ',', 'text', 'summarization', ',', 'syntactic', 'parsing', ',', 'word', 'sense', 'disambiguation', ',', 'ontology', 'construction', ',', 'sentiment', 'and', 'subjectivity', 'analysis', ',', 'text', 'clustering'], ['In', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'research', 'result', 'from', 'software', 'engineering', 'and', 'software', 'technology', 'have', 'often', 'been', 'neglected', '.'], ['of', 'kernelized', 'sorting', 'to', 'increase', 'it', 'robustness', 'and', 'performance', 'on', 'several', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tasks', ':', 'document', 'matching', 'from', 'parallel', 'and', 'comparable', 'corpora', ',', 'machine', 'transliteration', 'and', 'even', 'image', 'processing', '.', 'Empirically', 'we', 'show', 'that', ',', 'on', 'these', 'tasks', ',', 'a', 'semi-supervised', 'variant', 'of', 'kernelized'], ['will', 'be', 'structured', '.', 'In', 'the', 'word', 'of', 'statistical', 'natural', 'language', 'processing', ',', 'we', 'need', 'a', 'sophisticated', 'statistical', 'model', 'of', 'the', 'basic', 'elements', ',', 'such', 'a', 'word', 'or', 'phrases', ',', 'to', 'be', 'combined', 'with', 'the', 'structural', 'modeling', 'such', 'a', 'syntactic', 'parsing', 'or', 'dependency', 'analysis', '.', 'Since', 'the', 'basic', 'property', 'of', 'these', 'element'], ['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'framework', 'for', 'developing', 'probabilistic', 'classifier', 'in', 'natural', 'language', 'processing', '.', 'Our', 'focus', 'is', 'on', 'formulating', 'model', 'that', 'capture', 'the', 'most', 'important', 'interdependency', 'among', 'features', ',', 'to', 'avoid', 'overfitting', 'the', 'data', 'while', 'also', 'characterizing', 'the', 'data', 'well', '.', 'The', 'class'], ['Many', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'technique', 'have', 'been', 'used', 'in', 'Information', 'Retrieval', '.', 'The', 'result', 'are', 'not', 'encouraging', '.', 'Simple', 'method', '(', 'stopwording', ',', 'porter-style', 'stemming', ',', 'etc', '.', ')', 'usually', 'yield', 'significant', 'improvements', ',', 'while', 'higher-level', 'processing', '(', 'chunking', ',', 'parsing', ',', 'word', 'sense', 'disambiguation'], ['Abstract-', 'This', 'paper', 'explains', 'the', 'information', 'retrieval', 'using', 'natural', 'language', 'processing', 'for', 'Malayalam', 'language', 'in', 'these', 'basic'], ['in', 'the', 'state', 'of', 'the', 'art', 'plan', 'recognition', 'systems', '.', 'This', 'paper', 'will', 'outline', 'the', 'relation', 'between', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'plan', 'recognition', '(', 'PR', ')', ',', 'argue', 'that', 'each', 'of', 'them', 'can', 'effectively', 'inform', 'the', 'other', ',', 'and', 'then', 'focus', 'on', 'key', 'recent', 'research', 'result', 'in', 'NLP', 'and', 'argue', 'for', 'their', 'applicability', 'to', 'PR', '.', '1'], ['in', 'the', 'state', 'of', 'the', 'art', 'plan', 'recognition', 'systems', '.', 'This', 'paper', 'will', 'outline', 'the', 'relation', 'between', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'plan', 'recognition', '(', 'PR', ')', ',', 'argue', 'that', 'each', 'of', 'them', 'can', 'effectively', 'inform', 'the', 'other', ',', 'and', 'then', 'focus', 'on', 'key', 'recent', 'research', 'result', 'in', 'NLP', 'and', 'argue', 'for', 'their', 'applicability', 'to', 'PR', '.', '1'], ['Information', 'retrieval', 'is', 'the', 'process', 'of', 'finding', 'the', 'document', 'in', 'a', 'document', 'collection', 'that', 'satisfies', 'the', 'information', 'need', 'of', 'the', 'user', '.', 'The', 'document', 'are', 'natural', 'language', 'constructs', ',', 'and', 'the', 'motivation', 'of', 'this', 'work', 'is', 'to', 'investigate', 'how', 'natural', 'language', 'processing', 'can', 'be', 'used', 'to', 'improve'], ['of', 'logic', 'programming', 'within', 'both', 'natural', 'language', 'research', 'and', 'machine', 'learning', ',', 'we', 'point', 'out', 'opportunity', 'for', 'induction', 'of', 'linguistic', 'knowledge', 'within', 'logic', '(', 'programming', ')', '.', 'Keywords', ':', 'inductive', 'logic', 'programming', ',', 'natural', 'language', 'processing', ',', 'logic', 'programming', ',', 'machine', 'learning', '.', '1', 'Introduction', 'There', 'is', 'a'], ['What', 'is', 'a', 'statistical', 'method', 'and', 'how', 'can', 'it', 'be', 'used', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '?', 'In', 'this', 'paper', ',', 'we', 'start', 'from', 'a', 'definition', 'of', 'NLP', 'a', 'concerned', 'with', 'the', 'design', 'and', 'implementation', 'of', 'effective', 'natural', 'language', 'input', 'and', 'output', 'component', 'for', 'computational', 'systems', '.', 'We', 'distinguish', 'three'], ['In', 'this', 'report', ',', 'some', 'collaborative', 'work', 'between', 'the', 'field', 'of', 'Machine', 'Learning', '(', 'ML', ')', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'presented', '.', 'The', 'document', 'is', 'structured', 'in', 'two', 'parts', '.', 'The', 'first', 'part', 'includes', 'a', 'superficial', 'but', 'comprehensive', 'survey', 'covering', 'the', 'state', '--', 'of', '--', 'the', '--', 'art', 'of', 'machine', 'learning'], ['Abstract', '.', 'This', 'thesis', 'examines', 'the', 'use', 'of', 'machine', 'learning', 'technique', 'in', 'various', 'task', 'of', 'natural', 'language', 'processing', ',', 'mainly', 'for', 'the', 'task', 'of', 'information', 'extraction', 'from', 'texts', '.', 'The', 'objective', 'are', 'the', 'improvement', 'of', 'adaptability', 'of', 'information', 'extraction', 'system', 'to', 'new', 'thematic', 'do-mains', '(', 'or', 'even'], ['This', 'chapter', 'examines', 'the', 'application', 'of', 'natural', 'language', 'processing', 'to', 'computerassisted', 'language', 'learning', 'including', 'the', 'history', 'of', 'work', 'in', 'this', 'field', 'over', 'the', 'last', 'thirtyfive', 'year', 'but', 'with', 'a', 'focus', 'on', 'current', 'development', 'and', 'opportunities', '.', '36.1'], ['Traditional', 'approach', 'tointerpretation', 'in', 'natural', 'language', 'processing', 'typically', 'fall', 'into', 'one', 'of', 'three', 'classes', ':', 'syntax-driven', ',', 'semantics-driven', ',', 'or', 'frame/task', 'based', '.', 'Syntax-driven', 'approach', 'use', 'a', 'domain-independent', 'grammar', 'to', 'drive', 'the', 'interpretation', 'process', 'and', 'produce', 'a', 'global', 'parse'], ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'very', 'large', 'and', 'diverse', 'subtopic', 'of', 'artificial', 'intelligence', '.', 'As', 'a', 'result', ',', 'NLP', 'itself', 'ha', 'many', 'subtopics', 'including', 'optical', 'character', 'recognition', ',', 'text', 'to', 'speech', 'translators', ',', 'foreign', 'language', 'reading', 'and', 'writing', 'aids', ',', 'machine', 'translation', ',', 'and', 'speech', 'recognition'], ['Probabilistic', 'finite-state', 'string', 'transducer', '(', 'FSTs', ')', 'are', 'extremely', 'popular', 'in', 'natural', 'language', 'processing', ',', 'due', 'to', 'powerful', 'generic', 'method', 'for', 'applying', ',', 'composing', ',', 'and', 'learning', 'them', '.', 'Unfortunately', ',', 'FSTs', 'are', 'not', 'a', 'good', 'fit', 'for', 'much', 'of', 'the', 'current', 'work', 'on', 'probabilistic', 'modeling', 'for', 'machine'], ['ABSTRACT', '.', 'In', 'this', 'special', 'issue', 'of', 'TAL', ',', 'we', 'look', 'at', 'the', 'fundamental', 'principle', 'underlying', 'evaluation', 'in', 'natural', 'language', 'processing', '.', 'We', 'adopt', 'a', 'global', 'point', 'of', 'view', 'that', 'go', 'beyond', 'the', 'horizon', 'of', 'a', 'single', 'evaluation', 'campaign', 'or', 'a', 'particular', 'protocol', '.', 'After', 'a', 'brief', 'review', 'of', 'history', 'and', 'terminology'], ['Abstract', 'not', 'found'], ['Natural', 'language', 'processing', 'system', '(', 'NLP', ')', 'that', 'extract', 'clinical', 'information', 'from', 'textual', 'report', 'were', 'shown', 'to', 'be', 'effective', 'for', 'limited', 'domain', 'and', 'for', 'particular', 'applications', '.', 'Because', 'an', 'NLP', 'system', 'typically', 'requires', 'substantial', 'resource', 'to', 'develop', ',', 'it', 'is', 'beneficial', 'if', 'it', 'is', 'designed', 'to', 'be', 'easily'], ['fact', 'form', 'a', 'link', 'between', 'IE', ',', 'a', 'recent', 'development', 'in', 'Natural', 'Language', 'Processing', ',', 'and', 'logic', 'programming', 'with', 'Prolog', '.', '1'], ['We', 'describe', 'a', 'single', 'convolutional', 'neural', 'network', 'architecture', 'that', ',', 'given', 'a', 'sentence', ',', 'output', 'a', 'host', 'of', 'language', 'processing', 'predictions', ':', 'part-of-speech', 'tags', ',', 'chunks', ',', 'named', 'entity', 'tags', ',', 'semantic', 'roles', ',', 'semantically', 'similar', 'word', 'and', 'the', 'likelihood', 'that', 'the', 'sentence', 'make', 'sense', '(', 'grammatically'], ['We', 'developed', 'a', 'prototype', 'information', 'retrieval', 'system', 'which', 'us', 'advanced', 'natural', 'language', 'processing', 'technique', 'to', 'enhance', 'the', 'effectiveness', 'of', 'traditional', 'key-word', 'based', 'document', 'retrieval', '.', 'The', 'backbone', 'of', 'our', 'system', 'is', 'a', 'statistical', 'retrieval', 'engine', 'which', 'performs', 'automated', 'indexing'], ['Abstract', 'not', 'found'], ['In', 'this', 'paper', 'we', 'will', 'discus', 'several', 'issue', 'and', 'requirement', 'for', 'enabling', 'natural', 'language', 'processing', 'system', 'to', 'become', 'context-adaptive', '.', 'Given', 'the', 'fact', 'that', 'emerging', 'system', 'feature', 'speaker', 'independent', 'continuous', 'speech', 'recognition', 'restricted', 'to', 'individual', 'domain', 'and', 'are', 'equipped', 'with', 'syntactic'], ['In', 'Fall', '2004', 'I', 'introduced', 'a', 'new', 'course', 'called', 'Applied', 'Natural', 'Language', 'Processing', ',', 'in', 'which', 'student', 'acquire', 'an', 'understanding', 'of', 'which', 'text', 'analysis', 'technique', 'are', 'currently', 'feasible', 'for', 'practical', 'applications', '.'], ['Abstract', 'not', 'found'], ['Abstract', ':', 'Natural', 'language', 'processing', 'is', 'the', 'study', 'of', 'mathematical', 'and', 'computational', 'modelling', 'of', 'various', 'aspect', 'of', 'language', 'and', 'the', 'improvement', 'of', 'a', 'wide', 'range', 'of', 'systems', '.', 'Natural', 'language', 'is', 'any', 'language', 'that', 'arises', 'a', 'an', 'innate', 'facility', 'for', 'language', 'possessed', 'by', 'the', 'human', 'intellect', ';', 'it', 'may'], ['Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'which', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', ',', 'includes', 'speech', 'synthesis', ',', 'Speech', 'recognition', ',', 'and', 'Machine', 'translation', '.', 'Natural', 'Language', 'Processing', 'ha', 'a', 'wide', 'range', 'of', 'application', 'in', 'the', 'Indian', 'context', '.', 'Most', 'of', 'the', 'rural', 'Indian', 'community', 'is', 'unable', 'to', 'make', 'use'], ['An', 'Evaluation', 'of', 'LOLITA', 'and', 'related', 'Natural', 'Language', 'Processing', 'Systems', 'Paul', 'Callaghan', 'Submitted', 'to', 'the', 'University', 'of', 'Durham', 'for', 'the', 'degree', 'of', 'Ph.D.', ',', 'August', '1997', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-', 'This', 'research', 'address', 'the', 'question', ',', '``', 'how', 'do', 'we', 'evaluate', 'system', 'like', 'LOLITA', '?', \"''\", 'LOLITA', 'is', 'the', 'Natural'], ['Previous', 'work', 'demonstrated', 'that', 'Web', 'count', 'can', 'be', 'used', 'to', 'approximate', 'bigram', 'counts', ',', 'suggesting', 'that', 'Web-based', 'frequency', 'should', 'be', 'useful', 'for', 'a', 'wide', 'variety', 'of', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tasks', '.', 'However', ',', 'only', 'a', 'limited', 'number', 'of', 'task', 'have', 'so', 'far', 'been', 'tested', 'using', 'Web-scale', 'data', 'set'], ['This', 'chapter', 'examines', 'the', 'application', 'of', 'natural', 'language', 'processing', 'to', 'computerassisted', 'language', 'learning', 'including', 'the', 'history', 'of', 'work', 'in', 'this', 'field', 'over', 'the', 'last', 'thirtyfive', 'year', 'but', 'with', 'a', 'focus', 'on', 'current', 'development', 'and', 'opportunities', '.', '16.1', 'Introduction', 'This', 'chapter', 'focus', 'on', 'application'], ['This', 'paper', 'describes', 'a', 'natural', 'language', 'system', 'which', 'improves', 'it', 'own', 'performance', 'through', 'learning', '.', 'The', 'system', 'process', 'short', 'English', 'narrative', 'and', 'is', 'able', 'to', 'acquire', ',', 'from', 'a', 'single', 'narrative', ',', 'a', 'new', 'schema', 'for', 'a', 'stereotypical', 'set', 'of', 'actions', '.', 'During', 'the', 'understanding', 'process', ',', 'the', 'system', 'attempt'], ['We', 'classify', 'and', 'review', 'current', 'approach', 'to', 'software', 'infrastructure', 'for', 'research', ',', 'development', 'and', 'delivery', 'of', 'NLP', 'systems', '.', 'The', 'task'], ['Confidence', 'measure', 'are', 'a', 'practical', 'solution', 'for', 'improving', 'the', 'usefulness', 'of', 'Natural', 'Language', 'Processing', 'applications', '.', 'Confidence', 'estimation', 'is', 'a', 'generic', 'machine', 'learning', 'approach', 'for', 'deriving', 'confidence', 'measures', '.', 'We', 'give', 'an', 'overview', 'of', 'the', 'application', 'of', 'confidence', 'estimation', 'in', 'various', 'field'], ['!', 'lex-sign', 'sense-id', ':', 'sense-id', 'dictionary', '?', '=', '``', 'LDOCE', \"''\", '!', 'lex-sign', 'sense-id', ':', 'sense-id', 'ldb-entry-no', '?', '=', '``', '12364', \"''\", '!', 'lex-sign', 'sense-id', ':', 'sense-id', 'sense-no', '?', '=', '``', '0', \"''\", '.', 'When', 'loaded', 'into', 'the', 'LKB', ',', '(', '9', ')', 'will', 'be', 'expanded', 'into', 'a', 'fully-fledged', 'representation', 'for', 'the', 'transitive', 'use', 'of', 'experience', ';', 'by', 'integrating', 'word-specific', 'information', 'provided', 'by', '(', '9', ')', 'with', 'the', 'information', 'encoded', 'by', 'the', 'LKB', 'type', 'strict-trans-sign', '.', 'Thus', ',', 'although', 'neither', 'LDOCE', ',', 'LLCE', 'or', 'the', 'earlier', 'subcategorised', 'lexicon', 'contain', 'all', 'the', 'information', 'about', 'psychological', 'verb', 'defined', 'in', 'Sanfilippo', '&', 'aposs', 'type', 'system', ',', 'by', 'using', 'the', 'conjunction', 'of', 'information', 'available', 'from', 'all', 'three', ',', 'it', 'proved', 'possible', 'to', 'effectively', 'enrich', 'this', 'information', 'at', 'the', 'same', 'time', 'a', 'mapping', 'it', 'into', 'a', 'formal', 'representation', '.', '4.2.5', 'Towards', 'a', 'Multilingual', 'LKB', 'A', 'goal', 'of', 'ACQUILEX', 'is', 'to', 'demonstrate', 'that', 'an', 'LKB', 'can', 'be', 'produced', 'that', 'usefully', 'exploit', 'various', 'MRD', 'source', 'and', 'integrates', 'multilingual', 'information', '.', 'The', 'use', 'of', 'a', 'common', 'LRL', 'with', 'a', 'common', 'type', 'system', ',', 'make', 'it', 'possi', '...'], ['We', 'describe', 'the', 'design', 'and', 'use', 'of', 'the', 'Stanford', 'CoreNLP', 'toolkit', ',', 'an', 'extensible', 'pipeline', 'that', 'provides', 'core', 'natural', 'lan-guage', 'analysis', '.', 'This', 'toolkit', 'is', 'quite', 'widely', 'used', ',', 'both', 'in', 'the', 'research', 'NLP', 'community', 'and', 'also', 'among', 'commercial', 'and', 'govern-ment', 'user', 'of', 'open', 'source', 'NLP', 'technol-ogy', '.', 'We', 'suggest'], ['Gaussian', 'Processes', '(', 'GPs', ')', 'are', 'a', 'powerful', 'mod-elling', 'framework', 'incorporating', 'kernel', 'and', 'Bayesian', 'inference', ',', 'and', 'are', 'recognised', 'a', 'state-of-the-art', 'for', 'many', 'machine', 'learning', 'tasks', '.'], [':', 'A', 'fundamental', 'issue', 'in', 'natural', 'language', 'processing', 'is', 'the', 'prerequisite', 'of', 'an', 'enormous', 'quantity', 'of', 'preprogrammed', 'knowledge', 'concerning', 'both', 'the', 'language', 'and', 'the', 'domain', 'under', 'examination', '.', 'Manual', 'acquisition', 'of', 'this', 'knowledge', 'is', 'tedious', 'and', 'error', 'prone', '.', 'Development', 'of', 'an', 'automated', 'acquisition'], ['``', 'that', 'support', 'sophisticated', 'natural', 'language', 'processing', 'while', 'significantly', 'simplifying', 'the', 'interface', 'between', 'domain-specific', 'knowledge', 'and', 'general', 'linguis-', 'tic', 'resources', '.', 'This', 'paper', 'present', 'the', 'result', 'of', 'our', 'experience', 'in', 'designing', 'and', 'using', 'the', 'upper', 'model', 'in', 'a', 'variety', 'of', 'application', 'over', 'the', 'past', '5', 'year'], ['into', 'the', 'same', 'or', 'neighboring', 'map', 'nodes', '.', 'Nodes', 'may', 'thus', 'be', 'viewed', 'a', 'word', 'categories', '.', 'Although', 'no', 'a', 'priori', 'information', 'about', 'class', 'is', 'given', ',', 'during', 'the', 'self-organizing', 'process', 'a', 'model', 'of', 'the', 'word', 'class', 'emerges', '.', 'The', 'central', 'topic', 'of', 'the', 'thesis', 'is', 'the', 'use', 'of', 'the', 'SOM', 'in', 'natural', 'language', 'processing', '.', 'The', 'approach'], ['This', 'paper', 'present', 'a', 'workbench', 'built', 'by', 'Priberam', 'Informática', 'for', 'the', 'development', 'of', 'the', 'company', '’', 's', 'natural', 'language', 'processing', 'technology', '.', 'This', 'workbench', 'includes', 'a', 'set', 'of', 'linguistic', 'resource', 'and', 'software', 'tool', 'that', 'have', 'been', 'applied', 'in', 'a', 'considerable', 'number', 'of', 'practical', 'purposes', ',', 'covering'], ['Abstract—Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'effective', 'approach', 'for', 'bringing', 'improvement', 'in', 'educational', 'setting', '.', 'Implementing', 'NLP', 'involves', 'initiating', 'the', 'process', 'of', 'learning', 'through', 'the', 'natural', 'acquisition', 'in', 'the', 'educational', 'systems', '.', 'It', 'is', 'based', 'on', 'effective', 'approach', 'for', 'providing', 'a', 'solution'], ['ABSTRACT', ':', 'After', 'twenty', 'year', 'of', 'disfavor', ',', 'a', 'technology', 'ha', 'returned', 'which', 'imitates', 'the', 'process', 'of', 'the', 'brain', '.', 'Natural', 'language', 'experiment', '(', 'Sejnowski', '&', 'Rosenberg', ':', '1986', ')', 'demonstrate', 'that', 'neural', 'network', 'computing', 'architecture', 'can', 'learn', 'from', 'actual', 'spoken', 'language', ',', 'observe', 'rule', 'of', 'pronunciation'], ['Text', 'statistic', 'are', 'frequently', 'used', 'in', 'stylometry', 'and', 'cryptography', 'studies', '.', 'In', 'this', 'paper', ',', 'some', 'text', 'statistic', 'tool', 'are', 'developed', 'in', 'ISO', 'Prolog', 'for', 'natural', 'language', 'processing', '.', 'Details', 'are', 'given', 'on', 'the', 'usage', 'of', '21', 'user-callable', 'predicates', '.', 'Logic', 'and', 'limitation', 'of', 'the', 'program', 'are', 'also', 'discussed'], ['We', 'summarize', 'our', 'experience', 'using', 'FrameNet', 'in', 'two', 'rather', 'different', 'project', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'We', 'conclude', 'that', 'NLP', 'can', 'benefit', 'from', 'FrameNet', 'in', 'different', 'ways', ',', 'but', 'we', 'sketch', 'some', 'problem', 'that', 'need', 'to', 'be', 'overcome', '.', '1']]\n",
            "['Abstract', 'not', 'found', 'describe', 'a', 'method', 'for', 'statistical', 'modeling', 'based', 'on', 'maximum', 'entropy', '.', 'We', 'present', 'a', 'maximum-likelihood', 'approach', 'for', 'automatically', 'constructing', 'maximum', 'entropy', 'model', 'and', 'describe', 'how', 'to', 'implement', 'this', 'approach', 'efficiently', ',', 'using', 'a', 'example', 'several', 'problem', 'in', 'natural', 'language', 'processing', '.', 'Scaling', 'conditional', 'random', 'field', 'for', 'natural', 'language', 'processing', 'Terms', 'and', 'Conditions', ':', 'Terms', 'and', 'Conditions', ':', 'Copyright', 'in', 'work', 'deposited', 'in', 'Minerva', 'Access', 'is', 'retained', 'by', 'the', 'The', 'paper', 'address', 'the', 'issue', 'of', 'cooperation', 'between', 'linguistics', 'and', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'in', 'general', ',', 'and', 'between', 'linguistics', 'and', 'machine', 'translation', '(', 'MT', ')', ',', 'in', 'particular', '.', 'It', 'focus', 'on', 'just', 'one', 'direction', 'of', 'such', 'cooperation', ',', 'namely', 'application', 'of', 'linguistics', 'to', 'NLP', ',', 'virtually', 'In', 'most', 'natural', 'language', 'processing', 'applications', ',', 'Description', 'Logics', 'have', 'been', 'used', 'to', 'encode', 'in', 'a', 'knowledge', 'base', 'some', 'syntactic', ',', 'semantic', ',', 'and', 'pragmatic', 'element', 'needed', 'to', 'drive', 'the', 'semantic', 'interpretation', 'and', 'the', 'natural', 'language', 'generation', 'processes', '.', 'More', 'recently', ',', 'Description', 'Logics', 'have', 'been', 'We', 'propose', 'a', 'unified', 'neural', 'network', 'architecture', 'and', 'learning', 'algorithm', 'that', 'can', 'be', 'applied', 'to', 'various', 'natural', 'language', 'processing', 'task', 'including', 'part-of-speech', 'tagging', ',', 'chunking', ',', 'named', 'entity', 'recognition', ',', 'and', 'semantic', 'role', 'labeling', '.', 'This', 'versatility', 'is', 'achieved', 'by', 'trying', 'to', 'avoid', 'task', 'Natural', 'Language', 'Processing', 'The', 'subject', 'of', 'Natural', 'Language', 'Processing', 'can', 'be', 'considered', 'in', 'both', 'broad', 'and', 'narrow', 'senses', '.', 'In', 'the', 'broad', 'sense', ',', 'it', 'cover', 'processing', 'issue', 'at', 'all', 'level', 'of', 'natural', 'language', 'understanding', ',', 'including', 'speech', 'recognition', ',', 'syntactic', 'and', 'semantic', 'analysis', 'of', 'sentence', 'Robots', 'that', 'interact', 'with', 'human', 'face-to-face', 'using', 'natural', 'language', 'need', 'to', 'be', 'responsive', 'to', 'the', 'way', 'human', 'use', 'language', 'in', 'those', 'situations', '.', 'We', 'propose', 'a', 'psychologicallyinspired', 'natural', 'language', 'processing', 'system', 'for', 'robot', 'which', 'performs', 'incremental', 'semantic', 'interpretation', 'of', 'spoken', 'utterance', 'Natural', 'language', 'are', 'language', 'spoken', 'by', 'humans', '.', 'Currently', 'we', 'are', 'not', 'yet', 'at', 'the', 'point', 'where', 'these', 'language', 'in', 'all', 'of', 'their', 'unprocessed', 'form', 'can', 'be', 'understood', 'by', 'computers', '.', 'Natural', 'language', 'processing', 'is', 'the', 'collection', 'of', 'technique', 'employed', 'to', 'try', 'and', 'accomplish', 'that', 'goal', '.', 'The', 'field', 'of', 'natural', 'ABSTRACT', ':', 'Ambiguity', 'can', 'be', 'referred', 'a', 'the', 'ability', 'of', 'having', 'more', 'than', 'one', 'meaning', 'or', 'being', 'understood', 'in', 'more', 'than', 'one', 'way', '.', 'Natural', 'language', 'are', 'ambiguous', ',', 'so', 'computer', 'are', 'not', 'able', 'to', 'understand', 'language', 'the', 'way', 'people', 'do', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'concerned', 'with', 'the', 'development', 'Introduction', 'Statistical', 'natural', 'language', 'processing', '(', 'SNLP', ')', 'is', 'a', 'field', 'lying', 'in', 'the', 'intersection', 'of', 'natural', 'language', 'processing', 'and', 'machine', 'learning', '.', 'SNLP', 'di', '#', 'ers', 'from', 'traditional', 'natural', 'language', 'processing', 'in', 'that', 'instead', 'of', 'having', 'a', 'linguist', 'manually', 'construct', 'some', 'model', 'of', 'a', 'given', 'linguistic', 'text', 'directly', '(', 'rather', 'than', 'e.g', '.', 'title', 'and', 'abstracts', ')', ',', 'and', 'suggests', 'appropriate', 'approach', 'to', 'doing', 'this', ',', 'with', 'a', 'focus', 'on', 'the', 'role', 'of', 'natural', 'language', 'processing', '.', 'The', 'paper', 'also', 'comment', 'on', 'possible', 'connection', 'with', 'data', 'and', 'knowledge', 'retrieval', ',', 'and', 'concludes', 'by', 'emphasizing', 'the', 'importance', 'of', 'rigorous', 'ABSTRACT', ':', 'Language', 'is', 'way', 'of', 'communicating', 'your', 'word', 'Language', 'help', 'in', 'understanding', 'the', 'world', ',', 'we', 'get', 'a', 'better', 'insight', 'of', 'the', 'world', '.', 'Language', 'help', 'speaker', 'to', 'be', 'a', 'vague', 'or', 'a', 'precise', 'a', 'they', 'like', '.', 'NLP', 'Stands', 'for', 'natural', 'language', 'processing..', 'Natural', 'language', 'are', 'those', 'language', 'that', 'are', 'spoken', 'We', 'report', 'experiment', 'on', 'the', 'use', 'of', 'standard', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tool', 'for', 'the', 'analysis', 'of', 'music', 'lyrics', '.', 'A', 'significant', 'amount', 'of', 'music', 'audio', 'ha', 'lyrics', '.', 'Lyrics', 'encode', 'an', 'important', 'part', 'of', 'the', 'semantics', 'of', 'a', 'song', ',', 'therefore', 'their', 'analysis', 'complement', 'that', 'of', 'acoustic', 'and', 'cultural', 'this', 'paper', ',', 'we', 'will', 'describe', 'a', 'simple', 'rule-based', 'approach', 'to', 'automated', 'learning', 'of', 'linguistic', 'knowledge', '.', 'This', 'approach', 'ha', 'been', 'shown', 'for', 'a', 'number', 'of', 'task', 'to', 'capture', 'information', 'in', 'a', 'clearer', 'and', 'more', 'direct', 'fashion', 'without', 'a', 'compromise', 'in', 'performance', '.', 'We', 'present', 'a', 'detailed', 'case', 'study', 'of', 'this', 'learning', 'method', 'applied', 'to', 'part', 'of', 'speech', 'tagging', 'This', 'paper', 'focus', 'on', 'connectionist', 'model', 'in', 'natural', 'language', 'processing', '.', 'We', 'briefly', 'present', 'and', 'discus', 'several', 'aspect', 'of', 'high', 'level', 'task', 'which', 'recently', 'have', 'been', 'approached', 'with', 'connectionism', ',', 'either', 'with', 'localist', 'or', 'parallel', 'distributed', 'processing', 'models', '.', 'Several', 'interesting', 'architecture', 'process', 'of', 'language', 'understanding', '.', 'This', 'is', 'a', 'new', 'approach', 'in', 'natural', 'language', 'processing', 'based', 'on', 'the', 'deterministic', 'chaotic', 'behavior', 'of', 'dynamical', 'systems', '.', '1', 'this', 'paper', '(', 'see', '[', 'Schank', '86', ']', 'for', 'a', 'theoretical', 'discussion', 'and', '[', 'Kass', '86', ']', 'and', '[', 'Leake', 'and', 'Owens', '86', ']', 'for', 'brief', 'discussion', 'of', 'a', 'program', 'built', 'around', 'these', '.principles', ')', ';', 'the', 'goal', 'here', 'is', 'simply', 'to', 'point', 'out', 'how', 'our', 'interest', 'in', 'natural', 'language', 'processing', 'ha', 'led', 'u', 'naturally', ',', 'and', 'indeed', 'inevitably', 'Objectives', 'To', 'provide', 'an', 'overview', 'and', 'tutorial', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'modern', 'NLP-system', 'design', '.', 'Target', 'audience', 'This', 'tutorial', 'target', 'the', 'medical', 'informatics', 'generalist', 'who', 'ha', 'limited', 'acquaintance', 'with', 'the', 'principle', 'behind', 'NLP', 'and/or', 'limited', 'knowledge', 'of', 'the', 'current', 'state', 'This', 'paper', 'briefly', 'describes', 'the', 'current', 'implementation', 'status', 'of', 'an', 'intelligent', 'information', 'retrieval', 'system', ',', 'MARIE', ',', 'that', 'employ', 'natural', 'language', 'processing', 'techniques', '.', 'Descriptive', 'caption', 'are', 'used', 'to', 'iden-', 'tify', 'photographic', 'image', 'concerning', 'various', 'military', 'projects', '.', 'The', 'caption', 'are', 'parsed', 'based', 'and', 'literature', 'resources', '.', 'We', 'describe', 'here', 'a', 'system', 'for', 'agent', 'directed', 'natural', 'language', 'processing', 'to', 'extract', 'information', 'from', 'journal', 'articles', '.', 'An', 'interface', 'wa', 'developed', 'to', 'permit', 'curation', 'of', 'the', 'NLP', 'result', 'and', 'deposition', 'of', 'accepted', 'result', 'into', 'a', 'knowledge', 'base', '.', 'Motivation', ':', 'The', 'advent', 'of', 'high', 'to', 'evaluation', 'in', 'speech', 'processing', '.', 'Part', '2', 'survey', 'significant', 'evaluation', 'work', 'done', 'so', 'far', ',', 'for', 'instance', 'in', 'machine', 'translation', ',', 'and', 'discus', 'the', 'particular', 'problem', 'of', 'generic', 'system', 'evaluation', '.', 'The', 'conclusion', 'is', 'that', 'evaluation', 'strategy', 'and', 'technique', 'for', 'NLP', 'need', 'much', 'more', 'development', ',', 'in', 'particular', 'similar', 'to', 'the', 'way', 'human', 'intuitively', 'do', 'in', 'order', 'to', 'eliminate', 'noisy', 'content', '.', 'In', 'this', 'paper', ',', 'we', 'describe', 'a', 'combination', 'of', 'HTML', 'DOM', 'analysis', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'technique', 'for', 'automated', 'extraction', 'of', 'main', 'article', 'with', 'associated', 'image', 'from', 'web', 'pages', '.', 'Abstract', '--', 'Natural', 'Language', 'Processing', 'is', 'a', 'theoretically', 'motivated', 'range', 'of', 'computational', 'technique', 'for', 'analysing', 'and', 'representing', 'naturally', 'occurring', 'text', 'at', 'one', 'or', 'more', 'level', 'of', 'linguistic', 'analysis', 'for', 'the', 'purpose', 'of', 'achieving', 'human-like', 'language', 'processing', 'for', 'a', 'range', 'of', 'task', 'This', 'paper', 'review', 'the', 'process', 'involved', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'It', 'then', 'demonstrates', 'the', 'various', 'kind', 'of', 'choice', 'that', 'need', 'be', 'taken', 'during', 'the', 'execution', 'of', 'the', 'word', 'morphology', ',', 'the', 'syntactic', 'text', 'analysis', ',', 'or', 'text', 'generation', 'components', '.', 'It', 'compare', 'the', 'time', 'complexity', 'This', 'article', 'focus', 'on', 'the', 'derivation', 'of', 'large', 'lexicon', 'for', 'natural', 'language', 'processing', '.', 'We', 'describe', 'the', 'development', 'of', 'a', 'dictionary', 'support', 'environment', 'linking', 'a', 'restructured', 'version', 'of', 'the', 'Longman', 'Dictionary', 'of', 'Contemporary', 'English', 'to', 'natural', 'language', 'processing', 'systems', '.', 'The', 'process', 'We', 'introduce', 'a', 'method', 'for', 'analyzing', 'the', 'complexity', 'of', 'natural', 'language', 'processing', 'tasks', ',', 'and', 'for', 'predicting', 'the', 'difficulty', 'new', 'NLP', 'tasks', '.', 'Our', 'complexity', 'measure', 'are', 'derived', 'from', 'the', 'Kolmogorov', 'complexity', 'of', 'a', 'class', 'of', 'automaton', '—', 'meaning', 'automata', ',', 'whose', 'purpose', 'is', 'to', 'extract', 'relevant', 'piece', ',', 'sounds', ',', 'text', 'and', 'motion', '.', 'The', 'technique', 'developed', 'from', 'deep', 'learning', 'research', 'have', 'already', 'been', 'impacting', 'the', 'research', 'of', 'natural', 'language', 'process', '.', 'This', 'paper', 'review', 'the', 'recent', 'research', 'on', 'deep', 'learning', ',', 'it', 'application', 'and', 'recent', 'development', 'in', 'natural', 'language', 'processing', '.', '1', 'This', 'is', 'an', 'author-produced', 'version', 'of', 'a', 'paper', 'published', 'in', 'The', 'Abstract—Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'the', 'application', 'of', 'automated', 'parsing', 'and', 'machine', 'learning', 'technique', 'to', 'analyze', 'standard', 'text', '.', 'Applications', 'of', 'NLP', 'to', 'requirement', 'engineering', 'include', 'extraction', 'of', 'ontology', 'from', 'a', 'requirement', 'specification', ',', 'and', 'use', 'of', 'NLP', 'to', 'verify', 'the', 'consistency', 'statistical', 'baseline', 'including', ':', 'the', 'forgiving', 'nature', 'but', 'broad', 'coverage', 'of', 'the', 'typical', 'retrieval', 'task', ';', 'the', 'lack', 'of', 'good', 'weighting', 'scheme', 'for', 'compound', 'index', 'terms', ';', 'and', 'the', 'implicit', 'linguistic', 'processing', 'inherent', 'in', 'the', 'statistical', 'methods', '.', 'Natural', 'language', 'processing', 'technique', 'may', 'be', 'more', 'important', 'Work', 'in', 'computational', 'linguistics', 'began', 'very', 'soon', 'after', 'the', 'development', 'of', 'the', 'first', 'computer', '(', 'Booth', ',', 'Brandwood', 'and', 'Cleave', '1958', ')', ',', 'yet', 'in', 'the', 'intervening', 'four', 'decade', 'there', 'ha', 'been', 'a', 'pervasive', 'feeling', 'that', 'progress', 'in', 'computer', 'understanding', 'of', 'natural', 'language', 'ha', 'not', 'been', 'commensurate', 'the', 'voice', 'recognition', 'for', 'a', 'natural', 'language', '(', 'Tamil', ')', 'by', 'combining', 'the', 'digital', 'and', 'mathematical', 'knowledge', 'using', 'MFCC', 'and', 'DTW', 'to', 'extract', 'and', 'match', 'the', 'feature', 'to', 'improve', 'the', 'accuracy', 'for', 'better', 'performance', '.', 'Abstract', ':', 'Testing', 'against', 'natural', 'language', 'requirement', 'is', 'the', 'standard', 'approach', 'for', 'system', 'and', 'acceptance', 'testing', '.', 'This', 'test', 'is', 'often', 'performed', 'by', 'an', 'independent', 'test', 'organization', 'unfamiliar', 'with', 'the', 'application', 'area', '.', 'The', 'only', 'thing', 'the', 'tester', 'have', 'to', 'go', 'by', 'are', 'the', 'written', 'requirements', '.', 'So', 'Abstract', 'not', 'found', 'conversational', 'partners', '.', 'But', 'it', 'also', 'provides', 'u', 'with', 'information', 'about', 'being', 'creative', ',', 'making', 'associations', ',', 'storytelling', 'and', 'language', 'use', '.', 'Many', 'more', 'subtlety', 'in', 'face-to-face', 'and', 'multiparty', 'interaction', 'can', 'be', 'added', ',', 'such', 'a', 'using', 'humor', 'to', 'persuade', 'and', 'dominate', ',', 'to', 'soften', 'or', 'avoid', 'a', 'face', 'threatening', 'act', 'Abstract', 'not', 'found', 'In', 'recent', 'years', ',', 'machine', 'learning', '(', 'ML', ')', 'ha', 'been', 'used', 'more', 'and', 'more', 'to', 'solve', 'complex', 'task', 'in', 'different', 'disciplines', ',', 'ranging', 'from', 'Data', 'Mining', 'to', 'Information', 'We', 'argue', 'that', 'manual', 'and', 'automatic', 'thesaurus', 'are', 'alternative', 'resource', 'for', 'the', 'same', 'NLP', 'tasks', '.', 'This', 'involves', 'the', 'radical', 'step', 'of', 'interpreting', 'manual', 'thesaurus', 'a', 'classification', 'of', 'word', 'rather', 'than', 'word', 'senses', ':', 'the', 'case', 'for', 'this', 'is', 'made', '.', 'The', 'range', 'of', 'role', 'for', 'thesaurus', 'within', 'NLP', 'is', 'briefly', 'presented', 'and', 'the', 'WASPS', 'thesaurus', 'is', 'introduced', '.', 'Thesaurus', 'evaluation', 'is', 'now', 'becoming', 'urgent', '.', 'A', 'range', 'of', 'evaluation', 'strategies', ',', 'all', 'embedded', 'within', 'NLP', 'tasks', ',', 'is', 'proposed', '.', 'Introduction', 'Patterns', 'in', 'music', 'have', 'been', 'the', 'object', 'of', 'intensive', 'study', 'in', 'the', 'past', 'years', '.', '\\\\One', 'of', 'the', 'purpose', 'of', 'analyzing', 'musical', 'structure', 'and', 'form', 'is', 'to', 'discover', 'the', 'pattern', 'that', 'are', 'explicit', 'or', 'implicit', 'in', 'musical', 'works', \"''\", 'Simon', '[', '13', ']', '.', 'Patterns', 'comprise', 'periodicity', ',', 'make', 'use', 'of', 'alphabets', ',', 'can', 'be', 'compound', '(', 'made', 'up', 'of', 'subpatterns', ')', 'and', 'posse', 'phrase', 'structure', 'with', 'various', 'form', 'of', 'punctuation', '.', 'Traditionally', ',', 'composer', 'have', 'employed', 'pattern', 'propagation', 'intuitively', ',', 'but', 'algorithmic', 'composition', 'technique', 'allow', 'the', 'pattern', 'propagation', 'to', 'be', 'formalized', ',', 'albeit', 'a', 'high', 'level', '.', 'During', 'composition', ',', 'all', 'the', 'musical', 'pattern', 'evolve', 'according', 'to', 'the', 'rule', 'and', 'constraint', 'specied', 'at', 'the', 'design', 'stage', '.', 'In', 'jazz', 'improvisation', ',', 'the', 'musician', 'invents', 'a', 'solo', 'guided', 'by', 'a', 'progression', 'of', 'chord', '(', 'the', 'changes', ')', '.', 'One', 'approach', '[', '1', ']', 'to', 'learn', 'improvising', 'is', 'to', 'memorize', 'pattern', '(', 'short', 'chunk', 'of', 'music', ')', 'that', 't', 'sub-progressions', ',', 'and', 'to', 'concatenate', 'them', 'to', 'form', 'a', 'whole', 'solo', 'that', 't', 'a', 'whole', 'progression', '.', 'One', 'Abstract', 'Many', 'information', 'retrieval', '(', 'IR', ')', 'system', 'retrieve', 'relevant', 'document', 'based', 'on', 'exact', 'matching', 'of', 'keywords', 'between', 'a', 'query', 'and', 'documents', '.', 'This', 'method', 'degrades', 'precision', 'rate', '.', 'In', 'order', 'to', 'solve', 'the', 'problem', ',', 'we', 'collected', 'semantically', 'related', 'word', 'and', 'assigned', 'semantic', 'relationship', 'used', 'in', 'general', 'thesaurus', 'and', 'a', 'special', 'relationship', 'called', 'keyfact', 'term', '(', 'FT', ')', 'manually', '.', 'In', 'addition', 'to', 'the', 'semantic', 'knowledge', ',', 'we', 'automatically', 'constructed', 'statistic', 'knowledge', 'based', 'on', 'the', 'concept', 'of', 'mutual', 'information', '.', 'Keyfact', 'is', 'an', 'extended', 'concept', 'of', 'keyword', 'represented', 'by', 'noun', 'and', 'compound', 'noun', '.', 'Keyfact', 'can', 'be', 'a', 'verb', 'and', 'an', 'adjective', 'including', 'subject', 'or', 'object', 'term', '.', 'We', 'first', 'retrieved', 'relevant', 'document', 'with', 'original', 'query', 'using', 'tf', '*', 'idf', 'weighting', 'formula', 'and', 'then', 'an', 'expanded', 'query', 'including', 'keyfacts', 'is', 'used', 'in', 'both', 'second', 'document', 'ranking', 'and', 'word', 'sense', 'disambiguating', '.', 'So', 'we', 'made', 'an', 'improvement', 'in', 'precision', 'rate', 'using', 'keyfact', 'network', '.', '1', 'this', 'paper', 'we', 'argue', 'that', 'questionanswering', '(', 'QA', ')', 'over', 'technical', 'domain', 'is', 'distinctly', 'different', 'from', 'TREC-based', 'QA', 'or', 'Web-based', 'QA', 'and', 'it', 'can', 'not', 'benefit', 'lom', 'data-intensive', 'approach', 'Universit', '&', 'quot', ';', 'at', 'de', 'Saarlandes', 'Proceedings', 'of', 'the', 'Workshop', 'on', 'uni-hamburg.de', 'Abstract', 'not', 'found', 'Abstract', 'not', 'found', 'SRI', 'ha', 'developed', 'a', 'new', 'architecture', 'for', 'integrating', 'speech', 'and', 'natural-language', 'processing', 'that', 'applies', 'linguistic', 'constraint', 'during', 'recognition', 'by', 'incrementally', 'expanding', 'the', 'state-transition', 'network', 'embodied', 'in', 'a', 'unification', 'grammar', '.', 'We', 'compare', 'this', 'dynamic-gralnlnar-network', '(', 'DGN', ')', 'approach', 'This', 'chapter', 'considers', 'the', 'revolution', 'that', 'ha', 'taken', 'place', 'in', 'natural', 'language', 'processing', 'research', 'over', 'the', 'last', 'five', 'years', '.', 'It', 'begin', 'by', 'providing', 'a', 'brief', 'guide', 'to', 'the', 'structure', 'of', 'the', 'field', 'and', 'then', 'present', 'a', 'caricature', 'of', 'two', 'competing', 'paradigm', 'of', '1980s', 'NLP', 'research', 'and', 'indicates', 'the', 'reason', 'visual', 'development', 'environment', 'to', 'support', 'the', 'visual', 'assembly', ',', 'execution', 'and', 'analysis', 'of', 'modular', 'natural', 'language', 'processing', 'systems', '.', 'The', 'visual', 'model', 'is', 'an', 'executable', 'data', 'flow', 'program', 'graph', ',', 'automatically', 'synthesised', 'from', 'data', 'dependency', 'declaration', 'of', 'language', 'processing', 'modules', '.', 'The', 'graph', 'In', 'this', 'Chapter', 'the', 'basic', 'us', 'of', 'Description', 'Logics', 'for', 'Natural', 'Language', 'Processing', 'will', 'be', 'analysed', ',', 'together', 'with', 'a', 'little', 'bit', 'of', 'history', ',', 'and', 'the', 'role', 'of', 'Description', 'Logics', 'in', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'computational', 'linguistics', 'will', 'be', 'pointed', 'out', '.', '18.1', 'Introduction', 'Since', 'the', 'early', 'day', 'We', 'applied', 'a', 'structure', 'learning', 'model', ',', 'Max-Margin', 'Structure', '(', 'MMS', ')', ',', 'to', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', ',', 'where', 'the', 'aim', 'is', 'to', 'capture', 'the', 'latent', 'relationship', 'within', 'the', 'output', 'language', 'domain', '.', 'We', 'formulate', 'this', 'model', 'a', 'an', 'extension', 'of', 'multi–class', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', 'and', 'present', 'a', '-mation', 'Infrastructure', ',', 'digital', 'libraries', ',', 'networked', 'services', ',', 'digital', 'convergence', 'or', 'intelligent', 'agents', '.', 'This', 'attention', 'is', 'moving', 'natural', 'language', 'processing', 'along', 'the', 'critical', 'path', 'for', 'all', 'kind', 'of', 'novel', 'applications', '.', 'This', 'article', 'will', 'mention', 'a', 'number', 'of', 'successful', 'application', 'of', 'natural', 'language', 'processing', '(', 'NLP', 'Over', 'the', 'last', 'few', 'years', ',', 'a', 'number', 'of', 'area', 'of', 'natural', 'language', 'processing', 'have', 'begun', 'applying', 'graph-based', 'techniques', '.', 'These', 'include', ',', 'among', 'others', ',', 'text', 'summarization', ',', 'syntactic', 'parsing', ',', 'word', 'sense', 'disambiguation', ',', 'ontology', 'construction', ',', 'sentiment', 'and', 'subjectivity', 'analysis', ',', 'text', 'clustering', 'In', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'research', 'result', 'from', 'software', 'engineering', 'and', 'software', 'technology', 'have', 'often', 'been', 'neglected', '.', 'of', 'kernelized', 'sorting', 'to', 'increase', 'it', 'robustness', 'and', 'performance', 'on', 'several', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tasks', ':', 'document', 'matching', 'from', 'parallel', 'and', 'comparable', 'corpora', ',', 'machine', 'transliteration', 'and', 'even', 'image', 'processing', '.', 'Empirically', 'we', 'show', 'that', ',', 'on', 'these', 'tasks', ',', 'a', 'semi-supervised', 'variant', 'of', 'kernelized', 'will', 'be', 'structured', '.', 'In', 'the', 'word', 'of', 'statistical', 'natural', 'language', 'processing', ',', 'we', 'need', 'a', 'sophisticated', 'statistical', 'model', 'of', 'the', 'basic', 'elements', ',', 'such', 'a', 'word', 'or', 'phrases', ',', 'to', 'be', 'combined', 'with', 'the', 'structural', 'modeling', 'such', 'a', 'syntactic', 'parsing', 'or', 'dependency', 'analysis', '.', 'Since', 'the', 'basic', 'property', 'of', 'these', 'element', 'In', 'this', 'paper', ',', 'we', 'describe', 'a', 'framework', 'for', 'developing', 'probabilistic', 'classifier', 'in', 'natural', 'language', 'processing', '.', 'Our', 'focus', 'is', 'on', 'formulating', 'model', 'that', 'capture', 'the', 'most', 'important', 'interdependency', 'among', 'features', ',', 'to', 'avoid', 'overfitting', 'the', 'data', 'while', 'also', 'characterizing', 'the', 'data', 'well', '.', 'The', 'class', 'Many', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'technique', 'have', 'been', 'used', 'in', 'Information', 'Retrieval', '.', 'The', 'result', 'are', 'not', 'encouraging', '.', 'Simple', 'method', '(', 'stopwording', ',', 'porter-style', 'stemming', ',', 'etc', '.', ')', 'usually', 'yield', 'significant', 'improvements', ',', 'while', 'higher-level', 'processing', '(', 'chunking', ',', 'parsing', ',', 'word', 'sense', 'disambiguation', 'Abstract-', 'This', 'paper', 'explains', 'the', 'information', 'retrieval', 'using', 'natural', 'language', 'processing', 'for', 'Malayalam', 'language', 'in', 'these', 'basic', 'in', 'the', 'state', 'of', 'the', 'art', 'plan', 'recognition', 'systems', '.', 'This', 'paper', 'will', 'outline', 'the', 'relation', 'between', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'plan', 'recognition', '(', 'PR', ')', ',', 'argue', 'that', 'each', 'of', 'them', 'can', 'effectively', 'inform', 'the', 'other', ',', 'and', 'then', 'focus', 'on', 'key', 'recent', 'research', 'result', 'in', 'NLP', 'and', 'argue', 'for', 'their', 'applicability', 'to', 'PR', '.', '1', 'in', 'the', 'state', 'of', 'the', 'art', 'plan', 'recognition', 'systems', '.', 'This', 'paper', 'will', 'outline', 'the', 'relation', 'between', 'natural', 'language', 'processing', '(', 'NLP', ')', 'and', 'plan', 'recognition', '(', 'PR', ')', ',', 'argue', 'that', 'each', 'of', 'them', 'can', 'effectively', 'inform', 'the', 'other', ',', 'and', 'then', 'focus', 'on', 'key', 'recent', 'research', 'result', 'in', 'NLP', 'and', 'argue', 'for', 'their', 'applicability', 'to', 'PR', '.', '1', 'Information', 'retrieval', 'is', 'the', 'process', 'of', 'finding', 'the', 'document', 'in', 'a', 'document', 'collection', 'that', 'satisfies', 'the', 'information', 'need', 'of', 'the', 'user', '.', 'The', 'document', 'are', 'natural', 'language', 'constructs', ',', 'and', 'the', 'motivation', 'of', 'this', 'work', 'is', 'to', 'investigate', 'how', 'natural', 'language', 'processing', 'can', 'be', 'used', 'to', 'improve', 'of', 'logic', 'programming', 'within', 'both', 'natural', 'language', 'research', 'and', 'machine', 'learning', ',', 'we', 'point', 'out', 'opportunity', 'for', 'induction', 'of', 'linguistic', 'knowledge', 'within', 'logic', '(', 'programming', ')', '.', 'Keywords', ':', 'inductive', 'logic', 'programming', ',', 'natural', 'language', 'processing', ',', 'logic', 'programming', ',', 'machine', 'learning', '.', '1', 'Introduction', 'There', 'is', 'a', 'What', 'is', 'a', 'statistical', 'method', 'and', 'how', 'can', 'it', 'be', 'used', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '?', 'In', 'this', 'paper', ',', 'we', 'start', 'from', 'a', 'definition', 'of', 'NLP', 'a', 'concerned', 'with', 'the', 'design', 'and', 'implementation', 'of', 'effective', 'natural', 'language', 'input', 'and', 'output', 'component', 'for', 'computational', 'systems', '.', 'We', 'distinguish', 'three', 'In', 'this', 'report', ',', 'some', 'collaborative', 'work', 'between', 'the', 'field', 'of', 'Machine', 'Learning', '(', 'ML', ')', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'presented', '.', 'The', 'document', 'is', 'structured', 'in', 'two', 'parts', '.', 'The', 'first', 'part', 'includes', 'a', 'superficial', 'but', 'comprehensive', 'survey', 'covering', 'the', 'state', '--', 'of', '--', 'the', '--', 'art', 'of', 'machine', 'learning', 'Abstract', '.', 'This', 'thesis', 'examines', 'the', 'use', 'of', 'machine', 'learning', 'technique', 'in', 'various', 'task', 'of', 'natural', 'language', 'processing', ',', 'mainly', 'for', 'the', 'task', 'of', 'information', 'extraction', 'from', 'texts', '.', 'The', 'objective', 'are', 'the', 'improvement', 'of', 'adaptability', 'of', 'information', 'extraction', 'system', 'to', 'new', 'thematic', 'do-mains', '(', 'or', 'even', 'This', 'chapter', 'examines', 'the', 'application', 'of', 'natural', 'language', 'processing', 'to', 'computerassisted', 'language', 'learning', 'including', 'the', 'history', 'of', 'work', 'in', 'this', 'field', 'over', 'the', 'last', 'thirtyfive', 'year', 'but', 'with', 'a', 'focus', 'on', 'current', 'development', 'and', 'opportunities', '.', '36.1', 'Traditional', 'approach', 'tointerpretation', 'in', 'natural', 'language', 'processing', 'typically', 'fall', 'into', 'one', 'of', 'three', 'classes', ':', 'syntax-driven', ',', 'semantics-driven', ',', 'or', 'frame/task', 'based', '.', 'Syntax-driven', 'approach', 'use', 'a', 'domain-independent', 'grammar', 'to', 'drive', 'the', 'interpretation', 'process', 'and', 'produce', 'a', 'global', 'parse', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'very', 'large', 'and', 'diverse', 'subtopic', 'of', 'artificial', 'intelligence', '.', 'As', 'a', 'result', ',', 'NLP', 'itself', 'ha', 'many', 'subtopics', 'including', 'optical', 'character', 'recognition', ',', 'text', 'to', 'speech', 'translators', ',', 'foreign', 'language', 'reading', 'and', 'writing', 'aids', ',', 'machine', 'translation', ',', 'and', 'speech', 'recognition', 'Probabilistic', 'finite-state', 'string', 'transducer', '(', 'FSTs', ')', 'are', 'extremely', 'popular', 'in', 'natural', 'language', 'processing', ',', 'due', 'to', 'powerful', 'generic', 'method', 'for', 'applying', ',', 'composing', ',', 'and', 'learning', 'them', '.', 'Unfortunately', ',', 'FSTs', 'are', 'not', 'a', 'good', 'fit', 'for', 'much', 'of', 'the', 'current', 'work', 'on', 'probabilistic', 'modeling', 'for', 'machine', 'ABSTRACT', '.', 'In', 'this', 'special', 'issue', 'of', 'TAL', ',', 'we', 'look', 'at', 'the', 'fundamental', 'principle', 'underlying', 'evaluation', 'in', 'natural', 'language', 'processing', '.', 'We', 'adopt', 'a', 'global', 'point', 'of', 'view', 'that', 'go', 'beyond', 'the', 'horizon', 'of', 'a', 'single', 'evaluation', 'campaign', 'or', 'a', 'particular', 'protocol', '.', 'After', 'a', 'brief', 'review', 'of', 'history', 'and', 'terminology', 'Abstract', 'not', 'found', 'Natural', 'language', 'processing', 'system', '(', 'NLP', ')', 'that', 'extract', 'clinical', 'information', 'from', 'textual', 'report', 'were', 'shown', 'to', 'be', 'effective', 'for', 'limited', 'domain', 'and', 'for', 'particular', 'applications', '.', 'Because', 'an', 'NLP', 'system', 'typically', 'requires', 'substantial', 'resource', 'to', 'develop', ',', 'it', 'is', 'beneficial', 'if', 'it', 'is', 'designed', 'to', 'be', 'easily', 'fact', 'form', 'a', 'link', 'between', 'IE', ',', 'a', 'recent', 'development', 'in', 'Natural', 'Language', 'Processing', ',', 'and', 'logic', 'programming', 'with', 'Prolog', '.', '1', 'We', 'describe', 'a', 'single', 'convolutional', 'neural', 'network', 'architecture', 'that', ',', 'given', 'a', 'sentence', ',', 'output', 'a', 'host', 'of', 'language', 'processing', 'predictions', ':', 'part-of-speech', 'tags', ',', 'chunks', ',', 'named', 'entity', 'tags', ',', 'semantic', 'roles', ',', 'semantically', 'similar', 'word', 'and', 'the', 'likelihood', 'that', 'the', 'sentence', 'make', 'sense', '(', 'grammatically', 'We', 'developed', 'a', 'prototype', 'information', 'retrieval', 'system', 'which', 'us', 'advanced', 'natural', 'language', 'processing', 'technique', 'to', 'enhance', 'the', 'effectiveness', 'of', 'traditional', 'key-word', 'based', 'document', 'retrieval', '.', 'The', 'backbone', 'of', 'our', 'system', 'is', 'a', 'statistical', 'retrieval', 'engine', 'which', 'performs', 'automated', 'indexing', 'Abstract', 'not', 'found', 'In', 'this', 'paper', 'we', 'will', 'discus', 'several', 'issue', 'and', 'requirement', 'for', 'enabling', 'natural', 'language', 'processing', 'system', 'to', 'become', 'context-adaptive', '.', 'Given', 'the', 'fact', 'that', 'emerging', 'system', 'feature', 'speaker', 'independent', 'continuous', 'speech', 'recognition', 'restricted', 'to', 'individual', 'domain', 'and', 'are', 'equipped', 'with', 'syntactic', 'In', 'Fall', '2004', 'I', 'introduced', 'a', 'new', 'course', 'called', 'Applied', 'Natural', 'Language', 'Processing', ',', 'in', 'which', 'student', 'acquire', 'an', 'understanding', 'of', 'which', 'text', 'analysis', 'technique', 'are', 'currently', 'feasible', 'for', 'practical', 'applications', '.', 'Abstract', 'not', 'found', 'Abstract', ':', 'Natural', 'language', 'processing', 'is', 'the', 'study', 'of', 'mathematical', 'and', 'computational', 'modelling', 'of', 'various', 'aspect', 'of', 'language', 'and', 'the', 'improvement', 'of', 'a', 'wide', 'range', 'of', 'systems', '.', 'Natural', 'language', 'is', 'any', 'language', 'that', 'arises', 'a', 'an', 'innate', 'facility', 'for', 'language', 'possessed', 'by', 'the', 'human', 'intellect', ';', 'it', 'may', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ',', 'which', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', ',', 'includes', 'speech', 'synthesis', ',', 'Speech', 'recognition', ',', 'and', 'Machine', 'translation', '.', 'Natural', 'Language', 'Processing', 'ha', 'a', 'wide', 'range', 'of', 'application', 'in', 'the', 'Indian', 'context', '.', 'Most', 'of', 'the', 'rural', 'Indian', 'community', 'is', 'unable', 'to', 'make', 'use', 'An', 'Evaluation', 'of', 'LOLITA', 'and', 'related', 'Natural', 'Language', 'Processing', 'Systems', 'Paul', 'Callaghan', 'Submitted', 'to', 'the', 'University', 'of', 'Durham', 'for', 'the', 'degree', 'of', 'Ph.D.', ',', 'August', '1997', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-', 'This', 'research', 'address', 'the', 'question', ',', '``', 'how', 'do', 'we', 'evaluate', 'system', 'like', 'LOLITA', '?', \"''\", 'LOLITA', 'is', 'the', 'Natural', 'Previous', 'work', 'demonstrated', 'that', 'Web', 'count', 'can', 'be', 'used', 'to', 'approximate', 'bigram', 'counts', ',', 'suggesting', 'that', 'Web-based', 'frequency', 'should', 'be', 'useful', 'for', 'a', 'wide', 'variety', 'of', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'tasks', '.', 'However', ',', 'only', 'a', 'limited', 'number', 'of', 'task', 'have', 'so', 'far', 'been', 'tested', 'using', 'Web-scale', 'data', 'set', 'This', 'chapter', 'examines', 'the', 'application', 'of', 'natural', 'language', 'processing', 'to', 'computerassisted', 'language', 'learning', 'including', 'the', 'history', 'of', 'work', 'in', 'this', 'field', 'over', 'the', 'last', 'thirtyfive', 'year', 'but', 'with', 'a', 'focus', 'on', 'current', 'development', 'and', 'opportunities', '.', '16.1', 'Introduction', 'This', 'chapter', 'focus', 'on', 'application', 'This', 'paper', 'describes', 'a', 'natural', 'language', 'system', 'which', 'improves', 'it', 'own', 'performance', 'through', 'learning', '.', 'The', 'system', 'process', 'short', 'English', 'narrative', 'and', 'is', 'able', 'to', 'acquire', ',', 'from', 'a', 'single', 'narrative', ',', 'a', 'new', 'schema', 'for', 'a', 'stereotypical', 'set', 'of', 'actions', '.', 'During', 'the', 'understanding', 'process', ',', 'the', 'system', 'attempt', 'We', 'classify', 'and', 'review', 'current', 'approach', 'to', 'software', 'infrastructure', 'for', 'research', ',', 'development', 'and', 'delivery', 'of', 'NLP', 'systems', '.', 'The', 'task', 'Confidence', 'measure', 'are', 'a', 'practical', 'solution', 'for', 'improving', 'the', 'usefulness', 'of', 'Natural', 'Language', 'Processing', 'applications', '.', 'Confidence', 'estimation', 'is', 'a', 'generic', 'machine', 'learning', 'approach', 'for', 'deriving', 'confidence', 'measures', '.', 'We', 'give', 'an', 'overview', 'of', 'the', 'application', 'of', 'confidence', 'estimation', 'in', 'various', 'field', '!', 'lex-sign', 'sense-id', ':', 'sense-id', 'dictionary', '?', '=', '``', 'LDOCE', \"''\", '!', 'lex-sign', 'sense-id', ':', 'sense-id', 'ldb-entry-no', '?', '=', '``', '12364', \"''\", '!', 'lex-sign', 'sense-id', ':', 'sense-id', 'sense-no', '?', '=', '``', '0', \"''\", '.', 'When', 'loaded', 'into', 'the', 'LKB', ',', '(', '9', ')', 'will', 'be', 'expanded', 'into', 'a', 'fully-fledged', 'representation', 'for', 'the', 'transitive', 'use', 'of', 'experience', ';', 'by', 'integrating', 'word-specific', 'information', 'provided', 'by', '(', '9', ')', 'with', 'the', 'information', 'encoded', 'by', 'the', 'LKB', 'type', 'strict-trans-sign', '.', 'Thus', ',', 'although', 'neither', 'LDOCE', ',', 'LLCE', 'or', 'the', 'earlier', 'subcategorised', 'lexicon', 'contain', 'all', 'the', 'information', 'about', 'psychological', 'verb', 'defined', 'in', 'Sanfilippo', '&', 'aposs', 'type', 'system', ',', 'by', 'using', 'the', 'conjunction', 'of', 'information', 'available', 'from', 'all', 'three', ',', 'it', 'proved', 'possible', 'to', 'effectively', 'enrich', 'this', 'information', 'at', 'the', 'same', 'time', 'a', 'mapping', 'it', 'into', 'a', 'formal', 'representation', '.', '4.2.5', 'Towards', 'a', 'Multilingual', 'LKB', 'A', 'goal', 'of', 'ACQUILEX', 'is', 'to', 'demonstrate', 'that', 'an', 'LKB', 'can', 'be', 'produced', 'that', 'usefully', 'exploit', 'various', 'MRD', 'source', 'and', 'integrates', 'multilingual', 'information', '.', 'The', 'use', 'of', 'a', 'common', 'LRL', 'with', 'a', 'common', 'type', 'system', ',', 'make', 'it', 'possi', '...', 'We', 'describe', 'the', 'design', 'and', 'use', 'of', 'the', 'Stanford', 'CoreNLP', 'toolkit', ',', 'an', 'extensible', 'pipeline', 'that', 'provides', 'core', 'natural', 'lan-guage', 'analysis', '.', 'This', 'toolkit', 'is', 'quite', 'widely', 'used', ',', 'both', 'in', 'the', 'research', 'NLP', 'community', 'and', 'also', 'among', 'commercial', 'and', 'govern-ment', 'user', 'of', 'open', 'source', 'NLP', 'technol-ogy', '.', 'We', 'suggest', 'Gaussian', 'Processes', '(', 'GPs', ')', 'are', 'a', 'powerful', 'mod-elling', 'framework', 'incorporating', 'kernel', 'and', 'Bayesian', 'inference', ',', 'and', 'are', 'recognised', 'a', 'state-of-the-art', 'for', 'many', 'machine', 'learning', 'tasks', '.', ':', 'A', 'fundamental', 'issue', 'in', 'natural', 'language', 'processing', 'is', 'the', 'prerequisite', 'of', 'an', 'enormous', 'quantity', 'of', 'preprogrammed', 'knowledge', 'concerning', 'both', 'the', 'language', 'and', 'the', 'domain', 'under', 'examination', '.', 'Manual', 'acquisition', 'of', 'this', 'knowledge', 'is', 'tedious', 'and', 'error', 'prone', '.', 'Development', 'of', 'an', 'automated', 'acquisition', '``', 'that', 'support', 'sophisticated', 'natural', 'language', 'processing', 'while', 'significantly', 'simplifying', 'the', 'interface', 'between', 'domain-specific', 'knowledge', 'and', 'general', 'linguis-', 'tic', 'resources', '.', 'This', 'paper', 'present', 'the', 'result', 'of', 'our', 'experience', 'in', 'designing', 'and', 'using', 'the', 'upper', 'model', 'in', 'a', 'variety', 'of', 'application', 'over', 'the', 'past', '5', 'year', 'into', 'the', 'same', 'or', 'neighboring', 'map', 'nodes', '.', 'Nodes', 'may', 'thus', 'be', 'viewed', 'a', 'word', 'categories', '.', 'Although', 'no', 'a', 'priori', 'information', 'about', 'class', 'is', 'given', ',', 'during', 'the', 'self-organizing', 'process', 'a', 'model', 'of', 'the', 'word', 'class', 'emerges', '.', 'The', 'central', 'topic', 'of', 'the', 'thesis', 'is', 'the', 'use', 'of', 'the', 'SOM', 'in', 'natural', 'language', 'processing', '.', 'The', 'approach', 'This', 'paper', 'present', 'a', 'workbench', 'built', 'by', 'Priberam', 'Informática', 'for', 'the', 'development', 'of', 'the', 'company', '’', 's', 'natural', 'language', 'processing', 'technology', '.', 'This', 'workbench', 'includes', 'a', 'set', 'of', 'linguistic', 'resource', 'and', 'software', 'tool', 'that', 'have', 'been', 'applied', 'in', 'a', 'considerable', 'number', 'of', 'practical', 'purposes', ',', 'covering', 'Abstract—Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'effective', 'approach', 'for', 'bringing', 'improvement', 'in', 'educational', 'setting', '.', 'Implementing', 'NLP', 'involves', 'initiating', 'the', 'process', 'of', 'learning', 'through', 'the', 'natural', 'acquisition', 'in', 'the', 'educational', 'systems', '.', 'It', 'is', 'based', 'on', 'effective', 'approach', 'for', 'providing', 'a', 'solution', 'ABSTRACT', ':', 'After', 'twenty', 'year', 'of', 'disfavor', ',', 'a', 'technology', 'ha', 'returned', 'which', 'imitates', 'the', 'process', 'of', 'the', 'brain', '.', 'Natural', 'language', 'experiment', '(', 'Sejnowski', '&', 'Rosenberg', ':', '1986', ')', 'demonstrate', 'that', 'neural', 'network', 'computing', 'architecture', 'can', 'learn', 'from', 'actual', 'spoken', 'language', ',', 'observe', 'rule', 'of', 'pronunciation', 'Text', 'statistic', 'are', 'frequently', 'used', 'in', 'stylometry', 'and', 'cryptography', 'studies', '.', 'In', 'this', 'paper', ',', 'some', 'text', 'statistic', 'tool', 'are', 'developed', 'in', 'ISO', 'Prolog', 'for', 'natural', 'language', 'processing', '.', 'Details', 'are', 'given', 'on', 'the', 'usage', 'of', '21', 'user-callable', 'predicates', '.', 'Logic', 'and', 'limitation', 'of', 'the', 'program', 'are', 'also', 'discussed', 'We', 'summarize', 'our', 'experience', 'using', 'FrameNet', 'in', 'two', 'rather', 'different', 'project', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.', 'We', 'conclude', 'that', 'NLP', 'can', 'benefit', 'from', 'FrameNet', 'in', 'different', 'ways', ',', 'but', 'we', 'sketch', 'some', 'problem', 'that', 'need', 'to', 'be', 'overcome', '.', '1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_sAdhdjkMGa",
        "outputId": "b4f13d7f-1ebf-4dd1-b70b-4cc564319b79"
      },
      "source": [
        "#1.1\r\n",
        "trigram_data=[*nltk.trigrams(final_list)]\r\n",
        "nltk.FreqDist(trigram_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({('Abstract', 'not', 'found'): 8,\n",
              "          ('not', 'found', 'describe'): 1,\n",
              "          ('found', 'describe', 'a'): 1,\n",
              "          ('describe', 'a', 'method'): 1,\n",
              "          ('a', 'method', 'for'): 2,\n",
              "          ('method', 'for', 'statistical'): 1,\n",
              "          ('for', 'statistical', 'modeling'): 1,\n",
              "          ('statistical', 'modeling', 'based'): 1,\n",
              "          ('modeling', 'based', 'on'): 1,\n",
              "          ('based', 'on', 'maximum'): 1,\n",
              "          ('on', 'maximum', 'entropy'): 1,\n",
              "          ('maximum', 'entropy', '.'): 1,\n",
              "          ('entropy', '.', 'We'): 1,\n",
              "          ('.', 'We', 'present'): 2,\n",
              "          ('We', 'present', 'a'): 2,\n",
              "          ('present', 'a', 'maximum-likelihood'): 1,\n",
              "          ('a', 'maximum-likelihood', 'approach'): 1,\n",
              "          ('maximum-likelihood', 'approach', 'for'): 1,\n",
              "          ('approach', 'for', 'automatically'): 1,\n",
              "          ('for', 'automatically', 'constructing'): 1,\n",
              "          ('automatically', 'constructing', 'maximum'): 1,\n",
              "          ('constructing', 'maximum', 'entropy'): 1,\n",
              "          ('maximum', 'entropy', 'model'): 1,\n",
              "          ('entropy', 'model', 'and'): 1,\n",
              "          ('model', 'and', 'describe'): 1,\n",
              "          ('and', 'describe', 'how'): 1,\n",
              "          ('describe', 'how', 'to'): 1,\n",
              "          ('how', 'to', 'implement'): 1,\n",
              "          ('to', 'implement', 'this'): 1,\n",
              "          ('implement', 'this', 'approach'): 1,\n",
              "          ('this', 'approach', 'efficiently'): 1,\n",
              "          ('approach', 'efficiently', ','): 1,\n",
              "          ('efficiently', ',', 'using'): 1,\n",
              "          (',', 'using', 'a'): 1,\n",
              "          ('using', 'a', 'example'): 1,\n",
              "          ('a', 'example', 'several'): 1,\n",
              "          ('example', 'several', 'problem'): 1,\n",
              "          ('several', 'problem', 'in'): 1,\n",
              "          ('problem', 'in', 'natural'): 1,\n",
              "          ('in', 'natural', 'language'): 14,\n",
              "          ('natural', 'language', 'processing'): 49,\n",
              "          ('language', 'processing', '.'): 9,\n",
              "          ('processing', '.', 'Scaling'): 1,\n",
              "          ('.', 'Scaling', 'conditional'): 1,\n",
              "          ('Scaling', 'conditional', 'random'): 1,\n",
              "          ('conditional', 'random', 'field'): 1,\n",
              "          ('random', 'field', 'for'): 1,\n",
              "          ('field', 'for', 'natural'): 1,\n",
              "          ('for', 'natural', 'language'): 4,\n",
              "          ('language', 'processing', 'Terms'): 1,\n",
              "          ('processing', 'Terms', 'and'): 1,\n",
              "          ('Terms', 'and', 'Conditions'): 2,\n",
              "          ('and', 'Conditions', ':'): 2,\n",
              "          ('Conditions', ':', 'Terms'): 1,\n",
              "          (':', 'Terms', 'and'): 1,\n",
              "          ('Conditions', ':', 'Copyright'): 1,\n",
              "          (':', 'Copyright', 'in'): 1,\n",
              "          ('Copyright', 'in', 'work'): 1,\n",
              "          ('in', 'work', 'deposited'): 1,\n",
              "          ('work', 'deposited', 'in'): 1,\n",
              "          ('deposited', 'in', 'Minerva'): 1,\n",
              "          ('in', 'Minerva', 'Access'): 1,\n",
              "          ('Minerva', 'Access', 'is'): 1,\n",
              "          ('Access', 'is', 'retained'): 1,\n",
              "          ('is', 'retained', 'by'): 1,\n",
              "          ('retained', 'by', 'the'): 1,\n",
              "          ('by', 'the', 'The'): 1,\n",
              "          ('the', 'The', 'paper'): 1,\n",
              "          ('The', 'paper', 'address'): 1,\n",
              "          ('paper', 'address', 'the'): 1,\n",
              "          ('address', 'the', 'issue'): 1,\n",
              "          ('the', 'issue', 'of'): 1,\n",
              "          ('issue', 'of', 'cooperation'): 1,\n",
              "          ('of', 'cooperation', 'between'): 1,\n",
              "          ('cooperation', 'between', 'linguistics'): 1,\n",
              "          ('between', 'linguistics', 'and'): 2,\n",
              "          ('linguistics', 'and', 'natural'): 1,\n",
              "          ('and', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', '('): 11,\n",
              "          ('processing', '(', 'NLP'): 10,\n",
              "          ('(', 'NLP', ')'): 21,\n",
              "          ('NLP', ')', ','): 3,\n",
              "          (')', ',', 'in'): 2,\n",
              "          (',', 'in', 'general'): 1,\n",
              "          ('in', 'general', ','): 1,\n",
              "          ('general', ',', 'and'): 1,\n",
              "          (',', 'and', 'between'): 1,\n",
              "          ('and', 'between', 'linguistics'): 1,\n",
              "          ('linguistics', 'and', 'machine'): 1,\n",
              "          ('and', 'machine', 'translation'): 1,\n",
              "          ('machine', 'translation', '('): 1,\n",
              "          ('translation', '(', 'MT'): 1,\n",
              "          ('(', 'MT', ')'): 1,\n",
              "          ('MT', ')', ','): 1,\n",
              "          (',', 'in', 'particular'): 2,\n",
              "          ('in', 'particular', '.'): 1,\n",
              "          ('particular', '.', 'It'): 1,\n",
              "          ('.', 'It', 'focus'): 1,\n",
              "          ('It', 'focus', 'on'): 1,\n",
              "          ('focus', 'on', 'just'): 1,\n",
              "          ('on', 'just', 'one'): 1,\n",
              "          ('just', 'one', 'direction'): 1,\n",
              "          ('one', 'direction', 'of'): 1,\n",
              "          ('direction', 'of', 'such'): 1,\n",
              "          ('of', 'such', 'cooperation'): 1,\n",
              "          ('such', 'cooperation', ','): 1,\n",
              "          ('cooperation', ',', 'namely'): 1,\n",
              "          (',', 'namely', 'application'): 1,\n",
              "          ('namely', 'application', 'of'): 1,\n",
              "          ('application', 'of', 'linguistics'): 1,\n",
              "          ('of', 'linguistics', 'to'): 1,\n",
              "          ('linguistics', 'to', 'NLP'): 1,\n",
              "          ('to', 'NLP', ','): 1,\n",
              "          ('NLP', ',', 'virtually'): 1,\n",
              "          (',', 'virtually', 'In'): 1,\n",
              "          ('virtually', 'In', 'most'): 1,\n",
              "          ('In', 'most', 'natural'): 1,\n",
              "          ('most', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'applications'): 1,\n",
              "          ('processing', 'applications', ','): 1,\n",
              "          ('applications', ',', 'Description'): 1,\n",
              "          (',', 'Description', 'Logics'): 2,\n",
              "          ('Description', 'Logics', 'have'): 2,\n",
              "          ('Logics', 'have', 'been'): 2,\n",
              "          ('have', 'been', 'used'): 2,\n",
              "          ('been', 'used', 'to'): 1,\n",
              "          ('used', 'to', 'encode'): 1,\n",
              "          ('to', 'encode', 'in'): 1,\n",
              "          ('encode', 'in', 'a'): 1,\n",
              "          ('in', 'a', 'knowledge'): 1,\n",
              "          ('a', 'knowledge', 'base'): 2,\n",
              "          ('knowledge', 'base', 'some'): 1,\n",
              "          ('base', 'some', 'syntactic'): 1,\n",
              "          ('some', 'syntactic', ','): 1,\n",
              "          ('syntactic', ',', 'semantic'): 1,\n",
              "          (',', 'semantic', ','): 1,\n",
              "          ('semantic', ',', 'and'): 1,\n",
              "          (',', 'and', 'pragmatic'): 1,\n",
              "          ('and', 'pragmatic', 'element'): 1,\n",
              "          ('pragmatic', 'element', 'needed'): 1,\n",
              "          ('element', 'needed', 'to'): 1,\n",
              "          ('needed', 'to', 'drive'): 1,\n",
              "          ('to', 'drive', 'the'): 2,\n",
              "          ('drive', 'the', 'semantic'): 1,\n",
              "          ('the', 'semantic', 'interpretation'): 1,\n",
              "          ('semantic', 'interpretation', 'and'): 1,\n",
              "          ('interpretation', 'and', 'the'): 1,\n",
              "          ('and', 'the', 'natural'): 1,\n",
              "          ('the', 'natural', 'language'): 1,\n",
              "          ('natural', 'language', 'generation'): 1,\n",
              "          ('language', 'generation', 'processes'): 1,\n",
              "          ('generation', 'processes', '.'): 1,\n",
              "          ('processes', '.', 'More'): 1,\n",
              "          ('.', 'More', 'recently'): 1,\n",
              "          ('More', 'recently', ','): 1,\n",
              "          ('recently', ',', 'Description'): 1,\n",
              "          ('have', 'been', 'We'): 1,\n",
              "          ('been', 'We', 'propose'): 1,\n",
              "          ('We', 'propose', 'a'): 2,\n",
              "          ('propose', 'a', 'unified'): 1,\n",
              "          ('a', 'unified', 'neural'): 1,\n",
              "          ('unified', 'neural', 'network'): 1,\n",
              "          ('neural', 'network', 'architecture'): 2,\n",
              "          ('network', 'architecture', 'and'): 1,\n",
              "          ('architecture', 'and', 'learning'): 1,\n",
              "          ('and', 'learning', 'algorithm'): 1,\n",
              "          ('learning', 'algorithm', 'that'): 1,\n",
              "          ('algorithm', 'that', 'can'): 1,\n",
              "          ('that', 'can', 'be'): 1,\n",
              "          ('can', 'be', 'applied'): 1,\n",
              "          ('be', 'applied', 'to'): 1,\n",
              "          ('applied', 'to', 'various'): 1,\n",
              "          ('to', 'various', 'natural'): 1,\n",
              "          ('various', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'task'): 1,\n",
              "          ('processing', 'task', 'including'): 1,\n",
              "          ('task', 'including', 'part-of-speech'): 1,\n",
              "          ('including', 'part-of-speech', 'tagging'): 1,\n",
              "          ('part-of-speech', 'tagging', ','): 1,\n",
              "          ('tagging', ',', 'chunking'): 1,\n",
              "          (',', 'chunking', ','): 1,\n",
              "          ('chunking', ',', 'named'): 1,\n",
              "          (',', 'named', 'entity'): 2,\n",
              "          ('named', 'entity', 'recognition'): 1,\n",
              "          ('entity', 'recognition', ','): 1,\n",
              "          ('recognition', ',', 'and'): 2,\n",
              "          (',', 'and', 'semantic'): 1,\n",
              "          ('and', 'semantic', 'role'): 1,\n",
              "          ('semantic', 'role', 'labeling'): 1,\n",
              "          ('role', 'labeling', '.'): 1,\n",
              "          ('labeling', '.', 'This'): 1,\n",
              "          ('.', 'This', 'versatility'): 1,\n",
              "          ('This', 'versatility', 'is'): 1,\n",
              "          ('versatility', 'is', 'achieved'): 1,\n",
              "          ('is', 'achieved', 'by'): 1,\n",
              "          ('achieved', 'by', 'trying'): 1,\n",
              "          ('by', 'trying', 'to'): 1,\n",
              "          ('trying', 'to', 'avoid'): 1,\n",
              "          ('to', 'avoid', 'task'): 1,\n",
              "          ('avoid', 'task', 'Natural'): 1,\n",
              "          ('task', 'Natural', 'Language'): 1,\n",
              "          ('Natural', 'Language', 'Processing'): 19,\n",
              "          ('Language', 'Processing', 'The'): 1,\n",
              "          ('Processing', 'The', 'subject'): 1,\n",
              "          ('The', 'subject', 'of'): 1,\n",
              "          ('subject', 'of', 'Natural'): 1,\n",
              "          ('of', 'Natural', 'Language'): 3,\n",
              "          ('Language', 'Processing', 'can'): 1,\n",
              "          ('Processing', 'can', 'be'): 1,\n",
              "          ('can', 'be', 'considered'): 1,\n",
              "          ('be', 'considered', 'in'): 1,\n",
              "          ('considered', 'in', 'both'): 1,\n",
              "          ('in', 'both', 'broad'): 1,\n",
              "          ('both', 'broad', 'and'): 1,\n",
              "          ('broad', 'and', 'narrow'): 1,\n",
              "          ('and', 'narrow', 'senses'): 1,\n",
              "          ('narrow', 'senses', '.'): 1,\n",
              "          ('senses', '.', 'In'): 1,\n",
              "          ('.', 'In', 'the'): 2,\n",
              "          ('In', 'the', 'broad'): 1,\n",
              "          ('the', 'broad', 'sense'): 1,\n",
              "          ('broad', 'sense', ','): 1,\n",
              "          ('sense', ',', 'it'): 1,\n",
              "          (',', 'it', 'cover'): 1,\n",
              "          ('it', 'cover', 'processing'): 1,\n",
              "          ('cover', 'processing', 'issue'): 1,\n",
              "          ('processing', 'issue', 'at'): 1,\n",
              "          ('issue', 'at', 'all'): 1,\n",
              "          ('at', 'all', 'level'): 1,\n",
              "          ('all', 'level', 'of'): 1,\n",
              "          ('level', 'of', 'natural'): 1,\n",
              "          ('of', 'natural', 'language'): 12,\n",
              "          ('natural', 'language', 'understanding'): 1,\n",
              "          ('language', 'understanding', ','): 1,\n",
              "          ('understanding', ',', 'including'): 1,\n",
              "          (',', 'including', 'speech'): 1,\n",
              "          ('including', 'speech', 'recognition'): 1,\n",
              "          ('speech', 'recognition', ','): 1,\n",
              "          ('recognition', ',', 'syntactic'): 1,\n",
              "          (',', 'syntactic', 'and'): 1,\n",
              "          ('syntactic', 'and', 'semantic'): 1,\n",
              "          ('and', 'semantic', 'analysis'): 1,\n",
              "          ('semantic', 'analysis', 'of'): 1,\n",
              "          ('analysis', 'of', 'sentence'): 1,\n",
              "          ('of', 'sentence', 'Robots'): 1,\n",
              "          ('sentence', 'Robots', 'that'): 1,\n",
              "          ('Robots', 'that', 'interact'): 1,\n",
              "          ('that', 'interact', 'with'): 1,\n",
              "          ('interact', 'with', 'human'): 1,\n",
              "          ('with', 'human', 'face-to-face'): 1,\n",
              "          ('human', 'face-to-face', 'using'): 1,\n",
              "          ('face-to-face', 'using', 'natural'): 1,\n",
              "          ('using', 'natural', 'language'): 2,\n",
              "          ('natural', 'language', 'need'): 1,\n",
              "          ('language', 'need', 'to'): 1,\n",
              "          ('need', 'to', 'be'): 2,\n",
              "          ('to', 'be', 'responsive'): 1,\n",
              "          ('be', 'responsive', 'to'): 1,\n",
              "          ('responsive', 'to', 'the'): 1,\n",
              "          ('to', 'the', 'way'): 2,\n",
              "          ('the', 'way', 'human'): 2,\n",
              "          ('way', 'human', 'use'): 1,\n",
              "          ('human', 'use', 'language'): 1,\n",
              "          ('use', 'language', 'in'): 1,\n",
              "          ('language', 'in', 'those'): 1,\n",
              "          ('in', 'those', 'situations'): 1,\n",
              "          ('those', 'situations', '.'): 1,\n",
              "          ('situations', '.', 'We'): 1,\n",
              "          ('.', 'We', 'propose'): 1,\n",
              "          ('propose', 'a', 'psychologicallyinspired'): 1,\n",
              "          ('a', 'psychologicallyinspired', 'natural'): 1,\n",
              "          ('psychologicallyinspired', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'system'): 3,\n",
              "          ('processing', 'system', 'for'): 1,\n",
              "          ('system', 'for', 'robot'): 1,\n",
              "          ('for', 'robot', 'which'): 1,\n",
              "          ('robot', 'which', 'performs'): 1,\n",
              "          ('which', 'performs', 'incremental'): 1,\n",
              "          ('performs', 'incremental', 'semantic'): 1,\n",
              "          ('incremental', 'semantic', 'interpretation'): 1,\n",
              "          ('semantic', 'interpretation', 'of'): 1,\n",
              "          ('interpretation', 'of', 'spoken'): 1,\n",
              "          ('of', 'spoken', 'utterance'): 1,\n",
              "          ('spoken', 'utterance', 'Natural'): 1,\n",
              "          ('utterance', 'Natural', 'language'): 1,\n",
              "          ('Natural', 'language', 'are'): 3,\n",
              "          ('language', 'are', 'language'): 1,\n",
              "          ('are', 'language', 'spoken'): 1,\n",
              "          ('language', 'spoken', 'by'): 1,\n",
              "          ('spoken', 'by', 'humans'): 1,\n",
              "          ('by', 'humans', '.'): 1,\n",
              "          ('humans', '.', 'Currently'): 1,\n",
              "          ('.', 'Currently', 'we'): 1,\n",
              "          ('Currently', 'we', 'are'): 1,\n",
              "          ('we', 'are', 'not'): 1,\n",
              "          ('are', 'not', 'yet'): 1,\n",
              "          ('not', 'yet', 'at'): 1,\n",
              "          ('yet', 'at', 'the'): 1,\n",
              "          ('at', 'the', 'point'): 1,\n",
              "          ('the', 'point', 'where'): 1,\n",
              "          ('point', 'where', 'these'): 1,\n",
              "          ('where', 'these', 'language'): 1,\n",
              "          ('these', 'language', 'in'): 1,\n",
              "          ('language', 'in', 'all'): 1,\n",
              "          ('in', 'all', 'of'): 1,\n",
              "          ('all', 'of', 'their'): 1,\n",
              "          ('of', 'their', 'unprocessed'): 1,\n",
              "          ('their', 'unprocessed', 'form'): 1,\n",
              "          ('unprocessed', 'form', 'can'): 1,\n",
              "          ('form', 'can', 'be'): 1,\n",
              "          ('can', 'be', 'understood'): 1,\n",
              "          ('be', 'understood', 'by'): 1,\n",
              "          ('understood', 'by', 'computers'): 1,\n",
              "          ('by', 'computers', '.'): 1,\n",
              "          ('computers', '.', 'Natural'): 1,\n",
              "          ('.', 'Natural', 'language'): 5,\n",
              "          ('Natural', 'language', 'processing'): 4,\n",
              "          ('language', 'processing', 'is'): 3,\n",
              "          ('processing', 'is', 'the'): 3,\n",
              "          ('is', 'the', 'collection'): 1,\n",
              "          ('the', 'collection', 'of'): 1,\n",
              "          ('collection', 'of', 'technique'): 1,\n",
              "          ('of', 'technique', 'employed'): 1,\n",
              "          ('technique', 'employed', 'to'): 1,\n",
              "          ('employed', 'to', 'try'): 1,\n",
              "          ('to', 'try', 'and'): 1,\n",
              "          ('try', 'and', 'accomplish'): 1,\n",
              "          ('and', 'accomplish', 'that'): 1,\n",
              "          ('accomplish', 'that', 'goal'): 1,\n",
              "          ('that', 'goal', '.'): 1,\n",
              "          ('goal', '.', 'The'): 1,\n",
              "          ('.', 'The', 'field'): 1,\n",
              "          ('The', 'field', 'of'): 1,\n",
              "          ('field', 'of', 'natural'): 1,\n",
              "          ('of', 'natural', 'ABSTRACT'): 1,\n",
              "          ('natural', 'ABSTRACT', ':'): 1,\n",
              "          ('ABSTRACT', ':', 'Ambiguity'): 1,\n",
              "          (':', 'Ambiguity', 'can'): 1,\n",
              "          ('Ambiguity', 'can', 'be'): 1,\n",
              "          ('can', 'be', 'referred'): 1,\n",
              "          ('be', 'referred', 'a'): 1,\n",
              "          ('referred', 'a', 'the'): 1,\n",
              "          ('a', 'the', 'ability'): 1,\n",
              "          ('the', 'ability', 'of'): 1,\n",
              "          ('ability', 'of', 'having'): 1,\n",
              "          ('of', 'having', 'more'): 1,\n",
              "          ('having', 'more', 'than'): 1,\n",
              "          ('more', 'than', 'one'): 2,\n",
              "          ('than', 'one', 'meaning'): 1,\n",
              "          ('one', 'meaning', 'or'): 1,\n",
              "          ('meaning', 'or', 'being'): 1,\n",
              "          ('or', 'being', 'understood'): 1,\n",
              "          ('being', 'understood', 'in'): 1,\n",
              "          ('understood', 'in', 'more'): 1,\n",
              "          ('in', 'more', 'than'): 1,\n",
              "          ('than', 'one', 'way'): 1,\n",
              "          ('one', 'way', '.'): 1,\n",
              "          ('way', '.', 'Natural'): 1,\n",
              "          ('language', 'are', 'ambiguous'): 1,\n",
              "          ('are', 'ambiguous', ','): 1,\n",
              "          ('ambiguous', ',', 'so'): 1,\n",
              "          (',', 'so', 'computer'): 1,\n",
              "          ('so', 'computer', 'are'): 1,\n",
              "          ('computer', 'are', 'not'): 1,\n",
              "          ('are', 'not', 'able'): 1,\n",
              "          ('not', 'able', 'to'): 1,\n",
              "          ('able', 'to', 'understand'): 1,\n",
              "          ('to', 'understand', 'language'): 1,\n",
              "          ('understand', 'language', 'the'): 1,\n",
              "          ('language', 'the', 'way'): 1,\n",
              "          ('the', 'way', 'people'): 1,\n",
              "          ('way', 'people', 'do'): 1,\n",
              "          ('people', 'do', '.'): 1,\n",
              "          ('do', '.', 'Natural'): 1,\n",
              "          ('.', 'Natural', 'Language'): 2,\n",
              "          ('Language', 'Processing', '('): 11,\n",
              "          ('Processing', '(', 'NLP'): 11,\n",
              "          ('NLP', ')', 'is'): 5,\n",
              "          (')', 'is', 'concerned'): 1,\n",
              "          ('is', 'concerned', 'with'): 1,\n",
              "          ('concerned', 'with', 'the'): 2,\n",
              "          ('with', 'the', 'development'): 1,\n",
              "          ('the', 'development', 'Introduction'): 1,\n",
              "          ('development', 'Introduction', 'Statistical'): 1,\n",
              "          ('Introduction', 'Statistical', 'natural'): 1,\n",
              "          ('Statistical', 'natural', 'language'): 1,\n",
              "          ('processing', '(', 'SNLP'): 1,\n",
              "          ('(', 'SNLP', ')'): 1,\n",
              "          ('SNLP', ')', 'is'): 1,\n",
              "          (')', 'is', 'a'): 2,\n",
              "          ('is', 'a', 'field'): 1,\n",
              "          ('a', 'field', 'lying'): 1,\n",
              "          ('field', 'lying', 'in'): 1,\n",
              "          ('lying', 'in', 'the'): 1,\n",
              "          ('in', 'the', 'intersection'): 1,\n",
              "          ('the', 'intersection', 'of'): 1,\n",
              "          ('intersection', 'of', 'natural'): 1,\n",
              "          ('language', 'processing', 'and'): 1,\n",
              "          ('processing', 'and', 'machine'): 1,\n",
              "          ('and', 'machine', 'learning'): 3,\n",
              "          ('machine', 'learning', '.'): 2,\n",
              "          ('learning', '.', 'SNLP'): 1,\n",
              "          ('.', 'SNLP', 'di'): 1,\n",
              "          ('SNLP', 'di', '#'): 1,\n",
              "          ('di', '#', 'ers'): 1,\n",
              "          ('#', 'ers', 'from'): 1,\n",
              "          ('ers', 'from', 'traditional'): 1,\n",
              "          ('from', 'traditional', 'natural'): 1,\n",
              "          ('traditional', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'in'): 1,\n",
              "          ('processing', 'in', 'that'): 1,\n",
              "          ('in', 'that', 'instead'): 1,\n",
              "          ('that', 'instead', 'of'): 1,\n",
              "          ('instead', 'of', 'having'): 1,\n",
              "          ('of', 'having', 'a'): 1,\n",
              "          ('having', 'a', 'linguist'): 1,\n",
              "          ('a', 'linguist', 'manually'): 1,\n",
              "          ('linguist', 'manually', 'construct'): 1,\n",
              "          ('manually', 'construct', 'some'): 1,\n",
              "          ('construct', 'some', 'model'): 1,\n",
              "          ('some', 'model', 'of'): 1,\n",
              "          ('model', 'of', 'a'): 1,\n",
              "          ('of', 'a', 'given'): 1,\n",
              "          ('a', 'given', 'linguistic'): 1,\n",
              "          ('given', 'linguistic', 'text'): 1,\n",
              "          ('linguistic', 'text', 'directly'): 1,\n",
              "          ('text', 'directly', '('): 1,\n",
              "          ('directly', '(', 'rather'): 1,\n",
              "          ('(', 'rather', 'than'): 1,\n",
              "          ('rather', 'than', 'e.g'): 1,\n",
              "          ('than', 'e.g', '.'): 1,\n",
              "          ('e.g', '.', 'title'): 1,\n",
              "          ('.', 'title', 'and'): 1,\n",
              "          ('title', 'and', 'abstracts'): 1,\n",
              "          ('and', 'abstracts', ')'): 1,\n",
              "          ('abstracts', ')', ','): 1,\n",
              "          (')', ',', 'and'): 1,\n",
              "          (',', 'and', 'suggests'): 1,\n",
              "          ('and', 'suggests', 'appropriate'): 1,\n",
              "          ('suggests', 'appropriate', 'approach'): 1,\n",
              "          ('appropriate', 'approach', 'to'): 1,\n",
              "          ('approach', 'to', 'doing'): 1,\n",
              "          ('to', 'doing', 'this'): 1,\n",
              "          ('doing', 'this', ','): 1,\n",
              "          ('this', ',', 'with'): 1,\n",
              "          (',', 'with', 'a'): 1,\n",
              "          ('with', 'a', 'focus'): 3,\n",
              "          ('a', 'focus', 'on'): 3,\n",
              "          ('focus', 'on', 'the'): 2,\n",
              "          ('on', 'the', 'role'): 1,\n",
              "          ('the', 'role', 'of'): 2,\n",
              "          ('role', 'of', 'natural'): 1,\n",
              "          ('processing', '.', 'The'): 2,\n",
              "          ('.', 'The', 'paper'): 1,\n",
              "          ('The', 'paper', 'also'): 1,\n",
              "          ('paper', 'also', 'comment'): 1,\n",
              "          ('also', 'comment', 'on'): 1,\n",
              "          ('comment', 'on', 'possible'): 1,\n",
              "          ('on', 'possible', 'connection'): 1,\n",
              "          ('possible', 'connection', 'with'): 1,\n",
              "          ('connection', 'with', 'data'): 1,\n",
              "          ('with', 'data', 'and'): 1,\n",
              "          ('data', 'and', 'knowledge'): 1,\n",
              "          ('and', 'knowledge', 'retrieval'): 1,\n",
              "          ('knowledge', 'retrieval', ','): 1,\n",
              "          ('retrieval', ',', 'and'): 1,\n",
              "          (',', 'and', 'concludes'): 1,\n",
              "          ('and', 'concludes', 'by'): 1,\n",
              "          ('concludes', 'by', 'emphasizing'): 1,\n",
              "          ('by', 'emphasizing', 'the'): 1,\n",
              "          ('emphasizing', 'the', 'importance'): 1,\n",
              "          ('the', 'importance', 'of'): 1,\n",
              "          ('importance', 'of', 'rigorous'): 1,\n",
              "          ('of', 'rigorous', 'ABSTRACT'): 1,\n",
              "          ('rigorous', 'ABSTRACT', ':'): 1,\n",
              "          ('ABSTRACT', ':', 'Language'): 1,\n",
              "          (':', 'Language', 'is'): 1,\n",
              "          ('Language', 'is', 'way'): 1,\n",
              "          ('is', 'way', 'of'): 1,\n",
              "          ('way', 'of', 'communicating'): 1,\n",
              "          ('of', 'communicating', 'your'): 1,\n",
              "          ('communicating', 'your', 'word'): 1,\n",
              "          ('your', 'word', 'Language'): 1,\n",
              "          ('word', 'Language', 'help'): 1,\n",
              "          ('Language', 'help', 'in'): 1,\n",
              "          ('help', 'in', 'understanding'): 1,\n",
              "          ('in', 'understanding', 'the'): 1,\n",
              "          ('understanding', 'the', 'world'): 1,\n",
              "          ('the', 'world', ','): 1,\n",
              "          ('world', ',', 'we'): 1,\n",
              "          (',', 'we', 'get'): 1,\n",
              "          ('we', 'get', 'a'): 1,\n",
              "          ('get', 'a', 'better'): 1,\n",
              "          ('a', 'better', 'insight'): 1,\n",
              "          ('better', 'insight', 'of'): 1,\n",
              "          ('insight', 'of', 'the'): 1,\n",
              "          ('of', 'the', 'world'): 1,\n",
              "          ('the', 'world', '.'): 1,\n",
              "          ('world', '.', 'Language'): 1,\n",
              "          ('.', 'Language', 'help'): 1,\n",
              "          ('Language', 'help', 'speaker'): 1,\n",
              "          ('help', 'speaker', 'to'): 1,\n",
              "          ('speaker', 'to', 'be'): 1,\n",
              "          ('to', 'be', 'a'): 1,\n",
              "          ('be', 'a', 'vague'): 1,\n",
              "          ('a', 'vague', 'or'): 1,\n",
              "          ('vague', 'or', 'a'): 1,\n",
              "          ('or', 'a', 'precise'): 1,\n",
              "          ('a', 'precise', 'a'): 1,\n",
              "          ('precise', 'a', 'they'): 1,\n",
              "          ('a', 'they', 'like'): 1,\n",
              "          ('they', 'like', '.'): 1,\n",
              "          ('like', '.', 'NLP'): 1,\n",
              "          ('.', 'NLP', 'Stands'): 1,\n",
              "          ('NLP', 'Stands', 'for'): 1,\n",
              "          ('Stands', 'for', 'natural'): 1,\n",
              "          ('natural', 'language', 'processing..'): 1,\n",
              "          ('language', 'processing..', 'Natural'): 1,\n",
              "          ('processing..', 'Natural', 'language'): 1,\n",
              "          ('language', 'are', 'those'): 1,\n",
              "          ('are', 'those', 'language'): 1,\n",
              "          ('those', 'language', 'that'): 1,\n",
              "          ('language', 'that', 'are'): 1,\n",
              "          ('that', 'are', 'spoken'): 1,\n",
              "          ('are', 'spoken', 'We'): 1,\n",
              "          ('spoken', 'We', 'report'): 1,\n",
              "          ('We', 'report', 'experiment'): 1,\n",
              "          ('report', 'experiment', 'on'): 1,\n",
              "          ('experiment', 'on', 'the'): 1,\n",
              "          ('on', 'the', 'use'): 1,\n",
              "          ('the', 'use', 'of'): 3,\n",
              "          ('use', 'of', 'standard'): 1,\n",
              "          ('of', 'standard', 'natural'): 1,\n",
              "          ('standard', 'natural', 'language'): 1,\n",
              "          ('NLP', ')', 'tool'): 1,\n",
              "          (')', 'tool', 'for'): 1,\n",
              "          ('tool', 'for', 'the'): 1,\n",
              "          ('for', 'the', 'analysis'): 1,\n",
              "          ('the', 'analysis', 'of'): 1,\n",
              "          ('analysis', 'of', 'music'): 1,\n",
              "          ('of', 'music', 'lyrics'): 1,\n",
              "          ('music', 'lyrics', '.'): 1,\n",
              "          ('lyrics', '.', 'A'): 1,\n",
              "          ('.', 'A', 'significant'): 1,\n",
              "          ('A', 'significant', 'amount'): 1,\n",
              "          ('significant', 'amount', 'of'): 1,\n",
              "          ('amount', 'of', 'music'): 1,\n",
              "          ('of', 'music', 'audio'): 1,\n",
              "          ('music', 'audio', 'ha'): 1,\n",
              "          ('audio', 'ha', 'lyrics'): 1,\n",
              "          ('ha', 'lyrics', '.'): 1,\n",
              "          ('lyrics', '.', 'Lyrics'): 1,\n",
              "          ('.', 'Lyrics', 'encode'): 1,\n",
              "          ('Lyrics', 'encode', 'an'): 1,\n",
              "          ('encode', 'an', 'important'): 1,\n",
              "          ('an', 'important', 'part'): 1,\n",
              "          ('important', 'part', 'of'): 1,\n",
              "          ('part', 'of', 'the'): 1,\n",
              "          ('of', 'the', 'semantics'): 1,\n",
              "          ('the', 'semantics', 'of'): 1,\n",
              "          ('semantics', 'of', 'a'): 1,\n",
              "          ('of', 'a', 'song'): 1,\n",
              "          ('a', 'song', ','): 1,\n",
              "          ('song', ',', 'therefore'): 1,\n",
              "          (',', 'therefore', 'their'): 1,\n",
              "          ('therefore', 'their', 'analysis'): 1,\n",
              "          ('their', 'analysis', 'complement'): 1,\n",
              "          ('analysis', 'complement', 'that'): 1,\n",
              "          ('complement', 'that', 'of'): 1,\n",
              "          ('that', 'of', 'acoustic'): 1,\n",
              "          ('of', 'acoustic', 'and'): 1,\n",
              "          ('acoustic', 'and', 'cultural'): 1,\n",
              "          ('and', 'cultural', 'this'): 1,\n",
              "          ('cultural', 'this', 'paper'): 1,\n",
              "          ('this', 'paper', ','): 5,\n",
              "          ('paper', ',', 'we'): 4,\n",
              "          (',', 'we', 'will'): 1,\n",
              "          ('we', 'will', 'describe'): 1,\n",
              "          ('will', 'describe', 'a'): 1,\n",
              "          ('describe', 'a', 'simple'): 1,\n",
              "          ('a', 'simple', 'rule-based'): 1,\n",
              "          ('simple', 'rule-based', 'approach'): 1,\n",
              "          ('rule-based', 'approach', 'to'): 1,\n",
              "          ('approach', 'to', 'automated'): 1,\n",
              "          ('to', 'automated', 'learning'): 1,\n",
              "          ('automated', 'learning', 'of'): 1,\n",
              "          ('learning', 'of', 'linguistic'): 1,\n",
              "          ('of', 'linguistic', 'knowledge'): 2,\n",
              "          ('linguistic', 'knowledge', '.'): 1,\n",
              "          ('knowledge', '.', 'This'): 1,\n",
              "          ('.', 'This', 'approach'): 1,\n",
              "          ('This', 'approach', 'ha'): 1,\n",
              "          ('approach', 'ha', 'been'): 1,\n",
              "          ('ha', 'been', 'shown'): 1,\n",
              "          ('been', 'shown', 'for'): 1,\n",
              "          ('shown', 'for', 'a'): 1,\n",
              "          ('for', 'a', 'number'): 1,\n",
              "          ('a', 'number', 'of'): 3,\n",
              "          ('number', 'of', 'task'): 2,\n",
              "          ('of', 'task', 'to'): 1,\n",
              "          ('task', 'to', 'capture'): 1,\n",
              "          ('to', 'capture', 'information'): 1,\n",
              "          ('capture', 'information', 'in'): 1,\n",
              "          ('information', 'in', 'a'): 1,\n",
              "          ('in', 'a', 'clearer'): 1,\n",
              "          ('a', 'clearer', 'and'): 1,\n",
              "          ('clearer', 'and', 'more'): 1,\n",
              "          ('and', 'more', 'direct'): 1,\n",
              "          ('more', 'direct', 'fashion'): 1,\n",
              "          ('direct', 'fashion', 'without'): 1,\n",
              "          ('fashion', 'without', 'a'): 1,\n",
              "          ('without', 'a', 'compromise'): 1,\n",
              "          ('a', 'compromise', 'in'): 1,\n",
              "          ('compromise', 'in', 'performance'): 1,\n",
              "          ('in', 'performance', '.'): 1,\n",
              "          ('performance', '.', 'We'): 1,\n",
              "          ('present', 'a', 'detailed'): 1,\n",
              "          ('a', 'detailed', 'case'): 1,\n",
              "          ('detailed', 'case', 'study'): 1,\n",
              "          ('case', 'study', 'of'): 1,\n",
              "          ('study', 'of', 'this'): 1,\n",
              "          ('of', 'this', 'learning'): 1,\n",
              "          ('this', 'learning', 'method'): 1,\n",
              "          ('learning', 'method', 'applied'): 1,\n",
              "          ('method', 'applied', 'to'): 1,\n",
              "          ('applied', 'to', 'part'): 1,\n",
              "          ('to', 'part', 'of'): 1,\n",
              "          ('part', 'of', 'speech'): 1,\n",
              "          ('of', 'speech', 'tagging'): 1,\n",
              "          ('speech', 'tagging', 'This'): 1,\n",
              "          ('tagging', 'This', 'paper'): 1,\n",
              "          ('This', 'paper', 'focus'): 1,\n",
              "          ('paper', 'focus', 'on'): 1,\n",
              "          ('focus', 'on', 'connectionist'): 1,\n",
              "          ('on', 'connectionist', 'model'): 1,\n",
              "          ('connectionist', 'model', 'in'): 1,\n",
              "          ('model', 'in', 'natural'): 1,\n",
              "          ('processing', '.', 'We'): 3,\n",
              "          ('.', 'We', 'briefly'): 1,\n",
              "          ('We', 'briefly', 'present'): 1,\n",
              "          ('briefly', 'present', 'and'): 1,\n",
              "          ('present', 'and', 'discus'): 1,\n",
              "          ('and', 'discus', 'several'): 1,\n",
              "          ('discus', 'several', 'aspect'): 1,\n",
              "          ('several', 'aspect', 'of'): 1,\n",
              "          ('aspect', 'of', 'high'): 1,\n",
              "          ('of', 'high', 'level'): 1,\n",
              "          ('high', 'level', 'task'): 1,\n",
              "          ('level', 'task', 'which'): 1,\n",
              "          ('task', 'which', 'recently'): 1,\n",
              "          ('which', 'recently', 'have'): 1,\n",
              "          ('recently', 'have', 'been'): 1,\n",
              "          ('have', 'been', 'approached'): 1,\n",
              "          ('been', 'approached', 'with'): 1,\n",
              "          ('approached', 'with', 'connectionism'): 1,\n",
              "          ('with', 'connectionism', ','): 1,\n",
              "          ('connectionism', ',', 'either'): 1,\n",
              "          (',', 'either', 'with'): 1,\n",
              "          ('either', 'with', 'localist'): 1,\n",
              "          ('with', 'localist', 'or'): 1,\n",
              "          ('localist', 'or', 'parallel'): 1,\n",
              "          ('or', 'parallel', 'distributed'): 1,\n",
              "          ('parallel', 'distributed', 'processing'): 1,\n",
              "          ('distributed', 'processing', 'models'): 1,\n",
              "          ('processing', 'models', '.'): 1,\n",
              "          ('models', '.', 'Several'): 1,\n",
              "          ('.', 'Several', 'interesting'): 1,\n",
              "          ('Several', 'interesting', 'architecture'): 1,\n",
              "          ('interesting', 'architecture', 'process'): 1,\n",
              "          ('architecture', 'process', 'of'): 1,\n",
              "          ('process', 'of', 'language'): 1,\n",
              "          ('of', 'language', 'understanding'): 1,\n",
              "          ('language', 'understanding', '.'): 1,\n",
              "          ('understanding', '.', 'This'): 1,\n",
              "          ('.', 'This', 'is'): 1,\n",
              "          ('This', 'is', 'a'): 1,\n",
              "          ('is', 'a', 'new'): 1,\n",
              "          ('a', 'new', 'approach'): 1,\n",
              "          ('new', 'approach', 'in'): 1,\n",
              "          ('approach', 'in', 'natural'): 1,\n",
              "          ('language', 'processing', 'based'): 1,\n",
              "          ('processing', 'based', 'on'): 1,\n",
              "          ('based', 'on', 'the'): 2,\n",
              "          ('on', 'the', 'deterministic'): 1,\n",
              "          ('the', 'deterministic', 'chaotic'): 1,\n",
              "          ('deterministic', 'chaotic', 'behavior'): 1,\n",
              "          ('chaotic', 'behavior', 'of'): 1,\n",
              "          ('behavior', 'of', 'dynamical'): 1,\n",
              "          ('of', 'dynamical', 'systems'): 1,\n",
              "          ('dynamical', 'systems', '.'): 1,\n",
              "          ('systems', '.', '1'): 1,\n",
              "          ('.', '1', 'this'): 2,\n",
              "          ('1', 'this', 'paper'): 2,\n",
              "          ('this', 'paper', '('): 1,\n",
              "          ('paper', '(', 'see'): 1,\n",
              "          ('(', 'see', '['): 1,\n",
              "          ('see', '[', 'Schank'): 1,\n",
              "          ('[', 'Schank', '86'): 1,\n",
              "          ('Schank', '86', ']'): 1,\n",
              "          ('86', ']', 'for'): 2,\n",
              "          (']', 'for', 'a'): 1,\n",
              "          ('for', 'a', 'theoretical'): 1,\n",
              "          ('a', 'theoretical', 'discussion'): 1,\n",
              "          ('theoretical', 'discussion', 'and'): 1,\n",
              "          ('discussion', 'and', '['): 1,\n",
              "          ('and', '[', 'Kass'): 1,\n",
              "          ('[', 'Kass', '86'): 1,\n",
              "          ('Kass', '86', ']'): 1,\n",
              "          ('86', ']', 'and'): 1,\n",
              "          (']', 'and', '['): 1,\n",
              "          ('and', '[', 'Leake'): 1,\n",
              "          ('[', 'Leake', 'and'): 1,\n",
              "          ('Leake', 'and', 'Owens'): 1,\n",
              "          ('and', 'Owens', '86'): 1,\n",
              "          ('Owens', '86', ']'): 1,\n",
              "          (']', 'for', 'brief'): 1,\n",
              "          ('for', 'brief', 'discussion'): 1,\n",
              "          ('brief', 'discussion', 'of'): 1,\n",
              "          ('discussion', 'of', 'a'): 1,\n",
              "          ('of', 'a', 'program'): 1,\n",
              "          ('a', 'program', 'built'): 1,\n",
              "          ('program', 'built', 'around'): 1,\n",
              "          ('built', 'around', 'these'): 1,\n",
              "          ('around', 'these', '.principles'): 1,\n",
              "          ('these', '.principles', ')'): 1,\n",
              "          ('.principles', ')', ';'): 1,\n",
              "          (')', ';', 'the'): 1,\n",
              "          (';', 'the', 'goal'): 1,\n",
              "          ('the', 'goal', 'here'): 1,\n",
              "          ('goal', 'here', 'is'): 1,\n",
              "          ('here', 'is', 'simply'): 1,\n",
              "          ('is', 'simply', 'to'): 1,\n",
              "          ('simply', 'to', 'point'): 1,\n",
              "          ('to', 'point', 'out'): 1,\n",
              "          ('point', 'out', 'how'): 1,\n",
              "          ('out', 'how', 'our'): 1,\n",
              "          ('how', 'our', 'interest'): 1,\n",
              "          ('our', 'interest', 'in'): 1,\n",
              "          ('interest', 'in', 'natural'): 1,\n",
              "          ('language', 'processing', 'ha'): 1,\n",
              "          ('processing', 'ha', 'led'): 1,\n",
              "          ('ha', 'led', 'u'): 1,\n",
              "          ('led', 'u', 'naturally'): 1,\n",
              "          ('u', 'naturally', ','): 1,\n",
              "          ('naturally', ',', 'and'): 1,\n",
              "          (',', 'and', 'indeed'): 1,\n",
              "          ('and', 'indeed', 'inevitably'): 1,\n",
              "          ('indeed', 'inevitably', 'Objectives'): 1,\n",
              "          ('inevitably', 'Objectives', 'To'): 1,\n",
              "          ('Objectives', 'To', 'provide'): 1,\n",
              "          ('To', 'provide', 'an'): 1,\n",
              "          ('provide', 'an', 'overview'): 1,\n",
              "          ('an', 'overview', 'and'): 1,\n",
              "          ('overview', 'and', 'tutorial'): 1,\n",
              "          ('and', 'tutorial', 'of'): 1,\n",
              "          ('tutorial', 'of', 'natural'): 1,\n",
              "          ('NLP', ')', 'and'): 3,\n",
              "          (')', 'and', 'modern'): 1,\n",
              "          ('and', 'modern', 'NLP-system'): 1,\n",
              "          ('modern', 'NLP-system', 'design'): 1,\n",
              "          ('NLP-system', 'design', '.'): 1,\n",
              "          ('design', '.', 'Target'): 1,\n",
              "          ('.', 'Target', 'audience'): 1,\n",
              "          ('Target', 'audience', 'This'): 1,\n",
              "          ('audience', 'This', 'tutorial'): 1,\n",
              "          ('This', 'tutorial', 'target'): 1,\n",
              "          ('tutorial', 'target', 'the'): 1,\n",
              "          ('target', 'the', 'medical'): 1,\n",
              "          ('the', 'medical', 'informatics'): 1,\n",
              "          ('medical', 'informatics', 'generalist'): 1,\n",
              "          ('informatics', 'generalist', 'who'): 1,\n",
              "          ('generalist', 'who', 'ha'): 1,\n",
              "          ('who', 'ha', 'limited'): 1,\n",
              "          ('ha', 'limited', 'acquaintance'): 1,\n",
              "          ('limited', 'acquaintance', 'with'): 1,\n",
              "          ('acquaintance', 'with', 'the'): 1,\n",
              "          ('with', 'the', 'principle'): 1,\n",
              "          ('the', 'principle', 'behind'): 1,\n",
              "          ('principle', 'behind', 'NLP'): 1,\n",
              "          ('behind', 'NLP', 'and/or'): 1,\n",
              "          ('NLP', 'and/or', 'limited'): 1,\n",
              "          ('and/or', 'limited', 'knowledge'): 1,\n",
              "          ('limited', 'knowledge', 'of'): 1,\n",
              "          ('knowledge', 'of', 'the'): 1,\n",
              "          ('of', 'the', 'current'): 2,\n",
              "          ('the', 'current', 'state'): 2,\n",
              "          ('current', 'state', 'This'): 1,\n",
              "          ('state', 'This', 'paper'): 1,\n",
              "          ('This', 'paper', 'briefly'): 1,\n",
              "          ('paper', 'briefly', 'describes'): 1,\n",
              "          ('briefly', 'describes', 'the'): 1,\n",
              "          ('describes', 'the', 'current'): 1,\n",
              "          ('the', 'current', 'implementation'): 1,\n",
              "          ('current', 'implementation', 'status'): 1,\n",
              "          ('implementation', 'status', 'of'): 1,\n",
              "          ('status', 'of', 'an'): 1,\n",
              "          ('of', 'an', 'intelligent'): 1,\n",
              "          ('an', 'intelligent', 'information'): 1,\n",
              "          ('intelligent', 'information', 'retrieval'): 1,\n",
              "          ('information', 'retrieval', 'system'): 2,\n",
              "          ('retrieval', 'system', ','): 1,\n",
              "          ('system', ',', 'MARIE'): 1,\n",
              "          (',', 'MARIE', ','): 1,\n",
              "          ('MARIE', ',', 'that'): 1,\n",
              "          (',', 'that', 'employ'): 1,\n",
              "          ('that', 'employ', 'natural'): 1,\n",
              "          ('employ', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'techniques'): 1,\n",
              "          ('processing', 'techniques', '.'): 1,\n",
              "          ('techniques', '.', 'Descriptive'): 1,\n",
              "          ('.', 'Descriptive', 'caption'): 1,\n",
              "          ('Descriptive', 'caption', 'are'): 1,\n",
              "          ('caption', 'are', 'used'): 1,\n",
              "          ('are', 'used', 'to'): 1,\n",
              "          ('used', 'to', 'iden-'): 1,\n",
              "          ('to', 'iden-', 'tify'): 1,\n",
              "          ('iden-', 'tify', 'photographic'): 1,\n",
              "          ('tify', 'photographic', 'image'): 1,\n",
              "          ('photographic', 'image', 'concerning'): 1,\n",
              "          ('image', 'concerning', 'various'): 1,\n",
              "          ('concerning', 'various', 'military'): 1,\n",
              "          ('various', 'military', 'projects'): 1,\n",
              "          ('military', 'projects', '.'): 1,\n",
              "          ('projects', '.', 'The'): 1,\n",
              "          ('.', 'The', 'caption'): 1,\n",
              "          ('The', 'caption', 'are'): 1,\n",
              "          ('caption', 'are', 'parsed'): 1,\n",
              "          ('are', 'parsed', 'based'): 1,\n",
              "          ('parsed', 'based', 'and'): 1,\n",
              "          ('based', 'and', 'literature'): 1,\n",
              "          ('and', 'literature', 'resources'): 1,\n",
              "          ('literature', 'resources', '.'): 1,\n",
              "          ('resources', '.', 'We'): 1,\n",
              "          ('.', 'We', 'describe'): 2,\n",
              "          ('We', 'describe', 'here'): 1,\n",
              "          ('describe', 'here', 'a'): 1,\n",
              "          ('here', 'a', 'system'): 1,\n",
              "          ('a', 'system', 'for'): 1,\n",
              "          ('system', 'for', 'agent'): 1,\n",
              "          ('for', 'agent', 'directed'): 1,\n",
              "          ('agent', 'directed', 'natural'): 1,\n",
              "          ('directed', 'natural', 'language'): 1,\n",
              "          ('language', 'processing', 'to'): 3,\n",
              "          ('processing', 'to', 'extract'): 1,\n",
              "          ('to', 'extract', 'information'): 1,\n",
              "          ('extract', 'information', 'from'): 1,\n",
              "          ('information', 'from', 'journal'): 1,\n",
              "          ('from', 'journal', 'articles'): 1,\n",
              "          ('journal', 'articles', '.'): 1,\n",
              "          ('articles', '.', 'An'): 1,\n",
              "          ('.', 'An', 'interface'): 1,\n",
              "          ('An', 'interface', 'wa'): 1,\n",
              "          ('interface', 'wa', 'developed'): 1,\n",
              "          ('wa', 'developed', 'to'): 1,\n",
              "          ('developed', 'to', 'permit'): 1,\n",
              "          ('to', 'permit', 'curation'): 1,\n",
              "          ('permit', 'curation', 'of'): 1,\n",
              "          ('curation', 'of', 'the'): 1,\n",
              "          ('of', 'the', 'NLP'): 1,\n",
              "          ('the', 'NLP', 'result'): 1,\n",
              "          ('NLP', 'result', 'and'): 1,\n",
              "          ('result', 'and', 'deposition'): 1,\n",
              "          ('and', 'deposition', 'of'): 1,\n",
              "          ('deposition', 'of', 'accepted'): 1,\n",
              "          ('of', 'accepted', 'result'): 1,\n",
              "          ('accepted', 'result', 'into'): 1,\n",
              "          ('result', 'into', 'a'): 1,\n",
              "          ('into', 'a', 'knowledge'): 1,\n",
              "          ('knowledge', 'base', '.'): 1,\n",
              "          ('base', '.', 'Motivation'): 1,\n",
              "          ('.', 'Motivation', ':'): 1,\n",
              "          ('Motivation', ':', 'The'): 1,\n",
              "          (':', 'The', 'advent'): 1,\n",
              "          ('The', 'advent', 'of'): 1,\n",
              "          ('advent', 'of', 'high'): 1,\n",
              "          ('of', 'high', 'to'): 1,\n",
              "          ('high', 'to', 'evaluation'): 1,\n",
              "          ('to', 'evaluation', 'in'): 1,\n",
              "          ('evaluation', 'in', 'speech'): 1,\n",
              "          ('in', 'speech', 'processing'): 1,\n",
              "          ('speech', 'processing', '.'): 1,\n",
              "          ('processing', '.', 'Part'): 1,\n",
              "          ('.', 'Part', '2'): 1,\n",
              "          ('Part', '2', 'survey'): 1,\n",
              "          ('2', 'survey', 'significant'): 1,\n",
              "          ('survey', 'significant', 'evaluation'): 1,\n",
              "          ('significant', 'evaluation', 'work'): 1,\n",
              "          ('evaluation', 'work', 'done'): 1,\n",
              "          ('work', 'done', 'so'): 1,\n",
              "          ('done', 'so', 'far'): 1,\n",
              "          ('so', 'far', ','): 1,\n",
              "          ('far', ',', 'for'): 1,\n",
              "          (',', 'for', 'instance'): 1,\n",
              "          ('for', 'instance', 'in'): 1,\n",
              "          ('instance', 'in', 'machine'): 1,\n",
              "          ('in', 'machine', 'translation'): 1,\n",
              "          ('machine', 'translation', ','): 2,\n",
              "          ('translation', ',', 'and'): 2,\n",
              "          (',', 'and', 'discus'): 1,\n",
              "          ('and', 'discus', 'the'): 1,\n",
              "          ('discus', 'the', 'particular'): 1,\n",
              "          ('the', 'particular', 'problem'): 1,\n",
              "          ('particular', 'problem', 'of'): 1,\n",
              "          ('problem', 'of', 'generic'): 1,\n",
              "          ('of', 'generic', 'system'): 1,\n",
              "          ('generic', 'system', 'evaluation'): 1,\n",
              "          ('system', 'evaluation', '.'): 1,\n",
              "          ('evaluation', '.', 'The'): 1,\n",
              "          ('.', 'The', 'conclusion'): 1,\n",
              "          ('The', 'conclusion', 'is'): 1,\n",
              "          ('conclusion', 'is', 'that'): 1,\n",
              "          ('is', 'that', 'evaluation'): 1,\n",
              "          ('that', 'evaluation', 'strategy'): 1,\n",
              "          ('evaluation', 'strategy', 'and'): 1,\n",
              "          ('strategy', 'and', 'technique'): 1,\n",
              "          ('and', 'technique', 'for'): 1,\n",
              "          ('technique', 'for', 'NLP'): 1,\n",
              "          ('for', 'NLP', 'need'): 1,\n",
              "          ('NLP', 'need', 'much'): 1,\n",
              "          ('need', 'much', 'more'): 1,\n",
              "          ('much', 'more', 'development'): 1,\n",
              "          ('more', 'development', ','): 1,\n",
              "          ('development', ',', 'in'): 1,\n",
              "          ('in', 'particular', 'similar'): 1,\n",
              "          ('particular', 'similar', 'to'): 1,\n",
              "          ('similar', 'to', 'the'): 1,\n",
              "          ('way', 'human', 'intuitively'): 1,\n",
              "          ('human', 'intuitively', 'do'): 1,\n",
              "          ('intuitively', 'do', 'in'): 1,\n",
              "          ('do', 'in', 'order'): 1,\n",
              "          ('in', 'order', 'to'): 1,\n",
              "          ('order', 'to', 'eliminate'): 1,\n",
              "          ('to', 'eliminate', 'noisy'): 1,\n",
              "          ('eliminate', 'noisy', 'content'): 1,\n",
              "          ('noisy', 'content', '.'): 1,\n",
              "          ('content', '.', 'In'): 1,\n",
              "          ('.', 'In', 'this'): 3,\n",
              "          ('In', 'this', 'paper'): 5,\n",
              "          (',', 'we', 'describe'): 2,\n",
              "          ('we', 'describe', 'a'): 2,\n",
              "          ('describe', 'a', 'combination'): 1,\n",
              "          ('a', 'combination', 'of'): 1,\n",
              "          ('combination', 'of', 'HTML'): 1,\n",
              "          ('of', 'HTML', 'DOM'): 1,\n",
              "          ('HTML', 'DOM', 'analysis'): 1,\n",
              "          ('DOM', 'analysis', 'and'): 1,\n",
              "          ('analysis', 'and', 'Natural'): 1,\n",
              "          ('and', 'Natural', 'Language'): 2,\n",
              "          ('NLP', ')', 'technique'): 2,\n",
              "          (')', 'technique', 'for'): 1,\n",
              "          ('technique', 'for', 'automated'): 1,\n",
              "          ('for', 'automated', 'extraction'): 1,\n",
              "          ('automated', 'extraction', 'of'): 1,\n",
              "          ('extraction', 'of', 'main'): 1,\n",
              "          ('of', 'main', 'article'): 1,\n",
              "          ('main', 'article', 'with'): 1,\n",
              "          ('article', 'with', 'associated'): 1,\n",
              "          ('with', 'associated', 'image'): 1,\n",
              "          ('associated', 'image', 'from'): 1,\n",
              "          ('image', 'from', 'web'): 1,\n",
              "          ('from', 'web', 'pages'): 1,\n",
              "          ('web', 'pages', '.'): 1,\n",
              "          ('pages', '.', 'Abstract'): 1,\n",
              "          ('.', 'Abstract', '--'): 1,\n",
              "          ('Abstract', '--', 'Natural'): 1,\n",
              "          ('--', 'Natural', 'Language'): 1,\n",
              "          ('Language', 'Processing', 'is'): 1,\n",
              "          ('Processing', 'is', 'a'): 1,\n",
              "          ('is', 'a', 'theoretically'): 1,\n",
              "          ('a', 'theoretically', 'motivated'): 1,\n",
              "          ('theoretically', 'motivated', 'range'): 1,\n",
              "          ('motivated', 'range', 'of'): 1,\n",
              "          ('range', 'of', 'computational'): 1,\n",
              "          ('of', 'computational', 'technique'): 1,\n",
              "          ('computational', 'technique', 'for'): 1,\n",
              "          ('technique', 'for', 'analysing'): 1,\n",
              "          ('for', 'analysing', 'and'): 1,\n",
              "          ('analysing', 'and', 'representing'): 1,\n",
              "          ('and', 'representing', 'naturally'): 1,\n",
              "          ('representing', 'naturally', 'occurring'): 1,\n",
              "          ('naturally', 'occurring', 'text'): 1,\n",
              "          ('occurring', 'text', 'at'): 1,\n",
              "          ('text', 'at', 'one'): 1,\n",
              "          ('at', 'one', 'or'): 1,\n",
              "          ('one', 'or', 'more'): 1,\n",
              "          ('or', 'more', 'level'): 1,\n",
              "          ('more', 'level', 'of'): 1,\n",
              "          ('level', 'of', 'linguistic'): 1,\n",
              "          ('of', 'linguistic', 'analysis'): 1,\n",
              "          ('linguistic', 'analysis', 'for'): 1,\n",
              "          ('analysis', 'for', 'the'): 1,\n",
              "          ('for', 'the', 'purpose'): 1,\n",
              "          ('the', 'purpose', 'of'): 2,\n",
              "          ('purpose', 'of', 'achieving'): 1,\n",
              "          ('of', 'achieving', 'human-like'): 1,\n",
              "          ('achieving', 'human-like', 'language'): 1,\n",
              "          ('human-like', 'language', 'processing'): 1,\n",
              "          ('language', 'processing', 'for'): 2,\n",
              "          ('processing', 'for', 'a'): 1,\n",
              "          ('for', 'a', 'range'): 1,\n",
              "          ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pid_ksKokaOn",
        "outputId": "07790003-77d7-433b-8035-d710c7458d04"
      },
      "source": [
        "from collections import Counter\r\n",
        "Single_Word_Dict=Counter(final_list)\r\n",
        "Single_Word_Dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Abstract': 13,\n",
              "         'not': 14,\n",
              "         'found': 8,\n",
              "         'describe': 9,\n",
              "         'a': 117,\n",
              "         'method': 7,\n",
              "         'for': 56,\n",
              "         'statistical': 7,\n",
              "         'modeling': 3,\n",
              "         'based': 8,\n",
              "         'on': 23,\n",
              "         'maximum': 2,\n",
              "         'entropy': 2,\n",
              "         '.': 146,\n",
              "         'We': 24,\n",
              "         'present': 7,\n",
              "         'maximum-likelihood': 1,\n",
              "         'approach': 17,\n",
              "         'automatically': 3,\n",
              "         'constructing': 1,\n",
              "         'model': 10,\n",
              "         'and': 118,\n",
              "         'how': 5,\n",
              "         'to': 74,\n",
              "         'implement': 1,\n",
              "         'this': 22,\n",
              "         'efficiently': 1,\n",
              "         ',': 152,\n",
              "         'using': 11,\n",
              "         'example': 1,\n",
              "         'several': 4,\n",
              "         'problem': 4,\n",
              "         'in': 75,\n",
              "         'natural': 64,\n",
              "         'language': 91,\n",
              "         'processing': 64,\n",
              "         'Scaling': 1,\n",
              "         'conditional': 1,\n",
              "         'random': 1,\n",
              "         'field': 8,\n",
              "         'Terms': 2,\n",
              "         'Conditions': 2,\n",
              "         ':': 19,\n",
              "         'Copyright': 1,\n",
              "         'work': 8,\n",
              "         'deposited': 1,\n",
              "         'Minerva': 1,\n",
              "         'Access': 1,\n",
              "         'is': 54,\n",
              "         'retained': 1,\n",
              "         'by': 18,\n",
              "         'the': 184,\n",
              "         'The': 26,\n",
              "         'paper': 21,\n",
              "         'address': 2,\n",
              "         'issue': 5,\n",
              "         'of': 176,\n",
              "         'cooperation': 2,\n",
              "         'between': 8,\n",
              "         'linguistics': 5,\n",
              "         '(': 51,\n",
              "         'NLP': 44,\n",
              "         ')': 47,\n",
              "         'general': 3,\n",
              "         'machine': 14,\n",
              "         'translation': 4,\n",
              "         'MT': 1,\n",
              "         'particular': 5,\n",
              "         'It': 5,\n",
              "         'focus': 10,\n",
              "         'just': 1,\n",
              "         'one': 5,\n",
              "         'direction': 1,\n",
              "         'such': 4,\n",
              "         'namely': 1,\n",
              "         'application': 11,\n",
              "         'virtually': 1,\n",
              "         'In': 17,\n",
              "         'most': 2,\n",
              "         'applications': 5,\n",
              "         'Description': 4,\n",
              "         'Logics': 4,\n",
              "         'have': 12,\n",
              "         'been': 13,\n",
              "         'used': 11,\n",
              "         'encode': 2,\n",
              "         'knowledge': 12,\n",
              "         'base': 2,\n",
              "         'some': 5,\n",
              "         'syntactic': 6,\n",
              "         'semantic': 8,\n",
              "         'pragmatic': 1,\n",
              "         'element': 2,\n",
              "         'needed': 1,\n",
              "         'drive': 2,\n",
              "         'interpretation': 3,\n",
              "         'generation': 2,\n",
              "         'processes': 1,\n",
              "         'More': 1,\n",
              "         'recently': 2,\n",
              "         'propose': 2,\n",
              "         'unified': 1,\n",
              "         'neural': 3,\n",
              "         'network': 5,\n",
              "         'architecture': 5,\n",
              "         'learning': 20,\n",
              "         'algorithm': 1,\n",
              "         'that': 38,\n",
              "         'can': 16,\n",
              "         'be': 26,\n",
              "         'applied': 4,\n",
              "         'various': 8,\n",
              "         'task': 11,\n",
              "         'including': 8,\n",
              "         'part-of-speech': 2,\n",
              "         'tagging': 2,\n",
              "         'chunking': 2,\n",
              "         'named': 2,\n",
              "         'entity': 2,\n",
              "         'recognition': 12,\n",
              "         'role': 4,\n",
              "         'labeling': 1,\n",
              "         'This': 29,\n",
              "         'versatility': 1,\n",
              "         'achieved': 1,\n",
              "         'trying': 1,\n",
              "         'avoid': 3,\n",
              "         'Natural': 29,\n",
              "         'Language': 23,\n",
              "         'Processing': 20,\n",
              "         'subject': 2,\n",
              "         'considered': 1,\n",
              "         'both': 5,\n",
              "         'broad': 3,\n",
              "         'narrow': 1,\n",
              "         'senses': 2,\n",
              "         'sense': 5,\n",
              "         'it': 13,\n",
              "         'cover': 1,\n",
              "         'at': 7,\n",
              "         'all': 7,\n",
              "         'level': 4,\n",
              "         'understanding': 6,\n",
              "         'speech': 8,\n",
              "         'analysis': 11,\n",
              "         'sentence': 3,\n",
              "         'Robots': 1,\n",
              "         'interact': 1,\n",
              "         'with': 21,\n",
              "         'human': 4,\n",
              "         'face-to-face': 2,\n",
              "         'need': 6,\n",
              "         'responsive': 1,\n",
              "         'way': 5,\n",
              "         'use': 12,\n",
              "         'those': 2,\n",
              "         'situations': 1,\n",
              "         'psychologicallyinspired': 1,\n",
              "         'system': 19,\n",
              "         'robot': 1,\n",
              "         'which': 9,\n",
              "         'performs': 2,\n",
              "         'incremental': 1,\n",
              "         'spoken': 4,\n",
              "         'utterance': 1,\n",
              "         'are': 26,\n",
              "         'humans': 1,\n",
              "         'Currently': 1,\n",
              "         'we': 17,\n",
              "         'yet': 2,\n",
              "         'point': 4,\n",
              "         'where': 2,\n",
              "         'these': 5,\n",
              "         'their': 4,\n",
              "         'unprocessed': 1,\n",
              "         'form': 5,\n",
              "         'understood': 2,\n",
              "         'computers': 1,\n",
              "         'collection': 2,\n",
              "         'technique': 12,\n",
              "         'employed': 2,\n",
              "         'try': 1,\n",
              "         'accomplish': 1,\n",
              "         'goal': 3,\n",
              "         'ABSTRACT': 4,\n",
              "         'Ambiguity': 1,\n",
              "         'referred': 1,\n",
              "         'ability': 1,\n",
              "         'having': 2,\n",
              "         'more': 9,\n",
              "         'than': 4,\n",
              "         'meaning': 2,\n",
              "         'or': 17,\n",
              "         'being': 2,\n",
              "         'ambiguous': 1,\n",
              "         'so': 3,\n",
              "         'computer': 3,\n",
              "         'able': 2,\n",
              "         'understand': 1,\n",
              "         'people': 1,\n",
              "         'do': 3,\n",
              "         'concerned': 2,\n",
              "         'development': 11,\n",
              "         'Introduction': 5,\n",
              "         'Statistical': 1,\n",
              "         'SNLP': 2,\n",
              "         'lying': 1,\n",
              "         'intersection': 1,\n",
              "         'di': 1,\n",
              "         '#': 1,\n",
              "         'ers': 1,\n",
              "         'from': 18,\n",
              "         'traditional': 2,\n",
              "         'instead': 1,\n",
              "         'linguist': 1,\n",
              "         'manually': 2,\n",
              "         'construct': 1,\n",
              "         'given': 4,\n",
              "         'linguistic': 7,\n",
              "         'text': 11,\n",
              "         'directly': 1,\n",
              "         'rather': 3,\n",
              "         'e.g': 1,\n",
              "         'title': 1,\n",
              "         'abstracts': 1,\n",
              "         'suggests': 1,\n",
              "         'appropriate': 1,\n",
              "         'doing': 1,\n",
              "         'also': 5,\n",
              "         'comment': 1,\n",
              "         'possible': 2,\n",
              "         'connection': 1,\n",
              "         'data': 6,\n",
              "         'retrieval': 9,\n",
              "         'concludes': 1,\n",
              "         'emphasizing': 1,\n",
              "         'importance': 1,\n",
              "         'rigorous': 1,\n",
              "         'communicating': 1,\n",
              "         'your': 1,\n",
              "         'word': 13,\n",
              "         'help': 2,\n",
              "         'world': 2,\n",
              "         'get': 1,\n",
              "         'better': 2,\n",
              "         'insight': 1,\n",
              "         'speaker': 2,\n",
              "         'vague': 1,\n",
              "         'precise': 1,\n",
              "         'they': 1,\n",
              "         'like': 2,\n",
              "         'Stands': 1,\n",
              "         'processing..': 1,\n",
              "         'report': 3,\n",
              "         'experiment': 2,\n",
              "         'standard': 3,\n",
              "         'tool': 3,\n",
              "         'music': 4,\n",
              "         'lyrics': 2,\n",
              "         'A': 4,\n",
              "         'significant': 3,\n",
              "         'amount': 1,\n",
              "         'audio': 1,\n",
              "         'ha': 12,\n",
              "         'Lyrics': 1,\n",
              "         'an': 20,\n",
              "         'important': 3,\n",
              "         'part': 3,\n",
              "         'semantics': 1,\n",
              "         'song': 1,\n",
              "         'therefore': 1,\n",
              "         'complement': 1,\n",
              "         'acoustic': 1,\n",
              "         'cultural': 1,\n",
              "         'will': 9,\n",
              "         'simple': 1,\n",
              "         'rule-based': 1,\n",
              "         'automated': 5,\n",
              "         'shown': 2,\n",
              "         'number': 5,\n",
              "         'capture': 3,\n",
              "         'information': 19,\n",
              "         'clearer': 1,\n",
              "         'direct': 1,\n",
              "         'fashion': 1,\n",
              "         'without': 1,\n",
              "         'compromise': 1,\n",
              "         'performance': 4,\n",
              "         'detailed': 1,\n",
              "         'case': 2,\n",
              "         'study': 3,\n",
              "         'connectionist': 1,\n",
              "         'briefly': 3,\n",
              "         'discus': 3,\n",
              "         'aspect': 2,\n",
              "         'high': 3,\n",
              "         'approached': 1,\n",
              "         'connectionism': 1,\n",
              "         'either': 1,\n",
              "         'localist': 1,\n",
              "         'parallel': 2,\n",
              "         'distributed': 1,\n",
              "         'models': 1,\n",
              "         'Several': 1,\n",
              "         'interesting': 1,\n",
              "         'process': 11,\n",
              "         'new': 6,\n",
              "         'deterministic': 1,\n",
              "         'chaotic': 1,\n",
              "         'behavior': 1,\n",
              "         'dynamical': 1,\n",
              "         'systems': 9,\n",
              "         '1': 9,\n",
              "         'see': 1,\n",
              "         '[': 5,\n",
              "         'Schank': 1,\n",
              "         '86': 3,\n",
              "         ']': 5,\n",
              "         'theoretical': 1,\n",
              "         'discussion': 2,\n",
              "         'Kass': 1,\n",
              "         'Leake': 1,\n",
              "         'Owens': 1,\n",
              "         'brief': 3,\n",
              "         'program': 3,\n",
              "         'built': 2,\n",
              "         'around': 1,\n",
              "         '.principles': 1,\n",
              "         ';': 6,\n",
              "         'here': 2,\n",
              "         'simply': 1,\n",
              "         'out': 3,\n",
              "         'our': 4,\n",
              "         'interest': 1,\n",
              "         'led': 1,\n",
              "         'u': 2,\n",
              "         'naturally': 2,\n",
              "         'indeed': 1,\n",
              "         'inevitably': 1,\n",
              "         'Objectives': 1,\n",
              "         'To': 1,\n",
              "         'provide': 1,\n",
              "         'overview': 2,\n",
              "         'tutorial': 2,\n",
              "         'modern': 1,\n",
              "         'NLP-system': 1,\n",
              "         'design': 4,\n",
              "         'Target': 1,\n",
              "         'audience': 1,\n",
              "         'target': 1,\n",
              "         'medical': 1,\n",
              "         'informatics': 1,\n",
              "         'generalist': 1,\n",
              "         'who': 1,\n",
              "         'limited': 4,\n",
              "         'acquaintance': 1,\n",
              "         'principle': 2,\n",
              "         'behind': 1,\n",
              "         'and/or': 1,\n",
              "         'current': 7,\n",
              "         'state': 5,\n",
              "         'describes': 2,\n",
              "         'implementation': 2,\n",
              "         'status': 1,\n",
              "         'intelligent': 2,\n",
              "         'MARIE': 1,\n",
              "         'employ': 1,\n",
              "         'techniques': 2,\n",
              "         'Descriptive': 1,\n",
              "         'caption': 2,\n",
              "         'iden-': 1,\n",
              "         'tify': 1,\n",
              "         'photographic': 1,\n",
              "         'image': 3,\n",
              "         'concerning': 2,\n",
              "         'military': 1,\n",
              "         'projects': 1,\n",
              "         'parsed': 1,\n",
              "         'literature': 1,\n",
              "         'resources': 2,\n",
              "         'agent': 1,\n",
              "         'directed': 1,\n",
              "         'extract': 4,\n",
              "         'journal': 1,\n",
              "         'articles': 1,\n",
              "         'An': 2,\n",
              "         'interface': 2,\n",
              "         'wa': 1,\n",
              "         'developed': 5,\n",
              "         'permit': 1,\n",
              "         'curation': 1,\n",
              "         'result': 8,\n",
              "         'deposition': 1,\n",
              "         'accepted': 1,\n",
              "         'into': 6,\n",
              "         'Motivation': 1,\n",
              "         'advent': 1,\n",
              "         'evaluation': 8,\n",
              "         'Part': 1,\n",
              "         '2': 1,\n",
              "         'survey': 2,\n",
              "         'done': 1,\n",
              "         'far': 2,\n",
              "         'instance': 1,\n",
              "         'generic': 3,\n",
              "         'conclusion': 1,\n",
              "         'strategy': 1,\n",
              "         'much': 2,\n",
              "         'similar': 2,\n",
              "         'intuitively': 2,\n",
              "         'order': 2,\n",
              "         'eliminate': 1,\n",
              "         'noisy': 1,\n",
              "         'content': 1,\n",
              "         'combination': 1,\n",
              "         'HTML': 1,\n",
              "         'DOM': 1,\n",
              "         'extraction': 4,\n",
              "         'main': 1,\n",
              "         'article': 3,\n",
              "         'associated': 1,\n",
              "         'web': 1,\n",
              "         'pages': 1,\n",
              "         '--': 14,\n",
              "         'theoretically': 1,\n",
              "         'motivated': 1,\n",
              "         'range': 6,\n",
              "         'computational': 5,\n",
              "         'analysing': 1,\n",
              "         'representing': 1,\n",
              "         'occurring': 1,\n",
              "         'purpose': 3,\n",
              "         'achieving': 1,\n",
              "         'human-like': 1,\n",
              "         'review': 4,\n",
              "         'involved': 1,\n",
              "         'then': 5,\n",
              "         'demonstrates': 1,\n",
              "         'kind': 2,\n",
              "         'choice': 1,\n",
              "         'taken': 2,\n",
              "         'during': 3,\n",
              "         'execution': 2,\n",
              "         'morphology': 1,\n",
              "         'components': 1,\n",
              "         'compare': 2,\n",
              "         'time': 2,\n",
              "         'complexity': 4,\n",
              "         'derivation': 1,\n",
              "         'large': 2,\n",
              "         'lexicon': 2,\n",
              "         'dictionary': 2,\n",
              "         'support': 3,\n",
              "         'environment': 2,\n",
              "         'linking': 1,\n",
              "         'restructured': 1,\n",
              "         'version': 2,\n",
              "         'Longman': 1,\n",
              "         'Dictionary': 1,\n",
              "         'Contemporary': 1,\n",
              "         'English': 2,\n",
              "         'introduce': 1,\n",
              "         'analyzing': 2,\n",
              "         'tasks': 9,\n",
              "         'predicting': 1,\n",
              "         'difficulty': 1,\n",
              "         'Our': 2,\n",
              "         'measure': 2,\n",
              "         'derived': 1,\n",
              "         'Kolmogorov': 1,\n",
              "         'class': 4,\n",
              "         'automaton': 1,\n",
              "         '—': 1,\n",
              "         'automata': 1,\n",
              "         'whose': 1,\n",
              "         'relevant': 3,\n",
              "         'piece': 1,\n",
              "         'sounds': 1,\n",
              "         'motion': 1,\n",
              "         'deep': 2,\n",
              "         'research': 12,\n",
              "         'already': 1,\n",
              "         'impacting': 1,\n",
              "         'recent': 6,\n",
              "         'author-produced': 1,\n",
              "         'published': 1,\n",
              "         'Abstract—Natural': 2,\n",
              "         'parsing': 4,\n",
              "         'analyze': 1,\n",
              "         'Applications': 1,\n",
              "         'requirement': 4,\n",
              "         'engineering': 2,\n",
              "         'include': 2,\n",
              "         'ontology': 2,\n",
              "         'specification': 1,\n",
              "         'verify': 1,\n",
              "         'consistency': 1,\n",
              "         'baseline': 1,\n",
              "         'forgiving': 1,\n",
              "         'nature': 1,\n",
              "         'but': 6,\n",
              "         'coverage': 1,\n",
              "         'typical': 1,\n",
              "         'lack': 1,\n",
              "         'good': 2,\n",
              "         'weighting': 2,\n",
              "         'scheme': 1,\n",
              "         'compound': 3,\n",
              "         'index': 1,\n",
              "         'terms': 1,\n",
              "         'implicit': 2,\n",
              "         'inherent': 1,\n",
              "         'methods': 1,\n",
              "         'may': 3,\n",
              "         'Work': 1,\n",
              "         'began': 1,\n",
              "         'very': 2,\n",
              "         'soon': 1,\n",
              "         'after': 1,\n",
              "         'first': 3,\n",
              "         'Booth': 1,\n",
              "         'Brandwood': 1,\n",
              "         'Cleave': 1,\n",
              "         '1958': 1,\n",
              "         'intervening': 1,\n",
              "         'four': 1,\n",
              "         'decade': 1,\n",
              "         'there': 1,\n",
              "         'pervasive': 1,\n",
              "         'feeling': 1,\n",
              "         'progress': 1,\n",
              "         'commensurate': 1,\n",
              "         'voice': 1,\n",
              "         'Tamil': 1,\n",
              "         'combining': 1,\n",
              "         'digital': 3,\n",
              "         'mathematical': 2,\n",
              "         'MFCC': 1,\n",
              "         'DTW': 1,\n",
              "         'match': 1,\n",
              "         'feature': 2,\n",
              "         'improve': 2,\n",
              "         'accuracy': 1,\n",
              "         'Testing': 1,\n",
              "         'against': 1,\n",
              "         'acceptance': 1,\n",
              "         'testing': 1,\n",
              "         'test': 2,\n",
              "         'often': 2,\n",
              "         'performed': 1,\n",
              "         'independent': 2,\n",
              "         'organization': 1,\n",
              "         'unfamiliar': 1,\n",
              "         'area': 2,\n",
              "         'only': 2,\n",
              "         'thing': 1,\n",
              "         'tester': 1,\n",
              "         'go': 2,\n",
              "         'written': 1,\n",
              "         'requirements': 1,\n",
              "         'So': 2,\n",
              "         'conversational': 1,\n",
              "         'partners': 1,\n",
              "         'But': 1,\n",
              "         'provides': 2,\n",
              "         'about': 3,\n",
              "         'creative': 1,\n",
              "         'making': 1,\n",
              "         'associations': 1,\n",
              "         'storytelling': 1,\n",
              "         'Many': 3,\n",
              "         'subtlety': 1,\n",
              "         'multiparty': 1,\n",
              "         'interaction': 1,\n",
              "         'added': 1,\n",
              "         'humor': 1,\n",
              "         'persuade': 1,\n",
              "         'dominate': 1,\n",
              "         'soften': 1,\n",
              "         'face': 1,\n",
              "         'threatening': 1,\n",
              "         'act': 1,\n",
              "         'years': 4,\n",
              "         'ML': 2,\n",
              "         'solve': 2,\n",
              "         'complex': 1,\n",
              "         'different': 4,\n",
              "         'disciplines': 1,\n",
              "         'ranging': 1,\n",
              "         'Data': 1,\n",
              "         'Mining': 1,\n",
              "         'Information': 3,\n",
              "         'argue': 6,\n",
              "         'manual': 2,\n",
              "         'automatic': 1,\n",
              "         'thesaurus': 5,\n",
              "         'alternative': 1,\n",
              "         'resource': 3,\n",
              "         'same': 3,\n",
              "         'involves': 2,\n",
              "         'radical': 1,\n",
              "         'step': 1,\n",
              "         'interpreting': 1,\n",
              "         'classification': 1,\n",
              "         'made': 3,\n",
              "         'within': 5,\n",
              "         'presented': 2,\n",
              "         'WASPS': 1,\n",
              "         'introduced': 2,\n",
              "         'Thesaurus': 1,\n",
              "         'now': 1,\n",
              "         'becoming': 1,\n",
              "         'urgent': 1,\n",
              "         'strategies': 1,\n",
              "         'embedded': 1,\n",
              "         'proposed': 1,\n",
              "         'Patterns': 2,\n",
              "         'object': 2,\n",
              "         'intensive': 1,\n",
              "         'past': 2,\n",
              "         '\\\\One': 1,\n",
              "         'musical': 3,\n",
              "         'structure': 4,\n",
              "         'discover': 1,\n",
              "         'pattern': 5,\n",
              "         'explicit': 1,\n",
              "         'works': 1,\n",
              "         \"''\": 5,\n",
              "         'Simon': 1,\n",
              "         '13': 1,\n",
              "         'comprise': 1,\n",
              "         'periodicity': 1,\n",
              "         'make': 4,\n",
              "         'alphabets': 1,\n",
              "         'up': 1,\n",
              "         'subpatterns': 1,\n",
              "         'posse': 1,\n",
              "         'phrase': 1,\n",
              "         'punctuation': 1,\n",
              "         'Traditionally': 1,\n",
              "         'composer': 1,\n",
              "         'propagation': 2,\n",
              "         'algorithmic': 1,\n",
              "         'composition': 2,\n",
              "         'allow': 1,\n",
              "         'formalized': 1,\n",
              "         'albeit': 1,\n",
              "         'During': 2,\n",
              "         'evolve': 1,\n",
              "         'according': 1,\n",
              "         'rule': 2,\n",
              "         'constraint': 2,\n",
              "         'specied': 1,\n",
              "         'stage': 1,\n",
              "         'jazz': 1,\n",
              "         'improvisation': 1,\n",
              "         'musician': 1,\n",
              "         'invents': 1,\n",
              "         'solo': 2,\n",
              "         'guided': 1,\n",
              "         'progression': 2,\n",
              "         'chord': 1,\n",
              "         'changes': 1,\n",
              "         'One': 2,\n",
              "         'learn': 2,\n",
              "         'improvising': 1,\n",
              "         'memorize': 1,\n",
              "         'short': 2,\n",
              "         'chunk': 1,\n",
              "         't': 2,\n",
              "         'sub-progressions': 1,\n",
              "         'concatenate': 1,\n",
              "         'them': 4,\n",
              "         'whole': 2,\n",
              "         'IR': 1,\n",
              "         'retrieve': 1,\n",
              "         'document': 9,\n",
              "         'exact': 1,\n",
              "         'matching': 2,\n",
              "         'keywords': 1,\n",
              "         'query': 3,\n",
              "         'documents': 1,\n",
              "         'degrades': 1,\n",
              "         'precision': 2,\n",
              "         'rate': 2,\n",
              "         'collected': 1,\n",
              "         'semantically': 2,\n",
              "         'related': 2,\n",
              "         'assigned': 1,\n",
              "         'relationship': 3,\n",
              "         'special': 2,\n",
              "         'called': 2,\n",
              "         'keyfact': 2,\n",
              "         'term': 2,\n",
              "         'FT': 1,\n",
              "         'addition': 1,\n",
              "         'constructed': 1,\n",
              "         'statistic': 3,\n",
              "         'concept': 2,\n",
              "         'mutual': 1,\n",
              "         'Keyfact': 2,\n",
              "         'extended': 1,\n",
              "         'keyword': 1,\n",
              "         'represented': 1,\n",
              "         'noun': 2,\n",
              "         'verb': 2,\n",
              "         'adjective': 1,\n",
              "         'retrieved': 1,\n",
              "         'original': 1,\n",
              "         'tf': 1,\n",
              "         '*': 1,\n",
              "         'idf': 1,\n",
              "         'formula': 1,\n",
              "         'expanded': 2,\n",
              "         'keyfacts': 1,\n",
              "         'second': 1,\n",
              "         'ranking': 1,\n",
              "         'disambiguating': 1,\n",
              "         'improvement': 4,\n",
              "         'questionanswering': 1,\n",
              "         'QA': 3,\n",
              "         'over': 5,\n",
              "         'technical': 1,\n",
              "         'domain': 5,\n",
              "         'distinctly': 1,\n",
              "         'TREC-based': 1,\n",
              "         'Web-based': 2,\n",
              "         'benefit': 2,\n",
              "         'lom': 1,\n",
              "         'data-intensive': 1,\n",
              "         'Universit': 1,\n",
              "         '&': 3,\n",
              "         'quot': 1,\n",
              "         'de': 1,\n",
              "         'Saarlandes': 1,\n",
              "         'Proceedings': 1,\n",
              "         'Workshop': 1,\n",
              "         'uni-hamburg.de': 1,\n",
              "         'SRI': 1,\n",
              "         'integrating': 2,\n",
              "         'natural-language': 1,\n",
              "         'applies': 1,\n",
              "         'incrementally': 1,\n",
              "         'expanding': 1,\n",
              "         'state-transition': 1,\n",
              "         'embodied': 1,\n",
              "         'unification': 1,\n",
              "         'grammar': 2,\n",
              "         'dynamic-gralnlnar-network': 1,\n",
              "         'DGN': 1,\n",
              "         'chapter': 4,\n",
              "         'considers': 1,\n",
              "         'revolution': 1,\n",
              "         'place': 1,\n",
              "         'last': 4,\n",
              "         'five': 1,\n",
              "         'begin': 1,\n",
              "         'providing': 2,\n",
              "         'guide': 1,\n",
              "         'caricature': 1,\n",
              "         'two': 3,\n",
              "         'competing': 1,\n",
              "         'paradigm': 1,\n",
              "         '1980s': 1,\n",
              "         'indicates': 1,\n",
              "         'reason': 1,\n",
              "         'visual': 3,\n",
              "         'assembly': 1,\n",
              "         'modular': 1,\n",
              "         'executable': 1,\n",
              "         'flow': 1,\n",
              "         'graph': 2,\n",
              "         'synthesised': 1,\n",
              "         'dependency': 2,\n",
              "         'declaration': 1,\n",
              "         'modules': 1,\n",
              "         'Chapter': 1,\n",
              "         'basic': 4,\n",
              "         'us': 2,\n",
              "         'analysed': 1,\n",
              "         'together': 1,\n",
              "         'little': 1,\n",
              "         'bit': 1,\n",
              "         'history': 4,\n",
              "         'art': 4,\n",
              "         'pointed': 1,\n",
              "         '18.1': 1,\n",
              "         'Since': 2,\n",
              "         'early': 1,\n",
              "         'day': 1,\n",
              "         'Max-Margin': 1,\n",
              "         'Structure': 1,\n",
              "         'MMS': 1,\n",
              "         'aim': 1,\n",
              "         'latent': 1,\n",
              "         'output': 3,\n",
              "         'formulate': 1,\n",
              "         'extension': 1,\n",
              "         'multi–class': 1,\n",
              "         'Support': 1,\n",
              "         'Vector': 1,\n",
              "         'Machine': 3,\n",
              "         'SVM': 1,\n",
              "         '-mation': 1,\n",
              "         'Infrastructure': 1,\n",
              "         'libraries': 1,\n",
              "         'networked': 1,\n",
              "         'services': 1,\n",
              "         'convergence': 1,\n",
              "         'agents': 1,\n",
              "         'attention': 1,\n",
              "         'moving': 1,\n",
              "         'along': 1,\n",
              "         'critical': 1,\n",
              "         'path': 1,\n",
              "         'novel': 1,\n",
              "         'mention': 1,\n",
              "         'successful': 1,\n",
              "         'Over': 1,\n",
              "         'few': 1,\n",
              "         'begun': 1,\n",
              "         'applying': 2,\n",
              "         'graph-based': 1,\n",
              "         'These': 1,\n",
              "         'among': 3,\n",
              "         'others': 1,\n",
              "         'summarization': 1,\n",
              "         'disambiguation': 2,\n",
              "         'construction': 1,\n",
              "         'sentiment': 1,\n",
              "         'subjectivity': 1,\n",
              "         'clustering': 1,\n",
              "         'software': 4,\n",
              "         'technology': 3,\n",
              "         'neglected': 1,\n",
              "         'kernelized': 2,\n",
              "         'sorting': 1,\n",
              "         'increase': 1,\n",
              "         'robustness': 1,\n",
              "         'comparable': 1,\n",
              "         'corpora': 1,\n",
              "         'transliteration': 1,\n",
              "         'even': 2,\n",
              "         'Empirically': 1,\n",
              "         'show': 1,\n",
              "         'semi-supervised': 1,\n",
              "         'variant': 1,\n",
              "         'structured': 2,\n",
              "         'sophisticated': 2,\n",
              "         'elements': 1,\n",
              "         'phrases': 1,\n",
              "         'combined': 1,\n",
              "         'structural': 1,\n",
              "         'property': 1,\n",
              "         'framework': 2,\n",
              "         'developing': 1,\n",
              "         'probabilistic': 2,\n",
              "         'classifier': 1,\n",
              "         'formulating': 1,\n",
              "         'interdependency': 1,\n",
              "         'features': 1,\n",
              "         'overfitting': 1,\n",
              "         'while': 3,\n",
              "         'characterizing': 1,\n",
              "         'well': 1,\n",
              "         'Retrieval': 1,\n",
              "         'encouraging': 1,\n",
              "         'Simple': 1,\n",
              "         'stopwording': 1,\n",
              "         'porter-style': 1,\n",
              "         'stemming': 1,\n",
              "         'etc': 1,\n",
              "         'usually': 1,\n",
              "         'yield': 1,\n",
              "         'improvements': 1,\n",
              "         'higher-level': 1,\n",
              "         'Abstract-': 1,\n",
              "         'explains': 1,\n",
              "         'Malayalam': 1,\n",
              "         'plan': 4,\n",
              "         'outline': 2,\n",
              "         'relation': 2,\n",
              "         'PR': 4,\n",
              "         'each': 2,\n",
              "         'effectively': 3,\n",
              "         'inform': 2,\n",
              "         'other': 2,\n",
              "         'key': 2,\n",
              "         'applicability': 2,\n",
              "         'finding': 1,\n",
              "         'satisfies': 1,\n",
              "         'user': 2,\n",
              "         'constructs': 1,\n",
              "         'motivation': 1,\n",
              "         'investigate': 1,\n",
              "         'logic': 5,\n",
              "         'programming': 5,\n",
              "         'opportunity': 1,\n",
              "         'induction': 1,\n",
              "         'Keywords': 1,\n",
              "         'inductive': 1,\n",
              "         'There': 1,\n",
              "         'What': 1,\n",
              "         '?': 5,\n",
              "         'start': 1,\n",
              "         'definition': 1,\n",
              "         'effective': 4,\n",
              "         'input': 1,\n",
              "         'component': 1,\n",
              "         'distinguish': 1,\n",
              "         'three': 3,\n",
              "         'collaborative': 1,\n",
              "         'Learning': 1,\n",
              "         'parts': 1,\n",
              "         'includes': 3,\n",
              "         'superficial': 1,\n",
              "         'comprehensive': 1,\n",
              "         'covering': 2,\n",
              "         'thesis': 2,\n",
              "         'examines': 3,\n",
              "         'mainly': 1,\n",
              "         'texts': 1,\n",
              "         'objective': 1,\n",
              "         'adaptability': 1,\n",
              "         'thematic': 1,\n",
              "         'do-mains': 1,\n",
              "         'computerassisted': 2,\n",
              "         'thirtyfive': 2,\n",
              "         'year': 4,\n",
              "         'opportunities': 2,\n",
              "         '36.1': 1,\n",
              "         'Traditional': 1,\n",
              "         'tointerpretation': 1,\n",
              "         'typically': 2,\n",
              "         'fall': 1,\n",
              "         'classes': 1,\n",
              "         'syntax-driven': 1,\n",
              "         'semantics-driven': 1,\n",
              "         'frame/task': 1,\n",
              "         'Syntax-driven': 1,\n",
              "         'domain-independent': 1,\n",
              "         'produce': 1,\n",
              "         'global': 2,\n",
              "         'parse': 1,\n",
              "         'diverse': 1,\n",
              "         'subtopic': 1,\n",
              "         'artificial': 2,\n",
              "         'intelligence': 2,\n",
              "         'As': 1,\n",
              "         'itself': 1,\n",
              "         'many': 2,\n",
              "         'subtopics': 1,\n",
              "         'optical': 1,\n",
              "         'character': 1,\n",
              "         'translators': 1,\n",
              "         'foreign': 1,\n",
              "         'reading': 1,\n",
              "         'writing': 1,\n",
              "         'aids': 1,\n",
              "         'Probabilistic': 1,\n",
              "         'finite-state': 1,\n",
              "         'string': 1,\n",
              "         'transducer': 1,\n",
              "         'FSTs': 2,\n",
              "         'extremely': 1,\n",
              "         'popular': 1,\n",
              "         'due': 1,\n",
              "         'powerful': 2,\n",
              "         'composing': 1,\n",
              "         'Unfortunately': 1,\n",
              "         'fit': 1,\n",
              "         'TAL': 1,\n",
              "         'look': 1,\n",
              "         'fundamental': 2,\n",
              "         'underlying': 1,\n",
              "         'adopt': 1,\n",
              "         'view': 1,\n",
              "         'beyond': 1,\n",
              "         'horizon': 1,\n",
              "         'single': 3,\n",
              "         'campaign': 1,\n",
              "         'protocol': 1,\n",
              "         'After': 2,\n",
              "         'terminology': 1,\n",
              "         'clinical': 1,\n",
              "         'textual': 1,\n",
              "         'were': 1,\n",
              "         'Because': 1,\n",
              "         'requires': 1,\n",
              "         'substantial': 1,\n",
              "         'develop': 1,\n",
              "         'beneficial': 1,\n",
              "         'if': 1,\n",
              "         'designed': 1,\n",
              "         'easily': 1,\n",
              "         'fact': 2,\n",
              "         'link': 1,\n",
              "         'IE': 1,\n",
              "         'Prolog': 2,\n",
              "         'convolutional': 1,\n",
              "         ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2AmxzLbkmaH",
        "outputId": "6cd911a9-99b8-4ec3-b132-df9a12bfdd2f"
      },
      "source": [
        "bigram_data=[*nltk.bigrams(final_list)]\r\n",
        "bigram_data_count=nltk.FreqDist(bigram_data)\r\n",
        "print(bigram_data_count.items())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([(('Abstract', 'not'), 8), (('not', 'found'), 8), (('found', 'describe'), 1), (('describe', 'a'), 5), (('a', 'method'), 2), (('method', 'for'), 3), (('for', 'statistical'), 1), (('statistical', 'modeling'), 1), (('modeling', 'based'), 1), (('based', 'on'), 5), (('on', 'maximum'), 1), (('maximum', 'entropy'), 2), (('entropy', '.'), 1), (('.', 'We'), 14), (('We', 'present'), 2), (('present', 'a'), 5), (('a', 'maximum-likelihood'), 1), (('maximum-likelihood', 'approach'), 1), (('approach', 'for'), 5), (('for', 'automatically'), 1), (('automatically', 'constructing'), 1), (('constructing', 'maximum'), 1), (('entropy', 'model'), 1), (('model', 'and'), 1), (('and', 'describe'), 1), (('describe', 'how'), 1), (('how', 'to'), 1), (('to', 'implement'), 1), (('implement', 'this'), 1), (('this', 'approach'), 1), (('approach', 'efficiently'), 1), (('efficiently', ','), 1), ((',', 'using'), 1), (('using', 'a'), 1), (('a', 'example'), 1), (('example', 'several'), 1), (('several', 'problem'), 1), (('problem', 'in'), 1), (('in', 'natural'), 14), (('natural', 'language'), 61), (('language', 'processing'), 57), (('processing', '.'), 11), (('.', 'Scaling'), 1), (('Scaling', 'conditional'), 1), (('conditional', 'random'), 1), (('random', 'field'), 1), (('field', 'for'), 1), (('for', 'natural'), 4), (('processing', 'Terms'), 1), (('Terms', 'and'), 2), (('and', 'Conditions'), 2), (('Conditions', ':'), 2), ((':', 'Terms'), 1), ((':', 'Copyright'), 1), (('Copyright', 'in'), 1), (('in', 'work'), 1), (('work', 'deposited'), 1), (('deposited', 'in'), 1), (('in', 'Minerva'), 1), (('Minerva', 'Access'), 1), (('Access', 'is'), 1), (('is', 'retained'), 1), (('retained', 'by'), 1), (('by', 'the'), 3), (('the', 'The'), 1), (('The', 'paper'), 2), (('paper', 'address'), 1), (('address', 'the'), 2), (('the', 'issue'), 1), (('issue', 'of'), 2), (('of', 'cooperation'), 1), (('cooperation', 'between'), 1), (('between', 'linguistics'), 2), (('linguistics', 'and'), 2), (('and', 'natural'), 1), (('processing', '('), 12), (('(', 'NLP'), 22), (('NLP', ')'), 21), ((')', ','), 9), ((',', 'in'), 4), (('in', 'general'), 2), (('general', ','), 1), ((',', 'and'), 19), (('and', 'between'), 1), (('and', 'machine'), 4), (('machine', 'translation'), 3), (('translation', '('), 1), (('(', 'MT'), 1), (('MT', ')'), 1), (('in', 'particular'), 2), (('particular', '.'), 1), (('.', 'It'), 5), (('It', 'focus'), 1), (('focus', 'on'), 9), (('on', 'just'), 1), (('just', 'one'), 1), (('one', 'direction'), 1), (('direction', 'of'), 1), (('of', 'such'), 1), (('such', 'cooperation'), 1), (('cooperation', ','), 1), ((',', 'namely'), 1), (('namely', 'application'), 1), (('application', 'of'), 6), (('of', 'linguistics'), 1), (('linguistics', 'to'), 1), (('to', 'NLP'), 1), (('NLP', ','), 1), ((',', 'virtually'), 1), (('virtually', 'In'), 1), (('In', 'most'), 1), (('most', 'natural'), 1), (('processing', 'applications'), 1), (('applications', ','), 1), ((',', 'Description'), 2), (('Description', 'Logics'), 4), (('Logics', 'have'), 2), (('have', 'been'), 6), (('been', 'used'), 3), (('used', 'to'), 4), (('to', 'encode'), 1), (('encode', 'in'), 1), (('in', 'a'), 6), (('a', 'knowledge'), 2), (('knowledge', 'base'), 2), (('base', 'some'), 1), (('some', 'syntactic'), 1), (('syntactic', ','), 1), ((',', 'semantic'), 2), (('semantic', ','), 1), (('and', 'pragmatic'), 1), (('pragmatic', 'element'), 1), (('element', 'needed'), 1), (('needed', 'to'), 1), (('to', 'drive'), 2), (('drive', 'the'), 2), (('the', 'semantic'), 2), (('semantic', 'interpretation'), 2), (('interpretation', 'and'), 1), (('and', 'the'), 8), (('the', 'natural'), 2), (('language', 'generation'), 1), (('generation', 'processes'), 1), (('processes', '.'), 1), (('.', 'More'), 1), (('More', 'recently'), 1), (('recently', ','), 1), (('been', 'We'), 1), (('We', 'propose'), 2), (('propose', 'a'), 2), (('a', 'unified'), 1), (('unified', 'neural'), 1), (('neural', 'network'), 3), (('network', 'architecture'), 2), (('architecture', 'and'), 1), (('and', 'learning'), 2), (('learning', 'algorithm'), 1), (('algorithm', 'that'), 1), (('that', 'can'), 1), (('can', 'be'), 10), (('be', 'applied'), 1), (('applied', 'to'), 2), (('to', 'various'), 1), (('various', 'natural'), 1), (('processing', 'task'), 1), (('task', 'including'), 1), (('including', 'part-of-speech'), 1), (('part-of-speech', 'tagging'), 1), (('tagging', ','), 1), ((',', 'chunking'), 1), (('chunking', ','), 2), ((',', 'named'), 2), (('named', 'entity'), 2), (('entity', 'recognition'), 1), (('recognition', ','), 4), (('and', 'semantic'), 2), (('semantic', 'role'), 1), (('role', 'labeling'), 1), (('labeling', '.'), 1), (('.', 'This'), 15), (('This', 'versatility'), 1), (('versatility', 'is'), 1), (('is', 'achieved'), 1), (('achieved', 'by'), 1), (('by', 'trying'), 1), (('trying', 'to'), 1), (('to', 'avoid'), 2), (('avoid', 'task'), 1), (('task', 'Natural'), 1), (('Natural', 'Language'), 19), (('Language', 'Processing'), 20), (('Processing', 'The'), 1), (('The', 'subject'), 1), (('subject', 'of'), 1), (('of', 'Natural'), 3), (('Processing', 'can'), 1), (('be', 'considered'), 1), (('considered', 'in'), 1), (('in', 'both'), 2), (('both', 'broad'), 1), (('broad', 'and'), 1), (('and', 'narrow'), 1), (('narrow', 'senses'), 1), (('senses', '.'), 1), (('.', 'In'), 8), (('In', 'the'), 2), (('the', 'broad'), 1), (('broad', 'sense'), 1), (('sense', ','), 1), ((',', 'it'), 4), (('it', 'cover'), 1), (('cover', 'processing'), 1), (('processing', 'issue'), 1), (('issue', 'at'), 1), (('at', 'all'), 1), (('all', 'level'), 1), (('level', 'of'), 2), (('of', 'natural'), 13), (('language', 'understanding'), 2), (('understanding', ','), 1), ((',', 'including'), 1), (('including', 'speech'), 1), (('speech', 'recognition'), 3), ((',', 'syntactic'), 2), (('syntactic', 'and'), 1), (('semantic', 'analysis'), 1), (('analysis', 'of'), 3), (('of', 'sentence'), 1), (('sentence', 'Robots'), 1), (('Robots', 'that'), 1), (('that', 'interact'), 1), (('interact', 'with'), 1), (('with', 'human'), 1), (('human', 'face-to-face'), 1), (('face-to-face', 'using'), 1), (('using', 'natural'), 2), (('language', 'need'), 1), (('need', 'to'), 2), (('to', 'be'), 7), (('be', 'responsive'), 1), (('responsive', 'to'), 1), (('to', 'the'), 6), (('the', 'way'), 3), (('way', 'human'), 2), (('human', 'use'), 1), (('use', 'language'), 1), (('language', 'in'), 3), (('in', 'those'), 1), (('those', 'situations'), 1), (('situations', '.'), 1), (('a', 'psychologicallyinspired'), 1), (('psychologicallyinspired', 'natural'), 1), (('processing', 'system'), 3), (('system', 'for'), 2), (('for', 'robot'), 1), (('robot', 'which'), 1), (('which', 'performs'), 2), (('performs', 'incremental'), 1), (('incremental', 'semantic'), 1), (('interpretation', 'of'), 1), (('of', 'spoken'), 1), (('spoken', 'utterance'), 1), (('utterance', 'Natural'), 1), (('Natural', 'language'), 9), (('language', 'are'), 3), (('are', 'language'), 1), (('language', 'spoken'), 1), (('spoken', 'by'), 1), (('by', 'humans'), 1), (('humans', '.'), 1), (('.', 'Currently'), 1), (('Currently', 'we'), 1), (('we', 'are'), 1), (('are', 'not'), 4), (('not', 'yet'), 1), (('yet', 'at'), 1), (('at', 'the'), 4), (('the', 'point'), 1), (('point', 'where'), 1), (('where', 'these'), 1), (('these', 'language'), 1), (('in', 'all'), 1), (('all', 'of'), 1), (('of', 'their'), 1), (('their', 'unprocessed'), 1), (('unprocessed', 'form'), 1), (('form', 'can'), 1), (('be', 'understood'), 1), (('understood', 'by'), 1), (('by', 'computers'), 1), (('computers', '.'), 1), (('.', 'Natural'), 7), (('processing', 'is'), 3), (('is', 'the'), 8), (('the', 'collection'), 1), (('collection', 'of'), 1), (('of', 'technique'), 1), (('technique', 'employed'), 1), (('employed', 'to'), 1), (('to', 'try'), 1), (('try', 'and'), 1), (('and', 'accomplish'), 1), (('accomplish', 'that'), 1), (('that', 'goal'), 1), (('goal', '.'), 1), (('.', 'The'), 22), (('The', 'field'), 1), (('field', 'of'), 2), (('natural', 'ABSTRACT'), 1), (('ABSTRACT', ':'), 3), ((':', 'Ambiguity'), 1), (('Ambiguity', 'can'), 1), (('be', 'referred'), 1), (('referred', 'a'), 1), (('a', 'the'), 1), (('the', 'ability'), 1), (('ability', 'of'), 1), (('of', 'having'), 2), (('having', 'more'), 1), (('more', 'than'), 2), (('than', 'one'), 2), (('one', 'meaning'), 1), (('meaning', 'or'), 1), (('or', 'being'), 1), (('being', 'understood'), 1), (('understood', 'in'), 1), (('in', 'more'), 1), (('one', 'way'), 1), (('way', '.'), 1), (('are', 'ambiguous'), 1), (('ambiguous', ','), 1), ((',', 'so'), 1), (('so', 'computer'), 1), (('computer', 'are'), 1), (('not', 'able'), 1), (('able', 'to'), 2), (('to', 'understand'), 1), (('understand', 'language'), 1), (('language', 'the'), 1), (('way', 'people'), 1), (('people', 'do'), 1), (('do', '.'), 1), (('Processing', '('), 11), ((')', 'is'), 6), (('is', 'concerned'), 1), (('concerned', 'with'), 2), (('with', 'the'), 6), (('the', 'development'), 4), (('development', 'Introduction'), 1), (('Introduction', 'Statistical'), 1), (('Statistical', 'natural'), 1), (('(', 'SNLP'), 1), (('SNLP', ')'), 1), (('is', 'a'), 9), (('a', 'field'), 1), (('field', 'lying'), 1), (('lying', 'in'), 1), (('in', 'the'), 10), (('the', 'intersection'), 1), (('intersection', 'of'), 1), (('processing', 'and'), 1), (('machine', 'learning'), 9), (('learning', '.'), 3), (('.', 'SNLP'), 1), (('SNLP', 'di'), 1), (('di', '#'), 1), (('#', 'ers'), 1), (('ers', 'from'), 1), (('from', 'traditional'), 1), (('traditional', 'natural'), 1), (('processing', 'in'), 1), (('in', 'that'), 1), (('that', 'instead'), 1), (('instead', 'of'), 1), (('having', 'a'), 1), (('a', 'linguist'), 1), (('linguist', 'manually'), 1), (('manually', 'construct'), 1), (('construct', 'some'), 1), (('some', 'model'), 1), (('model', 'of'), 3), (('of', 'a'), 9), (('a', 'given'), 1), (('given', 'linguistic'), 1), (('linguistic', 'text'), 1), (('text', 'directly'), 1), (('directly', '('), 1), (('(', 'rather'), 1), (('rather', 'than'), 2), (('than', 'e.g'), 1), (('e.g', '.'), 1), (('.', 'title'), 1), (('title', 'and'), 1), (('and', 'abstracts'), 1), (('abstracts', ')'), 1), (('and', 'suggests'), 1), (('suggests', 'appropriate'), 1), (('appropriate', 'approach'), 1), (('approach', 'to'), 3), (('to', 'doing'), 1), (('doing', 'this'), 1), (('this', ','), 1), ((',', 'with'), 1), (('with', 'a'), 5), (('a', 'focus'), 3), (('on', 'the'), 6), (('the', 'role'), 2), (('role', 'of'), 2), (('paper', 'also'), 1), (('also', 'comment'), 1), (('comment', 'on'), 1), (('on', 'possible'), 1), (('possible', 'connection'), 1), (('connection', 'with'), 1), (('with', 'data'), 1), (('data', 'and'), 1), (('and', 'knowledge'), 1), (('knowledge', 'retrieval'), 1), (('retrieval', ','), 1), (('and', 'concludes'), 1), (('concludes', 'by'), 1), (('by', 'emphasizing'), 1), (('emphasizing', 'the'), 1), (('the', 'importance'), 1), (('importance', 'of'), 1), (('of', 'rigorous'), 1), (('rigorous', 'ABSTRACT'), 1), ((':', 'Language'), 1), (('Language', 'is'), 1), (('is', 'way'), 1), (('way', 'of'), 1), (('of', 'communicating'), 1), (('communicating', 'your'), 1), (('your', 'word'), 1), (('word', 'Language'), 1), (('Language', 'help'), 2), (('help', 'in'), 1), (('in', 'understanding'), 1), (('understanding', 'the'), 1), (('the', 'world'), 2), (('world', ','), 1), ((',', 'we'), 10), (('we', 'get'), 1), (('get', 'a'), 1), (('a', 'better'), 1), (('better', 'insight'), 1), (('insight', 'of'), 1), (('of', 'the'), 26), (('world', '.'), 1), (('.', 'Language'), 1), (('help', 'speaker'), 1), (('speaker', 'to'), 1), (('be', 'a'), 2), (('a', 'vague'), 1), (('vague', 'or'), 1), (('or', 'a'), 2), (('a', 'precise'), 1), (('precise', 'a'), 1), (('a', 'they'), 1), (('they', 'like'), 1), (('like', '.'), 1), (('.', 'NLP'), 1), (('NLP', 'Stands'), 1), (('Stands', 'for'), 1), (('language', 'processing..'), 1), (('processing..', 'Natural'), 1), (('are', 'those'), 1), (('those', 'language'), 1), (('language', 'that'), 2), (('that', 'are'), 2), (('are', 'spoken'), 1), (('spoken', 'We'), 1), (('We', 'report'), 1), (('report', 'experiment'), 1), (('experiment', 'on'), 1), (('the', 'use'), 3), (('use', 'of'), 8), (('of', 'standard'), 1), (('standard', 'natural'), 1), ((')', 'tool'), 1), (('tool', 'for'), 1), (('for', 'the'), 7), (('the', 'analysis'), 1), (('of', 'music'), 3), (('music', 'lyrics'), 1), (('lyrics', '.'), 2), (('.', 'A'), 2), (('A', 'significant'), 1), (('significant', 'amount'), 1), (('amount', 'of'), 1), (('music', 'audio'), 1), (('audio', 'ha'), 1), (('ha', 'lyrics'), 1), (('.', 'Lyrics'), 1), (('Lyrics', 'encode'), 1), (('encode', 'an'), 1), (('an', 'important'), 1), (('important', 'part'), 1), (('part', 'of'), 2), (('the', 'semantics'), 1), (('semantics', 'of'), 1), (('a', 'song'), 1), (('song', ','), 1), ((',', 'therefore'), 1), (('therefore', 'their'), 1), (('their', 'analysis'), 1), (('analysis', 'complement'), 1), (('complement', 'that'), 1), (('that', 'of'), 1), (('of', 'acoustic'), 1), (('acoustic', 'and'), 1), (('and', 'cultural'), 1), (('cultural', 'this'), 1), (('this', 'paper'), 8), (('paper', ','), 5), (('we', 'will'), 2), (('will', 'describe'), 1), (('a', 'simple'), 1), (('simple', 'rule-based'), 1), (('rule-based', 'approach'), 1), (('to', 'automated'), 1), (('automated', 'learning'), 1), (('learning', 'of'), 1), (('of', 'linguistic'), 4), (('linguistic', 'knowledge'), 2), (('knowledge', '.'), 1), (('This', 'approach'), 1), (('approach', 'ha'), 1), (('ha', 'been'), 3), (('been', 'shown'), 1), (('shown', 'for'), 1), (('for', 'a'), 6), (('a', 'number'), 3), (('number', 'of'), 5), (('of', 'task'), 3), (('task', 'to'), 1), (('to', 'capture'), 2), (('capture', 'information'), 1), (('information', 'in'), 1), (('a', 'clearer'), 1), (('clearer', 'and'), 1), (('and', 'more'), 2), (('more', 'direct'), 1), (('direct', 'fashion'), 1), (('fashion', 'without'), 1), (('without', 'a'), 1), (('a', 'compromise'), 1), (('compromise', 'in'), 1), (('in', 'performance'), 1), (('performance', '.'), 2), (('a', 'detailed'), 1), (('detailed', 'case'), 1), (('case', 'study'), 1), (('study', 'of'), 2), (('of', 'this'), 3), (('this', 'learning'), 1), (('learning', 'method'), 1), (('method', 'applied'), 1), (('to', 'part'), 1), (('of', 'speech'), 1), (('speech', 'tagging'), 1), (('tagging', 'This'), 1), (('This', 'paper'), 10), (('paper', 'focus'), 1), (('on', 'connectionist'), 1), (('connectionist', 'model'), 1), (('model', 'in'), 2), (('We', 'briefly'), 1), (('briefly', 'present'), 1), (('present', 'and'), 1), (('and', 'discus'), 2), (('discus', 'several'), 2), (('several', 'aspect'), 1), (('aspect', 'of'), 2), (('of', 'high'), 2), (('high', 'level'), 2), (('level', 'task'), 1), (('task', 'which'), 1), (('which', 'recently'), 1), (('recently', 'have'), 1), (('been', 'approached'), 1), (('approached', 'with'), 1), (('with', 'connectionism'), 1), (('connectionism', ','), 1), ((',', 'either'), 1), (('either', 'with'), 1), (('with', 'localist'), 1), (('localist', 'or'), 1), (('or', 'parallel'), 1), (('parallel', 'distributed'), 1), (('distributed', 'processing'), 1), (('processing', 'models'), 1), (('models', '.'), 1), (('.', 'Several'), 1), (('Several', 'interesting'), 1), (('interesting', 'architecture'), 1), (('architecture', 'process'), 1), (('process', 'of'), 4), (('of', 'language'), 4), (('understanding', '.'), 1), (('This', 'is'), 2), (('a', 'new'), 4), (('new', 'approach'), 1), (('approach', 'in'), 1), (('processing', 'based'), 1), (('the', 'deterministic'), 1), (('deterministic', 'chaotic'), 1), (('chaotic', 'behavior'), 1), (('behavior', 'of'), 1), (('of', 'dynamical'), 1), (('dynamical', 'systems'), 1), (('systems', '.'), 9), (('.', '1'), 8), (('1', 'this'), 2), (('paper', '('), 1), (('(', 'see'), 1), (('see', '['), 1), (('[', 'Schank'), 1), (('Schank', '86'), 1), (('86', ']'), 3), ((']', 'for'), 2), (('a', 'theoretical'), 1), (('theoretical', 'discussion'), 1), (('discussion', 'and'), 1), (('and', '['), 2), (('[', 'Kass'), 1), (('Kass', '86'), 1), ((']', 'and'), 1), (('[', 'Leake'), 1), (('Leake', 'and'), 1), (('and', 'Owens'), 1), (('Owens', '86'), 1), (('for', 'brief'), 1), (('brief', 'discussion'), 1), (('discussion', 'of'), 1), (('a', 'program'), 1), (('program', 'built'), 1), (('built', 'around'), 1), (('around', 'these'), 1), (('these', '.principles'), 1), (('.principles', ')'), 1), ((')', ';'), 1), ((';', 'the'), 2), (('the', 'goal'), 1), (('goal', 'here'), 1), (('here', 'is'), 1), (('is', 'simply'), 1), (('simply', 'to'), 1), (('to', 'point'), 1), (('point', 'out'), 2), (('out', 'how'), 1), (('how', 'our'), 1), (('our', 'interest'), 1), (('interest', 'in'), 1), (('processing', 'ha'), 1), (('ha', 'led'), 1), (('led', 'u'), 1), (('u', 'naturally'), 1), (('naturally', ','), 1), (('and', 'indeed'), 1), (('indeed', 'inevitably'), 1), (('inevitably', 'Objectives'), 1), (('Objectives', 'To'), 1), (('To', 'provide'), 1), (('provide', 'an'), 1), (('an', 'overview'), 2), (('overview', 'and'), 1), (('and', 'tutorial'), 1), (('tutorial', 'of'), 1), ((')', 'and'), 6), (('and', 'modern'), 1), (('modern', 'NLP-system'), 1), (('NLP-system', 'design'), 1), (('design', '.'), 1), (('.', 'Target'), 1), (('Target', 'audience'), 1), (('audience', 'This'), 1), (('This', 'tutorial'), 1), (('tutorial', 'target'), 1), (('target', 'the'), 1), (('the', 'medical'), 1), (('medical', 'informatics'), 1), (('informatics', 'generalist'), 1), (('generalist', 'who'), 1), (('who', 'ha'), 1), (('ha', 'limited'), 1), (('limited', 'acquaintance'), 1), (('acquaintance', 'with'), 1), (('the', 'principle'), 1), (('principle', 'behind'), 1), (('behind', 'NLP'), 1), (('NLP', 'and/or'), 1), (('and/or', 'limited'), 1), (('limited', 'knowledge'), 1), (('knowledge', 'of'), 1), (('the', 'current'), 4), (('current', 'state'), 2), (('state', 'This'), 1), (('paper', 'briefly'), 1), (('briefly', 'describes'), 1), (('describes', 'the'), 1), (('current', 'implementation'), 1), (('implementation', 'status'), 1), (('status', 'of'), 1), (('of', 'an'), 3), (('an', 'intelligent'), 1), (('intelligent', 'information'), 1), (('information', 'retrieval'), 4), (('retrieval', 'system'), 2), (('system', ','), 3), ((',', 'MARIE'), 1), (('MARIE', ','), 1), ((',', 'that'), 1), (('that', 'employ'), 1), (('employ', 'natural'), 1), (('processing', 'techniques'), 1), (('techniques', '.'), 2), (('.', 'Descriptive'), 1), (('Descriptive', 'caption'), 1), (('caption', 'are'), 2), (('are', 'used'), 1), (('to', 'iden-'), 1), (('iden-', 'tify'), 1), (('tify', 'photographic'), 1), (('photographic', 'image'), 1), (('image', 'concerning'), 1), (('concerning', 'various'), 1), (('various', 'military'), 1), (('military', 'projects'), 1), (('projects', '.'), 1), (('The', 'caption'), 1), (('are', 'parsed'), 1), (('parsed', 'based'), 1), (('based', 'and'), 1), (('and', 'literature'), 1), (('literature', 'resources'), 1), (('resources', '.'), 2), (('We', 'describe'), 4), (('describe', 'here'), 1), (('here', 'a'), 1), (('a', 'system'), 1), (('for', 'agent'), 1), (('agent', 'directed'), 1), (('directed', 'natural'), 1), (('processing', 'to'), 3), (('to', 'extract'), 3), (('extract', 'information'), 1), (('information', 'from'), 2), (('from', 'journal'), 1), (('journal', 'articles'), 1), (('articles', '.'), 1), (('.', 'An'), 1), (('An', 'interface'), 1), (('interface', 'wa'), 1), (('wa', 'developed'), 1), (('developed', 'to'), 1), (('to', 'permit'), 1), (('permit', 'curation'), 1), (('curation', 'of'), 1), (('the', 'NLP'), 1), (('NLP', 'result'), 1), (('result', 'and'), 1), (('and', 'deposition'), 1), (('deposition', 'of'), 1), (('of', 'accepted'), 1), (('accepted', 'result'), 1), (('result', 'into'), 1), (('into', 'a'), 3), (('base', '.'), 1), (('.', 'Motivation'), 1), (('Motivation', ':'), 1), ((':', 'The'), 1), (('The', 'advent'), 1), (('advent', 'of'), 1), (('high', 'to'), 1), (('to', 'evaluation'), 1), (('evaluation', 'in'), 2), (('in', 'speech'), 1), (('speech', 'processing'), 1), (('.', 'Part'), 1), (('Part', '2'), 1), (('2', 'survey'), 1), (('survey', 'significant'), 1), (('significant', 'evaluation'), 1), (('evaluation', 'work'), 1), (('work', 'done'), 1), (('done', 'so'), 1), (('so', 'far'), 2), (('far', ','), 1), ((',', 'for'), 1), (('for', 'instance'), 1), (('instance', 'in'), 1), (('in', 'machine'), 1), (('translation', ','), 2), (('discus', 'the'), 1), (('the', 'particular'), 1), (('particular', 'problem'), 1), (('problem', 'of'), 1), (('of', 'generic'), 1), (('generic', 'system'), 1), (('system', 'evaluation'), 1), (('evaluation', '.'), 1), (('The', 'conclusion'), 1), (('conclusion', 'is'), 1), (('is', 'that'), 1), (('that', 'evaluation'), 1), (('evaluation', 'strategy'), 1), (('strategy', 'and'), 1), (('and', 'technique'), 1), (('technique', 'for'), 3), (('for', 'NLP'), 1), (('NLP', 'need'), 1), (('need', 'much'), 1), (('much', 'more'), 1), (('more', 'development'), 1), (('development', ','), 1), (('particular', 'similar'), 1), (('similar', 'to'), 1), (('human', 'intuitively'), 1), (('intuitively', 'do'), 1), (('do', 'in'), 1), (('in', 'order'), 1), (('order', 'to'), 2), (('to', 'eliminate'), 1), (('eliminate', 'noisy'), 1), (('noisy', 'content'), 1), (('content', '.'), 1), (('In', 'this'), 8), (('we', 'describe'), 2), (('a', 'combination'), 1), (('combination', 'of'), 1), (('of', 'HTML'), 1), (('HTML', 'DOM'), 1), (('DOM', 'analysis'), 1), (('analysis', 'and'), 1), (('and', 'Natural'), 2), ((')', 'technique'), 2), (('for', 'automated'), 1), (('automated', 'extraction'), 1), (('extraction', 'of'), 2), (('of', 'main'), 1), (('main', 'article'), 1), (('article', 'with'), 1), (('with', 'associated'), 1), (('associated', 'image'), 1), (('image', 'from'), 1), (('from', 'web'), 1), (('web', 'pages'), 1), (('pages', '.'), 1), (('.', 'Abstract'), 3), (('Abstract', '--'), 1), (('--', 'Natural'), 1), (('Processing', 'is'), 1), (('a', 'theoretically'), 1), (('theoretically', 'motivated'), 1), (('motivated', 'range'), 1), (('range', 'of'), 6), (('of', 'computational'), 1), (('computational', 'technique'), 1), (('for', 'analysing'), 1), (('analysing', 'and'), 1), (('and', 'representing'), 1), (('representing', 'naturally'), 1), (('naturally', 'occurring'), 1), (('occurring', 'text'), 1), (('text', 'at'), 1), (('at', 'one'), 1), (('one', 'or'), 1), (('or', 'more'), 1), (('more', 'level'), 1), (('linguistic', 'analysis'), 1), (('analysis', 'for'), 1), (('the', 'purpose'), 2), (('purpose', 'of'), 2), (('of', 'achieving'), 1), (('achieving', 'human-like'), 1), (('human-like', 'language'), 1), (('processing', 'for'), 2), (('a', 'range'), 1), (('task', 'This'), 1), (('paper', 'review'), 2), (('review', 'the'), 2), (('the', 'process'), 4), (('process', 'involved'), 1), (('involved', 'in'), 1), (('in', 'Natural'), 2), ((')', '.'), 4), (('It', 'then'), 1), (('then', 'demonstrates'), 1), (('demonstrates', 'the'), 1), (('the', 'various'), 1), (('various', 'kind'), 1), (('kind', 'of'), 2), (('of', 'choice'), 1), (('choice', 'that'), 1), (('that', 'need'), 2), (('need', 'be'), 1), (('be', 'taken'), 1), (('taken', 'during'), 1), (('during', 'the'), 2), (('the', 'execution'), 1), (('execution', 'of'), 1), (('the', 'word'), 3), (('word', 'morphology'), 1), (('morphology', ','), 1), ((',', 'the'), 3), (('the', 'syntactic'), 1), (('syntactic', 'text'), 1), (('text', 'analysis'), 2), (('analysis', ','), 2), ((',', 'or'), 2), (('or', 'text'), 1), (('text', 'generation'), 1), (('generation', 'components'), 1), (('components', '.'), 1), (('It', 'compare'), 1), (('compare', 'the'), 1), (('the', 'time'), 1), (('time', 'complexity'), 1), (('complexity', 'This'), 1), (('This', 'article'), 2), (('article', 'focus'), 1), (('the', 'derivation'), 1), (('derivation', 'of'), 1), (('of', 'large'), 1), (('large', 'lexicon'), 1), (('lexicon', 'for'), 1), (('describe', 'the'), 2), (('development', 'of'), 3), (('a', 'dictionary'), 1), (('dictionary', 'support'), 1), (('support', 'environment'), 1), (('environment', 'linking'), 1), (('linking', 'a'), 1), (('a', 'restructured'), 1), (('restructured', 'version'), 1), (('version', 'of'), 2), (('the', 'Longman'), 1), (('Longman', 'Dictionary'), 1), (('Dictionary', 'of'), 1), (('of', 'Contemporary'), 1), (('Contemporary', 'English'), 1), (('English', 'to'), 1), (('to', 'natural'), 2), (('processing', 'systems'), 2), (('The', 'process'), 1), (('process', 'We'), 1), (('We', 'introduce'), 1), (('introduce', 'a'), 1), (('for', 'analyzing'), 1), (('analyzing', 'the'), 1), (('the', 'complexity'), 1), (('complexity', 'of'), 2), (('processing', 'tasks'), 1), (('tasks', ','), 4), (('and', 'for'), 2), (('for', 'predicting'), 1), (('predicting', 'the'), 1), (('the', 'difficulty'), 1), (('difficulty', 'new'), 1), (('new', 'NLP'), 1), (('NLP', 'tasks'), 3), (('tasks', '.'), 4), (('.', 'Our'), 2), (('Our', 'complexity'), 1), (('complexity', 'measure'), 1), (('measure', 'are'), 2), (('are', 'derived'), 1), (('derived', 'from'), 1), (('from', 'the'), 1), (('the', 'Kolmogorov'), 1), (('Kolmogorov', 'complexity'), 1), (('a', 'class'), 1), (('class', 'of'), 1), (('of', 'automaton'), 1), (('automaton', '—'), 1), (('—', 'meaning'), 1), (('meaning', 'automata'), 1), (('automata', ','), 1), ((',', 'whose'), 1), (('whose', 'purpose'), 1), (('purpose', 'is'), 1), (('is', 'to'), 6), (('extract', 'relevant'), 1), (('relevant', 'piece'), 1), (('piece', ','), 1), ((',', 'sounds'), 1), (('sounds', ','), 1), ((',', 'text'), 4), (('text', 'and'), 1), (('and', 'motion'), 1), (('motion', '.'), 1), (('The', 'technique'), 1), (('technique', 'developed'), 1), (('developed', 'from'), 1), (('from', 'deep'), 1), (('deep', 'learning'), 2), (('learning', 'research'), 1), (('research', 'have'), 1), (('have', 'already'), 1), (('already', 'been'), 1), (('been', 'impacting'), 1), (('impacting', 'the'), 1), (('the', 'research'), 2), (('research', 'of'), 1), (('language', 'process'), 1), (('process', '.'), 1), (('the', 'recent'), 1), (('recent', 'research'), 3), (('research', 'on'), 1), (('on', 'deep'), 1), (('learning', ','), 2), (('it', 'application'), 1), (('application', 'and'), 1), (('and', 'recent'), 1), (('recent', 'development'), 2), (('development', 'in'), 2), (('1', 'This'), 1), (('is', 'an'), 4), (('an', 'author-produced'), 1), (('author-produced', 'version'), 1), (('a', 'paper'), 1), (('paper', 'published'), 1), (('published', 'in'), 1), (('in', 'The'), 1), (('The', 'Abstract—Natural'), 1), (('Abstract—Natural', 'language'), 1), (('the', 'application'), 5), (('of', 'automated'), 1), (('automated', 'parsing'), 1), (('parsing', 'and'), 1), (('learning', 'technique'), 2), (('technique', 'to'), 2), (('to', 'analyze'), 1), (('analyze', 'standard'), 1), (('standard', 'text'), 1), (('text', '.'), 1), (('.', 'Applications'), 1), (('Applications', 'of'), 1), (('of', 'NLP'), 4), (('NLP', 'to'), 2), (('to', 'requirement'), 1), (('requirement', 'engineering'), 1), (('engineering', 'include'), 1), (('include', 'extraction'), 1), (('of', 'ontology'), 1), (('ontology', 'from'), 1), (('from', 'a'), 3), (('a', 'requirement'), 1), (('requirement', 'specification'), 1), (('specification', ','), 1), (('and', 'use'), 2), (('to', 'verify'), 1), (('verify', 'the'), 1), (('the', 'consistency'), 1), (('consistency', 'statistical'), 1), (('statistical', 'baseline'), 1), (('baseline', 'including'), 1), (('including', ':'), 1), ((':', 'the'), 2), (('the', 'forgiving'), 1), (('forgiving', 'nature'), 1), (('nature', 'but'), 1), (('but', 'broad'), 1), (('broad', 'coverage'), 1), (('coverage', 'of'), 1), (('the', 'typical'), 1), (('typical', 'retrieval'), 1), (('retrieval', 'task'), 1), (('task', ';'), 1), (('the', 'lack'), 1), (('lack', 'of'), 1), (('of', 'good'), 1), (('good', 'weighting'), 1), (('weighting', 'scheme'), 1), (('scheme', 'for'), 1), (('for', 'compound'), 1), (('compound', 'index'), 1), (('index', 'terms'), 1), (('terms', ';'), 1), ((';', 'and'), 1), (('the', 'implicit'), 1), (('implicit', 'linguistic'), 1), (('linguistic', 'processing'), 1), (('processing', 'inherent'), 1), (('inherent', 'in'), 1), (('the', 'statistical'), 1), (('statistical', 'methods'), 1), (('methods', '.'), 1), (('processing', 'technique'), 2), (('technique', 'may'), 1), (('may', 'be'), 1), (('be', 'more'), 1), (('more', 'important'), 1), (('important', 'Work'), 1), (('Work', 'in'), 1), (('in', 'computational'), 2), (('computational', 'linguistics'), 2), (('linguistics', 'began'), 1), (('began', 'very'), 1), (('very', 'soon'), 1), (('soon', 'after'), 1), (('after', 'the'), 1), (('the', 'first'), 1), (('first', 'computer'), 1), (('computer', '('), 1), (('(', 'Booth'), 1), (('Booth', ','), 1), ((',', 'Brandwood'), 1), (('Brandwood', 'and'), 1), (('and', 'Cleave'), 1), (('Cleave', '1958'), 1), (('1958', ')'), 1), ((',', 'yet'), 1), (('yet', 'in'), 1), (('the', 'intervening'), 1), (('intervening', 'four'), 1), (('four', 'decade'), 1), (('decade', 'there'), 1), (('there', 'ha'), 1), (('been', 'a'), 1), (('a', 'pervasive'), 1), (('pervasive', 'feeling'), 1), (('feeling', 'that'), 1), (('that', 'progress'), 1), (('progress', 'in'), 1), (('in', 'computer'), 1), (('computer', 'understanding'), 1), (('understanding', 'of'), 2), (('language', 'ha'), 1), (('ha', 'not'), 1), (('not', 'been'), 1), (('been', 'commensurate'), 1), (('commensurate', 'the'), 1), (('the', 'voice'), 1), (('voice', 'recognition'), 1), (('recognition', 'for'), 1), (('a', 'natural'), 2), (('language', '('), 1), (('(', 'Tamil'), 1), (('Tamil', ')'), 1), ((')', 'by'), 1), (('by', 'combining'), 1), (('combining', 'the'), 1), (('the', 'digital'), 1), (('digital', 'and'), 1), (('and', 'mathematical'), 1), (('mathematical', 'knowledge'), 1), (('knowledge', 'using'), 1), (('using', 'MFCC'), 1), (('MFCC', 'and'), 1), (('and', 'DTW'), 1), (('DTW', 'to'), 1), (('extract', 'and'), 1), (('and', 'match'), 1), (('match', 'the'), 1), (('the', 'feature'), 1), (('feature', 'to'), 1), (('to', 'improve'), 2), (('improve', 'the'), 1), (('the', 'accuracy'), 1), (('accuracy', 'for'), 1), (('for', 'better'), 1), (('better', 'performance'), 1), (('Abstract', ':'), 2), ((':', 'Testing'), 1), (('Testing', 'against'), 1), (('against', 'natural'), 1), (('language', 'requirement'), 1), (('requirement', 'is'), 1), (('the', 'standard'), 1), (('standard', 'approach'), 1), (('for', 'system'), 1), (('system', 'and'), 1), (('and', 'acceptance'), 1), (('acceptance', 'testing'), 1), (('testing', '.'), 1), (('This', 'test'), 1), (('test', 'is'), 1), (('is', 'often'), 1), (('often', 'performed'), 1), (('performed', 'by'), 1), (('by', 'an'), 1), (('an', 'independent'), 1), (('independent', 'test'), 1), (('test', 'organization'), 1), (('organization', 'unfamiliar'), 1), (('unfamiliar', 'with'), 1), (('application', 'area'), 1), (('area', '.'), 1), (('The', 'only'), 1), (('only', 'thing'), 1), (('thing', 'the'), 1), (('the', 'tester'), 1), (('tester', 'have'), 1), (('have', 'to'), 1), (('to', 'go'), 1), (('go', 'by'), 1), (('by', 'are'), 1), (('are', 'the'), 2), (('the', 'written'), 1), (('written', 'requirements'), 1), (('requirements', '.'), 1), (('.', 'So'), 2), (('So', 'Abstract'), 1), (('found', 'conversational'), 1), (('conversational', 'partners'), 1), (('partners', '.'), 1), (('.', 'But'), 1), (('But', 'it'), 1), (('it', 'also'), 1), (('also', 'provides'), 1), (('provides', 'u'), 1), (('u', 'with'), 1), (('with', 'information'), 1), (('information', 'about'), 3), (('about', 'being'), 1), (('being', 'creative'), 1), (('creative', ','), 1), ((',', 'making'), 1), (('making', 'associations'), 1), (('associations', ','), 1), ((',', 'storytelling'), 1), (('storytelling', 'and'), 1), (('and', 'language'), 1), (('language', 'use'), 1), (('use', '.'), 1), (('.', 'Many'), 1), (('Many', 'more'), 1), (('more', 'subtlety'), 1), (('subtlety', 'in'), 1), (('in', 'face-to-face'), 1), (('face-to-face', 'and'), 1), (('and', 'multiparty'), 1), (('multiparty', 'interaction'), 1), (('interaction', 'can'), 1), (('be', 'added'), 1), (('added', ','), 1), ((',', 'such'), 2), (('such', 'a'), 3), (('a', 'using'), 1), (('using', 'humor'), 1), (('humor', 'to'), 1), (('to', 'persuade'), 1), (('persuade', 'and'), 1), (('and', 'dominate'), 1), (('dominate', ','), 1), ((',', 'to'), 4), (('to', 'soften'), 1), (('soften', 'or'), 1), (('or', 'avoid'), 1), (('avoid', 'a'), 1), (('a', 'face'), 1), (('face', 'threatening'), 1), (('threatening', 'act'), 1), (('act', 'Abstract'), 1), (('found', 'In'), 2), (('In', 'recent'), 1), (('recent', 'years'), 1), (('years', ','), 2), ((',', 'machine'), 4), (('learning', '('), 1), (('(', 'ML'), 2), (('ML', ')'), 2), ((')', 'ha'), 1), (('used', 'more'), 1), (('more', 'and'), 1), (('more', 'to'), 1), (('to', 'solve'), 2), (('solve', 'complex'), 1), (('complex', 'task'), 1), (('task', 'in'), 1), (('in', 'different'), 2), (('different', 'disciplines'), 1), (('disciplines', ','), 1), ((',', 'ranging'), 1), (('ranging', 'from'), 1), (('from', 'Data'), 1), (('Data', 'Mining'), 1), (('Mining', 'to'), 1), (('to', 'Information'), 1), (('Information', 'We'), 1), (('We', 'argue'), 1), (('argue', 'that'), 4), (('that', 'manual'), 1), (('manual', 'and'), 1), (('and', 'automatic'), 1), (('automatic', 'thesaurus'), 1), (('thesaurus', 'are'), 1), (('are', 'alternative'), 1), (('alternative', 'resource'), 1), (('resource', 'for'), 1), (('the', 'same'), 3), (('same', 'NLP'), 1), (('This', 'involves'), 1), (('involves', 'the'), 1), (('the', 'radical'), 1), (('radical', 'step'), 1), (('step', 'of'), 1), (('of', 'interpreting'), 1), (('interpreting', 'manual'), 1), (('manual', 'thesaurus'), 1), (('thesaurus', 'a'), 1), (('a', 'classification'), 1), (('classification', 'of'), 1), (('of', 'word'), 1), (('word', 'rather'), 1), (('than', 'word'), 1), (('word', 'senses'), 1), (('senses', ':'), 1), (('the', 'case'), 1), (('case', 'for'), 1), (('for', 'this'), 1), (('this', 'is'), 1), (('is', 'made'), 1), (('made', '.'), 1), (('The', 'range'), 1), (('of', 'role'), 1), (('role', 'for'), 1), (('for', 'thesaurus'), 1), (('thesaurus', 'within'), 1), (('within', 'NLP'), 2), (('NLP', 'is'), 1), (('is', 'briefly'), 1), (('briefly', 'presented'), 1), (('presented', 'and'), 1), (('the', 'WASPS'), 1), (('WASPS', 'thesaurus'), 1), (('thesaurus', 'is'), 1), (('is', 'introduced'), 1), (('introduced', '.'), 1), (('.', 'Thesaurus'), 1), (('Thesaurus', 'evaluation'), 1), (('evaluation', 'is'), 1), (('is', 'now'), 1), (('now', 'becoming'), 1), (('becoming', 'urgent'), 1), (('urgent', '.'), 1), (('A', 'range'), 1), (('of', 'evaluation'), 1), (('evaluation', 'strategies'), 1), (('strategies', ','), 1), ((',', 'all'), 2), (('all', 'embedded'), 1), (('embedded', 'within'), 1), ((',', 'is'), 1), (('is', 'proposed'), 1), (('proposed', '.'), 1), (('.', 'Introduction'), 1), (('Introduction', 'Patterns'), 1), (('Patterns', 'in'), 1), (('in', 'music'), 1), (('music', 'have'), 1), (('been', 'the'), 1), (('the', 'object'), 1), (('object', 'of'), 1), (('of', 'intensive'), 1), (('intensive', 'study'), 1), (('study', 'in'), 1), (('the', 'past'), 2), (('past', 'years'), 1), (('years', '.'), 2), (('.', '\\\\One'), 1), (('\\\\One', 'of'), 1), (('of', 'analyzing'), 1), (('analyzing', 'musical'), 1), (('musical', 'structure'), 1), (('structure', 'and'), 1), (('and', 'form'), 1), (('form', 'is'), 1), (('to', 'discover'), 1), (('discover', 'the'), 1), (('the', 'pattern'), 2), (('pattern', 'that'), 1), (('are', 'explicit'), 1), (('explicit', 'or'), 1), (('or', 'implicit'), 1), (('implicit', 'in'), 1), (('in', 'musical'), 1), (('musical', 'works'), 1), (('works', \"''\"), 1), ((\"''\", 'Simon'), 1), (('Simon', '['), 1), (('[', '13'), 1), (('13', ']'), 1), ((']', '.'), 1), (('.', 'Patterns'), 1), (('Patterns', 'comprise'), 1), (('comprise', 'periodicity'), 1), (('periodicity', ','), 1), ((',', 'make'), 2), (('make', 'use'), 2), (('of', 'alphabets'), 1), (('alphabets', ','), 1), ((',', 'can'), 1), (('be', 'compound'), 1), (('compound', '('), 1), (('(', 'made'), 1), (('made', 'up'), 1), (('up', 'of'), 1), (('of', 'subpatterns'), 1), (('subpatterns', ')'), 1), (('and', 'posse'), 1), (('posse', 'phrase'), 1), (('phrase', 'structure'), 1), (('structure', 'with'), 1), (('with', 'various'), 1), (('various', 'form'), 1), (('form', 'of'), 1), (('of', 'punctuation'), 1), (('punctuation', '.'), 1), (('.', 'Traditionally'), 1), (('Traditionally', ','), 1), ((',', 'composer'), 1), (('composer', 'have'), 1), (('have', 'employed'), 1), (('employed', 'pattern'), 1), (('pattern', 'propagation'), 2), (('propagation', 'intuitively'), 1), (('intuitively', ','), 1), ((',', 'but'), 2), (('but', 'algorithmic'), 1), (('algorithmic', 'composition'), 1), (('composition', 'technique'), 1), (('technique', 'allow'), 1), (('allow', 'the'), 1), (('propagation', 'to'), 1), (('be', 'formalized'), 1), (('formalized', ','), 1), ((',', 'albeit'), 1), (('albeit', 'a'), 1), (('a', 'high'), 1), (('level', '.'), 1), (('.', 'During'), 2), (('During', 'composition'), 1), (('composition', ','), 1), (('all', 'the'), 2), (('the', 'musical'), 1), (('musical', 'pattern'), 1), (('pattern', 'evolve'), 1), (('evolve', 'according'), 1), (('according', 'to'), 1), (('the', 'rule'), 1), (('rule', 'and'), 1), (('and', 'constraint'), 1), (('constraint', 'specied'), 1), (('specied', 'at'), 1), (('the', 'design'), 3), (('design', 'stage'), 1), (('stage', '.'), 1), (('In', 'jazz'), 1), (('jazz', 'improvisation'), 1), (('improvisation', ','), 1), (('the', 'musician'), 1), (('musician', 'invents'), 1), (('invents', 'a'), 1), (('a', 'solo'), 1), (('solo', 'guided'), 1), (('guided', 'by'), 1), (('by', 'a'), 1), (('a', 'progression'), 1), (('progression', 'of'), 1), (('of', 'chord'), 1), (('chord', '('), 1), (('(', 'the'), 1), (('the', 'changes'), 1), (('changes', ')'), 1), (('.', 'One'), 2), (('One', 'approach'), 1), (('approach', '['), 1), (('[', '1'), 1), (('1', ']'), 1), ((']', 'to'), 1), (('to', 'learn'), 1), (('learn', 'improvising'), 1), (('improvising', 'is'), 1), (('to', 'memorize'), 1), (('memorize', 'pattern'), 1), (('pattern', '('), 1), (('(', 'short'), 1), (('short', 'chunk'), 1), (('chunk', 'of'), 1), (('music', ')'), 1), ((')', 'that'), 2), (('that', 't'), 2), (('t', 'sub-progressions'), 1), (('sub-progressions', ','), 1), (('and', 'to'), 1), (('to', 'concatenate'), 1), (('concatenate', 'them'), 1), (('them', 'to'), 1), (('to', 'form'), 1), (('form', 'a'), 2), (('a', 'whole'), 2), (('whole', 'solo'), 1), (('solo', 'that'), 1), (('t', 'a'), 1), (('whole', 'progression'), 1), (('progression', '.'), 1), (('One', 'Abstract'), 1), (('Abstract', 'Many'), 1), (('Many', 'information'), 1), (('retrieval', '('), 1), (('(', 'IR'), 1), (('IR', ')'), 1), ((')', 'system'), 1), (('system', 'retrieve'), 1), (('retrieve', 'relevant'), 1), (('relevant', 'document'), 2), (('document', 'based'), 1), (('on', 'exact'), 1), (('exact', 'matching'), 1), (('matching', 'of'), 1), (('of', 'keywords'), 1), (('keywords', 'between'), 1), (('between', 'a'), 1), (('a', 'query'), 1), (('query', 'and'), 1), (('and', 'documents'), 1), (('documents', '.'), 1), (('This', 'method'), 1), (('method', 'degrades'), 1), (('degrades', 'precision'), 1), (('precision', 'rate'), 2), (('rate', '.'), 1), (('In', 'order'), 1), (('solve', 'the'), 1), (('the', 'problem'), 1), (('problem', ','), 1), (('we', 'collected'), 1), (('collected', 'semantically'), 1), (('semantically', 'related'), 1), (('related', 'word'), 1), (('word', 'and'), 2), (('and', 'assigned'), 1), (('assigned', 'semantic'), 1), (('semantic', 'relationship'), 1), (('relationship', 'used'), 1), (('used', 'in'), 5), (('general', 'thesaurus'), 1), (('thesaurus', 'and'), 1), (('and', 'a'), 1), (('a', 'special'), 1), (('special', 'relationship'), 1), (('relationship', 'called'), 1), (('called', 'keyfact'), 1), (('keyfact', 'term'), 1), (('term', '('), 1), (('(', 'FT'), 1), (('FT', ')'), 1), ((')', 'manually'), 1), (('manually', '.'), 1), (('In', 'addition'), 1), (('addition', 'to'), 1), (('semantic', 'knowledge'), 1), (('knowledge', ','), 1), (('we', 'automatically'), 1), (('automatically', 'constructed'), 1), (('constructed', 'statistic'), 1), (('statistic', 'knowledge'), 1), (('knowledge', 'based'), 1), (('the', 'concept'), 1), (('concept', 'of'), 2), (('of', 'mutual'), 1), (('mutual', 'information'), 1), (('information', '.'), 2), (('.', 'Keyfact'), 2), (('Keyfact', 'is'), 1), (('an', 'extended'), 1), (('extended', 'concept'), 1), (('of', 'keyword'), 1), (('keyword', 'represented'), 1), (('represented', 'by'), 1), (('by', 'noun'), 1), (('noun', 'and'), 1), (('and', 'compound'), 1), (('compound', 'noun'), 1), (('noun', '.'), 1), (('Keyfact', 'can'), 1), (('a', 'verb'), 1), (('verb', 'and'), 1), (('and', 'an'), 1), (('an', 'adjective'), 1), (('adjective', 'including'), 1), (('including', 'subject'), 1), (('subject', 'or'), 1), (('or', 'object'), 1), (('object', 'term'), 1), (('term', '.'), 1), (('We', 'first'), 1), (('first', 'retrieved'), 1), (('retrieved', 'relevant'), 1), (('document', 'with'), 1), (('with', 'original'), 1), (('original', 'query'), 1), (('query', 'using'), 1), (('using', 'tf'), 1), (('tf', '*'), 1), (('*', 'idf'), 1), (('idf', 'weighting'), 1), (('weighting', 'formula'), 1), (('formula', 'and'), 1), (('and', 'then'), 4), (('then', 'an'), 1), (('an', 'expanded'), 1), (('expanded', 'query'), 1), (('query', 'including'), 1), (('including', 'keyfacts'), 1), (('keyfacts', 'is'), 1), (('is', 'used'), 1), (('both', 'second'), 1), (('second', 'document'), 1), (('document', 'ranking'), 1), (('ranking', 'and'), 1), (('and', 'word'), 1), (('word', 'sense'), 3), (('sense', 'disambiguating'), 1), (('disambiguating', '.'), 1), (('So', 'we'), 1), (('we', 'made'), 1), (('made', 'an'), 1), (('an', 'improvement'), 1), (('improvement', 'in'), 2), (('in', 'precision'), 1), (('rate', 'using'), 1), (('using', 'keyfact'), 1), (('keyfact', 'network'), 1), (('network', '.'), 1), (('paper', 'we'), 2), (('we', 'argue'), 1), (('that', 'questionanswering'), 1), (('questionanswering', '('), 1), (('(', 'QA'), 1), (('QA', ')'), 1), ((')', 'over'), 1), (('over', 'technical'), 1), (('technical', 'domain'), 1), (('domain', 'is'), 1), (('is', 'distinctly'), 1), (('distinctly', 'different'), 1), (('different', 'from'), 1), (('from', 'TREC-based'), 1), (('TREC-based', 'QA'), 1), (('QA', 'or'), 1), (('or', 'Web-based'), 1), (('Web-based', 'QA'), 1), (('QA', 'and'), 1), (('and', 'it'), 1), (('it', 'can'), 1), (('can', 'not'), 1), (('not', 'benefit'), 1), (('benefit', 'lom'), 1), (('lom', 'data-intensive'), 1), (('data-intensive', 'approach'), 1), (('approach', 'Universit'), 1), (('Universit', '&'), 1), (('&', 'quot'), 1), (('quot', ';'), 1), ((';', 'at'), 1), (('at', 'de'), 1), (('de', 'Saarlandes'), 1), (('Saarlandes', 'Proceedings'), 1), (('Proceedings', 'of'), 1), (('the', 'Workshop'), 1), (('Workshop', 'on'), 1), (('on', 'uni-hamburg.de'), 1), (('uni-hamburg.de', 'Abstract'), 1), (('found', 'Abstract'), 2), (('found', 'SRI'), 1), (('SRI', 'ha'), 1), (('ha', 'developed'), 1), (('developed', 'a'), 2), (('new', 'architecture'), 1), (('architecture', 'for'), 1), (('for', 'integrating'), 1), (('integrating', 'speech'), 1), (('speech', 'and'), 1), (('and', 'natural-language'), 1), (('natural-language', 'processing'), 1), (('processing', 'that'), 1), (('that', 'applies'), 1), (('applies', 'linguistic'), 1), (('linguistic', 'constraint'), 1), (('constraint', 'during'), 1), (('during', 'recognition'), 1), (('recognition', 'by'), 1), (('by', 'incrementally'), 1), (('incrementally', 'expanding'), 1), (('expanding', 'the'), 1), (('the', 'state-transition'), 1), (('state-transition', 'network'), 1), (('network', 'embodied'), 1), (('embodied', 'in'), 1), (('a', 'unification'), 1), (('unification', 'grammar'), 1), (('grammar', '.'), 1), (('We', 'compare'), 1), (('compare', 'this'), 1), (('this', 'dynamic-gralnlnar-network'), 1), (('dynamic-gralnlnar-network', '('), 1), (('(', 'DGN'), 1), (('DGN', ')'), 1), ((')', 'approach'), 1), (('approach', 'This'), 2), (('This', 'chapter'), 4), (('chapter', 'considers'), 1), (('considers', 'the'), 1), (('the', 'revolution'), 1), (('revolution', 'that'), 1), (('that', 'ha'), 1), (('ha', 'taken'), 1), (('taken', 'place'), 1), (('place', 'in'), 1), (('processing', 'research'), 1), (('research', 'over'), 1), (('over', 'the'), 4), (('the', 'last'), 4), (('last', 'five'), 1), (('five', 'years'), 1), (('It', 'begin'), 1), (('begin', 'by'), 1), (('by', 'providing'), 1), (('providing', 'a'), 2), (('a', 'brief'), 2), (('brief', 'guide'), 1), (('guide', 'to'), 1), (('the', 'structure'), 1), (('structure', 'of'), 1), (('the', 'field'), 2), (('field', 'and'), 1), (('then', 'present'), 1), (('a', 'caricature'), 1), (('caricature', 'of'), 1), (('of', 'two'), 1), (('two', 'competing'), 1), (('competing', 'paradigm'), 1), (('paradigm', 'of'), 1), (('of', '1980s'), 1), (('1980s', 'NLP'), 1), (('NLP', 'research'), 1), (('research', 'and'), 2), (('and', 'indicates'), 1), (('indicates', 'the'), 1), (('the', 'reason'), 1), (('reason', 'visual'), 1), (('visual', 'development'), 1), (('development', 'environment'), 1), (('environment', 'to'), 1), (('to', 'support'), 1), (('support', 'the'), 1), (('the', 'visual'), 1), (('visual', 'assembly'), 1), (('assembly', ','), 1), ((',', 'execution'), 1), (('execution', 'and'), 1), (('and', 'analysis'), 1), (('of', 'modular'), 1), (('modular', 'natural'), 1), (('The', 'visual'), 1), (('visual', 'model'), 1), (('model', 'is'), 1), (('an', 'executable'), 1), (('executable', 'data'), 1), (('data', 'flow'), 1), (('flow', 'program'), 1), (('program', 'graph'), 1), (('graph', ','), 1), ((',', 'automatically'), 1), (('automatically', 'synthesised'), 1), (('synthesised', 'from'), 1), (('from', 'data'), 1), (('data', 'dependency'), 1), (('dependency', 'declaration'), 1), (('declaration', 'of'), 1), (('processing', 'modules'), 1), (('modules', '.'), 1), (('The', 'graph'), 1), (('graph', 'In'), 1), (('this', 'Chapter'), 1), (('Chapter', 'the'), 1), (('the', 'basic'), 3), (('basic', 'us'), 1), (('us', 'of'), 1), (('of', 'Description'), 2), (('Logics', 'for'), 1), (('for', 'Natural'), 1), (('Processing', 'will'), 1), (('will', 'be'), 4), (('be', 'analysed'), 1), (('analysed', ','), 1), ((',', 'together'), 1), (('together', 'with'), 1), (('a', 'little'), 1), (('little', 'bit'), 1), (('bit', 'of'), 1), (('of', 'history'), 2), (('history', ','), 1), (('Logics', 'in'), 1), (('state', 'of'), 3), (('the', 'art'), 3), (('art', 'in'), 1), (('linguistics', 'will'), 1), (('be', 'pointed'), 1), (('pointed', 'out'), 1), (('out', '.'), 1), (('.', '18.1'), 1), (('18.1', 'Introduction'), 1), (('Introduction', 'Since'), 1), (('Since', 'the'), 2), (('the', 'early'), 1), (('early', 'day'), 1), (('day', 'We'), 1), (('We', 'applied'), 1), (('applied', 'a'), 1), (('a', 'structure'), 1), (('structure', 'learning'), 1), (('learning', 'model'), 1), (('model', ','), 1), ((',', 'Max-Margin'), 1), (('Max-Margin', 'Structure'), 1), (('Structure', '('), 1), (('(', 'MMS'), 1), (('MMS', ')'), 1), ((')', 'tasks'), 3), ((',', 'where'), 1), (('where', 'the'), 1), (('the', 'aim'), 1), (('aim', 'is'), 1), (('capture', 'the'), 2), (('the', 'latent'), 1), (('latent', 'relationship'), 1), (('relationship', 'within'), 1), (('within', 'the'), 1), (('the', 'output'), 1), (('output', 'language'), 1), (('language', 'domain'), 1), (('domain', '.'), 1), (('We', 'formulate'), 1), (('formulate', 'this'), 1), (('this', 'model'), 1), (('model', 'a'), 1), (('a', 'an'), 2), (('an', 'extension'), 1), (('extension', 'of'), 1), (('of', 'multi–class'), 1), (('multi–class', 'Support'), 1), (('Support', 'Vector'), 1), (('Vector', 'Machine'), 1), (('Machine', '('), 1), (('(', 'SVM'), 1), (('SVM', ')'), 1), (('and', 'present'), 1), (('a', '-mation'), 1), (('-mation', 'Infrastructure'), 1), (('Infrastructure', ','), 1), ((',', 'digital'), 2), (('digital', 'libraries'), 1), (('libraries', ','), 1), ((',', 'networked'), 1), (('networked', 'services'), 1), (('services', ','), 1), (('digital', 'convergence'), 1), (('convergence', 'or'), 1), (('or', 'intelligent'), 1), (('intelligent', 'agents'), 1), (('agents', '.'), 1), (('This', 'attention'), 1), (('attention', 'is'), 1), (('is', 'moving'), 1), (('moving', 'natural'), 1), (('processing', 'along'), 1), (('along', 'the'), 1), (('the', 'critical'), 1), (('critical', 'path'), 1), (('path', 'for'), 1), (('for', 'all'), 1), (('all', 'kind'), 1), (('of', 'novel'), 1), (('novel', 'applications'), 1), (('applications', '.'), 4), (('article', 'will'), 1), (('will', 'mention'), 1), (('mention', 'a'), 1), (('of', 'successful'), 1), (('successful', 'application'), 1), (('NLP', 'Over'), 1), (('Over', 'the'), 1), (('last', 'few'), 1), (('few', 'years'), 1), ((',', 'a'), 5), (('of', 'area'), 1), (('area', 'of'), 1), (('processing', 'have'), 1), (('have', 'begun'), 1), (('begun', 'applying'), 1), (('applying', 'graph-based'), 1), (('graph-based', 'techniques'), 1), (('.', 'These'), 1), (('These', 'include'), 1), (('include', ','), 1), ((',', 'among'), 1), (('among', 'others'), 1), (('others', ','), 1), (('text', 'summarization'), 1), (('summarization', ','), 1), (('syntactic', 'parsing'), 2), (('parsing', ','), 2), ((',', 'word'), 2), (('sense', 'disambiguation'), 2), (('disambiguation', ','), 1), ((',', 'ontology'), 1), (('ontology', 'construction'), 1), (('construction', ','), 1), ((',', 'sentiment'), 1), (('sentiment', 'and'), 1), (('and', 'subjectivity'), 1), (('subjectivity', 'analysis'), 1), (('text', 'clustering'), 1), (('clustering', 'In'), 1), (('In', 'Natural'), 1), ((',', 'research'), 1), (('research', 'result'), 3), (('result', 'from'), 1), (('from', 'software'), 1), (('software', 'engineering'), 1), (('engineering', 'and'), 1), (('and', 'software'), 2), (('software', 'technology'), 1), (('technology', 'have'), 1), (('have', 'often'), 1), (('often', 'been'), 1), (('been', 'neglected'), 1), (('neglected', '.'), 1), (('.', 'of'), 1), (('of', 'kernelized'), 2), (('kernelized', 'sorting'), 1), (('sorting', 'to'), 1), (('to', 'increase'), 1), (('increase', 'it'), 1), (('it', 'robustness'), 1), (('robustness', 'and'), 1), (('and', 'performance'), 1), (('performance', 'on'), 1), (('on', 'several'), 1), (('several', 'Natural'), 1), (('tasks', ':'), 1), ((':', 'document'), 1), (('document', 'matching'), 1), (('matching', 'from'), 1), (('from', 'parallel'), 1), (('parallel', 'and'), 1), (('and', 'comparable'), 1), (('comparable', 'corpora'), 1), (('corpora', ','), 1), (('machine', 'transliteration'), 1), (('transliteration', 'and'), 1), (('and', 'even'), 1), (('even', 'image'), 1), (('image', 'processing'), 1), (('.', 'Empirically'), 1), (('Empirically', 'we'), 1), (('we', 'show'), 1), (('show', 'that'), 1), (('that', ','), 2), ((',', 'on'), 1), (('on', 'these'), 1), (('these', 'tasks'), 1), (('a', 'semi-supervised'), 1), (('semi-supervised', 'variant'), 1), (('variant', 'of'), 1), (('kernelized', 'will'), 1), (('be', 'structured'), 1), (('structured', '.'), 1), (('word', 'of'), 1), (('of', 'statistical'), 1), (('statistical', 'natural'), 1), (('processing', ','), 4), (('we', 'need'), 1), (('need', 'a'), 1), (('a', 'sophisticated'), 1), (('sophisticated', 'statistical'), 1), (('statistical', 'model'), 1), (('basic', 'elements'), 1), (('elements', ','), 1), (('a', 'word'), 2), (('word', 'or'), 1), (('or', 'phrases'), 1), (('phrases', ','), 1), (('be', 'combined'), 1), (('combined', 'with'), 1), (('the', 'structural'), 1), (('structural', 'modeling'), 1), (('modeling', 'such'), 1), (('a', 'syntactic'), 1), (('parsing', 'or'), 1), (('or', 'dependency'), 1), (('dependency', 'analysis'), 1), (('analysis', '.'), 2), (('.', 'Since'), 1), (('basic', 'property'), 1), (('property', 'of'), 1), (('of', 'these'), 1), (('these', 'element'), 1), (('element', 'In'), 1), (('a', 'framework'), 1), (('framework', 'for'), 1), (('for', 'developing'), 1), (('developing', 'probabilistic'), 1), (('probabilistic', 'classifier'), 1), (('classifier', 'in'), 1), (('Our', 'focus'), 1), (('focus', 'is'), 1), (('is', 'on'), 1), (('on', 'formulating'), 1), (('formulating', 'model'), 1), (('model', 'that'), 1), (('that', 'capture'), 1), (('the', 'most'), 1), (('most', 'important'), 1), (('important', 'interdependency'), 1), (('interdependency', 'among'), 1), (('among', 'features'), 1), (('features', ','), 1), (('avoid', 'overfitting'), 1), (('overfitting', 'the'), 1), (('the', 'data'), 2), (('data', 'while'), 1), (('while', 'also'), 1), (('also', 'characterizing'), 1), (('characterizing', 'the'), 1), (('data', 'well'), 1), (('well', '.'), 1), (('The', 'class'), 1), (('class', 'Many'), 1), (('Many', 'Natural'), 1), (('technique', 'have'), 1), (('in', 'Information'), 1), (('Information', 'Retrieval'), 1), (('Retrieval', '.'), 1), (('The', 'result'), 1), (('result', 'are'), 1), (('not', 'encouraging'), 1), (('encouraging', '.'), 1), (('.', 'Simple'), 1), (('Simple', 'method'), 1), (('method', '('), 1), (('(', 'stopwording'), 1), (('stopwording', ','), 1), ((',', 'porter-style'), 1), (('porter-style', 'stemming'), 1), (('stemming', ','), 1), ((',', 'etc'), 1), (('etc', '.'), 1), (('.', ')'), 1), ((')', 'usually'), 1), (('usually', 'yield'), 1), (('yield', 'significant'), 1), (('significant', 'improvements'), 1), (('improvements', ','), 1), ((',', 'while'), 1), (('while', 'higher-level'), 1), (('higher-level', 'processing'), 1), (('(', 'chunking'), 1), ((',', 'parsing'), 1), (('disambiguation', 'Abstract-'), 1), (('Abstract-', 'This'), 1), (('paper', 'explains'), 1), (('explains', 'the'), 1), (('the', 'information'), 4), (('retrieval', 'using'), 1), (('for', 'Malayalam'), 1), (('Malayalam', 'language'), 1), (('in', 'these'), 1), (('these', 'basic'), 1), (('basic', 'in'), 1), (('the', 'state'), 3), (('art', 'plan'), 2), (('plan', 'recognition'), 4), (('recognition', 'systems'), 2), (('paper', 'will'), 2), (('will', 'outline'), 2), (('outline', 'the'), 2), (('the', 'relation'), 2), (('relation', 'between'), 2), (('between', 'natural'), 2), (('and', 'plan'), 2), (('recognition', '('), 2), (('(', 'PR'), 2), (('PR', ')'), 2), ((',', 'argue'), 2), (('that', 'each'), 2), (('each', 'of'), 2), (('of', 'them'), 2), (('them', 'can'), 2), (('can', 'effectively'), 2), (('effectively', 'inform'), 2), (('inform', 'the'), 2), (('the', 'other'), 2), (('other', ','), 2), (('then', 'focus'), 2), (('on', 'key'), 2), (('key', 'recent'), 2), (('result', 'in'), 2), (('in', 'NLP'), 2), (('NLP', 'and'), 2), (('and', 'argue'), 2), (('argue', 'for'), 2), (('for', 'their'), 2), (('their', 'applicability'), 2), (('applicability', 'to'), 2), (('to', 'PR'), 2), (('PR', '.'), 2), (('1', 'in'), 1), (('1', 'Information'), 1), (('Information', 'retrieval'), 1), (('retrieval', 'is'), 1), (('of', 'finding'), 1), (('finding', 'the'), 1), (('the', 'document'), 1), (('document', 'in'), 1), (('a', 'document'), 1), (('document', 'collection'), 1), (('collection', 'that'), 1), (('that', 'satisfies'), 1), (('satisfies', 'the'), 1), (('information', 'need'), 1), (('need', 'of'), 1), (('the', 'user'), 1), (('user', '.'), 1), (('The', 'document'), 2), (('document', 'are'), 1), (('are', 'natural'), 1), (('language', 'constructs'), 1), (('constructs', ','), 1), (('the', 'motivation'), 1), (('motivation', 'of'), 1), (('this', 'work'), 1), (('work', 'is'), 1), (('to', 'investigate'), 1), (('investigate', 'how'), 1), (('how', 'natural'), 1), (('processing', 'can'), 1), (('be', 'used'), 3), (('improve', 'of'), 1), (('of', 'logic'), 1), (('logic', 'programming'), 4), (('programming', 'within'), 1), (('within', 'both'), 1), (('both', 'natural'), 1), (('language', 'research'), 1), (('we', 'point'), 1), (('out', 'opportunity'), 1), (('opportunity', 'for'), 1), (('for', 'induction'), 1), (('induction', 'of'), 1), (('knowledge', 'within'), 1), (('within', 'logic'), 1), (('logic', '('), 1), (('(', 'programming'), 1), (('programming', ')'), 1), (('.', 'Keywords'), 1), (('Keywords', ':'), 1), ((':', 'inductive'), 1), (('inductive', 'logic'), 1), (('programming', ','), 2), ((',', 'natural'), 1), ((',', 'logic'), 1), (('1', 'Introduction'), 1), (('Introduction', 'There'), 1), (('There', 'is'), 1), (('a', 'What'), 1), (('What', 'is'), 1), (('a', 'statistical'), 2), (('statistical', 'method'), 1), (('method', 'and'), 1), (('and', 'how'), 1), (('how', 'can'), 1), (('can', 'it'), 1), (('it', 'be'), 1), ((')', '?'), 1), (('?', 'In'), 1), (('we', 'start'), 1), (('start', 'from'), 1), (('a', 'definition'), 1), (('definition', 'of'), 1), (('NLP', 'a'), 1), (('a', 'concerned'), 1), (('design', 'and'), 2), (('and', 'implementation'), 1), (('implementation', 'of'), 1), (('of', 'effective'), 1), (('effective', 'natural'), 1), (('language', 'input'), 1), (('input', 'and'), 1), (('and', 'output'), 1), (('output', 'component'), 1), (('component', 'for'), 1), (('for', 'computational'), 1), (('computational', 'systems'), 1), (('We', 'distinguish'), 1), (('distinguish', 'three'), 1), (('three', 'In'), 1), (('this', 'report'), 1), (('report', ','), 1), ((',', 'some'), 2), (('some', 'collaborative'), 1), (('collaborative', 'work'), 1), (('work', 'between'), 1), (('between', 'the'), 1), (('of', 'Machine'), 1), (('Machine', 'Learning'), 1), (('Learning', '('), 1), (('is', 'presented'), 1), (('presented', '.'), 1), (('document', 'is'), 1), (('is', 'structured'), 1), (('structured', 'in'), 1), (('in', 'two'), 2), (('two', 'parts'), 1), (('parts', '.'), 1), (('The', 'first'), 1), (('first', 'part'), 1), (('part', 'includes'), 1), (('includes', 'a'), 2), (('a', 'superficial'), 1), (('superficial', 'but'), 1), (('but', 'comprehensive'), 1), (('comprehensive', 'survey'), 1), (('survey', 'covering'), 1), (('covering', 'the'), 1), (('state', '--'), 1), (('--', 'of'), 1), (('of', '--'), 1), (('--', 'the'), 1), (('the', '--'), 1), (('--', 'art'), 1), (('art', 'of'), 1), (('of', 'machine'), 2), (('learning', 'Abstract'), 1), (('Abstract', '.'), 1), (('This', 'thesis'), 1), (('thesis', 'examines'), 1), (('examines', 'the'), 3), (('technique', 'in'), 1), (('in', 'various'), 2), (('various', 'task'), 1), (('task', 'of'), 2), ((',', 'mainly'), 1), (('mainly', 'for'), 1), (('the', 'task'), 1), (('of', 'information'), 3), (('information', 'extraction'), 2), (('extraction', 'from'), 1), (('from', 'texts'), 1), (('texts', '.'), 1), (('The', 'objective'), 1), (('objective', 'are'), 1), (('the', 'improvement'), 2), (('improvement', 'of'), 2), (('of', 'adaptability'), 1), (('adaptability', 'of'), 1), (('extraction', 'system'), 1), (('system', 'to'), 2), (('to', 'new'), 1), (('new', 'thematic'), 1), (('thematic', 'do-mains'), 1), (('do-mains', '('), 1), (('(', 'or'), 1), (('or', 'even'), 1), (('even', 'This'), 1), (('chapter', 'examines'), 2), (('to', 'computerassisted'), 2), (('computerassisted', 'language'), 2), (('language', 'learning'), 2), (('learning', 'including'), 2), (('including', 'the'), 2), (('the', 'history'), 2), (('history', 'of'), 2), (('of', 'work'), 2), (('work', 'in'), 2), (('in', 'this'), 2), (('this', 'field'), 2), (('field', 'over'), 2), (('last', 'thirtyfive'), 2), (('thirtyfive', 'year'), 2), (('year', 'but'), 2), (('but', 'with'), 2), (('on', 'current'), 2), (('current', 'development'), 2), (('development', 'and'), 3), (('and', 'opportunities'), 2), (('opportunities', '.'), 2), (('.', '36.1'), 1), (('36.1', 'Traditional'), 1), (('Traditional', 'approach'), 1), (('approach', 'tointerpretation'), 1), (('tointerpretation', 'in'), 1), (('processing', 'typically'), 1), (('typically', 'fall'), 1), (('fall', 'into'), 1), (('into', 'one'), 1), (('one', 'of'), 1), (('of', 'three'), 1), (('three', 'classes'), 1), (('classes', ':'), 1), ((':', 'syntax-driven'), 1), (('syntax-driven', ','), 1), ((',', 'semantics-driven'), 1), (('semantics-driven', ','), 1), (('or', 'frame/task'), 1), (('frame/task', 'based'), 1), (('based', '.'), 1), (('.', 'Syntax-driven'), 1), (('Syntax-driven', 'approach'), 1), (('approach', 'use'), 1), (('use', 'a'), 1), (('a', 'domain-independent'), 1), (('domain-independent', 'grammar'), 1), (('grammar', 'to'), 1), (('the', 'interpretation'), 1), (('interpretation', 'process'), 1), (('process', 'and'), 1), (('and', 'produce'), 1), (('produce', 'a'), 1), (('a', 'global'), 2), (('global', 'parse'), 1), (('parse', 'Natural'), 1), (('a', 'very'), 1), (('very', 'large'), 1), (('large', 'and'), 1), (('and', 'diverse'), 1), (('diverse', 'subtopic'), 1), (('subtopic', 'of'), 1), (('of', 'artificial'), 2), (('artificial', 'intelligence'), 2), (('intelligence', '.'), 1), (('.', 'As'), 1), (('As', 'a'), 1), (('a', 'result'), 1), (('result', ','), 1), ((',', 'NLP'), 1), (('NLP', 'itself'), 1), (('itself', 'ha'), 1), (('ha', 'many'), 1), (('many', 'subtopics'), 1), (('subtopics', 'including'), 1), (('including', 'optical'), 1), (('optical', 'character'), 1), (('character', 'recognition'), 1), (('text', 'to'), 1), (('to', 'speech'), 1), (('speech', 'translators'), 1), (('translators', ','), 1), ((',', 'foreign'), 1), (('foreign', 'language'), 1), (('language', 'reading'), 1), (('reading', 'and'), 1), (('and', 'writing'), 1), (('writing', 'aids'), 1), (('aids', ','), 1), (('and', 'speech'), 1), (('recognition', 'Probabilistic'), 1), (('Probabilistic', 'finite-state'), 1), (('finite-state', 'string'), 1), (('string', 'transducer'), 1), (('transducer', '('), 1), (('(', 'FSTs'), 1), (('FSTs', ')'), 1), ((')', 'are'), 2), (('are', 'extremely'), 1), (('extremely', 'popular'), 1), (('popular', 'in'), 1), ((',', 'due'), 1), (('due', 'to'), 1), (('to', 'powerful'), 1), (('powerful', 'generic'), 1), (('generic', 'method'), 1), (('for', 'applying'), 1), (('applying', ','), 1), ((',', 'composing'), 1), (('composing', ','), 1), (('learning', 'them'), 1), (('them', '.'), 1), (('.', 'Unfortunately'), 1), (('Unfortunately', ','), 1), ((',', 'FSTs'), 1), (('FSTs', 'are'), 1), (('not', 'a'), 1), (('a', 'good'), 1), (('good', 'fit'), 1), (('fit', 'for'), 1), (('for', 'much'), 1), (('much', 'of'), 1), (('current', 'work'), 1), (('work', 'on'), 1), (('on', 'probabilistic'), 1), (('probabilistic', 'modeling'), 1), (('modeling', 'for'), 1), (('for', 'machine'), 1), (('machine', 'ABSTRACT'), 1), (('ABSTRACT', '.'), 1), (('this', 'special'), 1), (('special', 'issue'), 1), (('of', 'TAL'), 1), (('TAL', ','), 1), (('we', 'look'), 1), (('look', 'at'), 1), (('the', 'fundamental'), 1), (('fundamental', 'principle'), 1), (('principle', 'underlying'), 1), (('underlying', 'evaluation'), 1), (('We', 'adopt'), 1), (('adopt', 'a'), 1), (('global', 'point'), 1), (('point', 'of'), 1), (('of', 'view'), 1), (('view', 'that'), 1), (('that', 'go'), 1), (('go', 'beyond'), 1), (('beyond', 'the'), 1), (('the', 'horizon'), 1), (('horizon', 'of'), 1), (('a', 'single'), 3), (('single', 'evaluation'), 1), (('evaluation', 'campaign'), 1), (('campaign', 'or'), 1), (('a', 'particular'), 1), (('particular', 'protocol'), 1), (('protocol', '.'), 1), (('.', 'After'), 1), (('After', 'a'), 1), (('brief', 'review'), 1), (('review', 'of'), 1), (('history', 'and'), 1), (('and', 'terminology'), 1), (('terminology', 'Abstract'), 1), (('found', 'Natural'), 1), (('system', '('), 1), (('that', 'extract'), 1), (('extract', 'clinical'), 1), (('clinical', 'information'), 1), (('from', 'textual'), 1), (('textual', 'report'), 1), (('report', 'were'), 1), (('were', 'shown'), 1), (('shown', 'to'), 1), (('be', 'effective'), 1), (('effective', 'for'), 1), (('for', 'limited'), 1), (('limited', 'domain'), 1), (('domain', 'and'), 2), (('for', 'particular'), 1), (('particular', 'applications'), 1), (('.', 'Because'), 1), (('Because', 'an'), 1), (('an', 'NLP'), 1), (('NLP', 'system'), 1), (('system', 'typically'), 1), (('typically', 'requires'), 1), (('requires', 'substantial'), 1), (('substantial', 'resource'), 1), (('resource', 'to'), 1), (('to', 'develop'), 1), (('develop', ','), 1), (('it', 'is'), 2), (('is', 'beneficial'), 1), (('beneficial', 'if'), 1), (('if', 'it'), 1), (('is', 'designed'), 1), (('designed', 'to'), 1), (('be', 'easily'), 1), (('easily', 'fact'), 1), (('fact', 'form'), 1), (('a', 'link'), 1), (('link', 'between'), 1), (('between', 'IE'), 1), (('IE', ','), 1), (('a', 'recent'), 1), (('Processing', ','), 2), (('and', 'logic'), 1), (('programming', 'with'), 1), (('with', 'Prolog'), 1), (('Prolog', '.'), 1), (('1', 'We'), 1), (('single', 'convolutional'), 1), (('convolutional', 'neural'), 1), (('architecture', 'that'), 1), ((',', 'given'), 1), (('given', 'a'), 1), (('a', 'sentence'), 1), (('sentence', ','), 1), ((',', 'output'), 1), (('output', 'a'), 1), (('a', 'host'), 1), (('host', 'of'), 1), (('processing', 'predictions'), 1), (('predictions', ':'), 1), ((':', 'part-of-speech'), 1), (('part-of-speech', 'tags'), 1), (('tags', ','), 2), ((',', 'chunks'), 1), (('chunks', ','), 1), (('entity', 'tags'), 1), (('semantic', 'roles'), 1), (('roles', ','), 1), ((',', 'semantically'), 1), (('semantically', 'similar'), 1), (('similar', 'word'), 1), (('the', 'likelihood'), 1), (('likelihood', 'that'), 1), (('that', 'the'), 1), (('the', 'sentence'), 1), (('sentence', 'make'), 1), (('make', 'sense'), 1), (('sense', '('), 1), (('(', 'grammatically'), 1), (('grammatically', 'We'), 1), (('We', 'developed'), 1), (('a', 'prototype'), 1), (('prototype', 'information'), 1), (('system', 'which'), 2), (('which', 'us'), 1), (('us', 'advanced'), 1), (('advanced', 'natural'), 1), (('to', 'enhance'), 1), (('enhance', 'the'), 1), (('the', 'effectiveness'), 1), (('effectiveness', 'of'), 1), (('of', 'traditional'), 1), (('traditional', 'key-word'), 1), (('key-word', 'based'), 1), (('based', 'document'), 1), (('document', 'retrieval'), 1), (('retrieval', '.'), 1), (('The', 'backbone'), 1), (('backbone', 'of'), 1), (('of', 'our'), 2), (('our', 'system'), 1), (('system', 'is'), 1), (('statistical', 'retrieval'), 1), (('retrieval', 'engine'), 1), (('engine', 'which'), 1), (('performs', 'automated'), 1), (('automated', 'indexing'), 1), (('indexing', 'Abstract'), 1), (('will', 'discus'), 1), (('several', 'issue'), 1), (('issue', 'and'), 1), (('and', 'requirement'), 1), (('requirement', 'for'), 1), (('for', 'enabling'), 1), (('enabling', 'natural'), 1), (('to', 'become'), 1), (('become', 'context-adaptive'), 1), (('context-adaptive', '.'), 1), (('.', 'Given'), 1), (('Given', 'the'), 1), (('the', 'fact'), 1), (('fact', 'that'), 1), (('that', 'emerging'), 1), (('emerging', 'system'), 1), (('system', 'feature'), 1), (('feature', 'speaker'), 1), (('speaker', 'independent'), 1), (('independent', 'continuous'), 1), (('continuous', 'speech'), 1), (('recognition', 'restricted'), 1), (('restricted', 'to'), 1), (('to', 'individual'), 1), (('individual', 'domain'), 1), (('and', 'are'), 2), (('are', 'equipped'), 1), (('equipped', 'with'), 1), (('with', 'syntactic'), 1), (('syntactic', 'In'), 1), (('In', 'Fall'), 1), (('Fall', '2004'), 1), (('2004', 'I'), 1), (('I', 'introduced'), 1), (('introduced', 'a'), 1), (('new', 'course'), 1), (('course', 'called'), 1), (('called', 'Applied'), 1), (('Applied', 'Natural'), 1), (('in', 'which'), 1), (('which', 'student'), 1), (('student', 'acquire'), 1), (('acquire', 'an'), 1), (('an', 'understanding'), 1), (('of', 'which'), 1), (('which', 'text'), 1), (('analysis', 'technique'), 1), (('technique', 'are'), 1), (('are', 'currently'), 1), (('currently', 'feasible'), 1), (('feasible', 'for'), 1), (('for', 'practical'), 1), (('practical', 'applications'), 1), ((':', 'Natural'), 1), (('the', 'study'), 1), (('of', 'mathematical'), 1), (('mathematical', 'and'), 1), (('and', 'computational'), 1), (('computational', 'modelling'), 1), (('modelling', 'of'), 1), (('of', 'various'), 1), (('various', 'aspect'), 1), (('language', 'and'), 2), (('a', 'wide'), 3), (('wide', 'range'), 2), (('of', 'systems'), 1), (('language', 'is'), 1), (('is', 'any'), 1), (('any', 'language'), 1), (('that', 'arises'), 1), (('arises', 'a'), 1), (('an', 'innate'), 1), (('innate', 'facility'), 1), (('facility', 'for'), 1), (('for', 'language'), 1), (('language', 'possessed'), 1), (('possessed', 'by'), 1), (('the', 'human'), 1), (('human', 'intellect'), 1), (('intellect', ';'), 1), ((';', 'it'), 1), (('it', 'may'), 1), (('may', 'Natural'), 1), ((',', 'which'), 1), (('which', 'is'), 1), (('a', 'branch'), 1), (('branch', 'of'), 1), (('intelligence', ','), 1), ((',', 'includes'), 1), (('includes', 'speech'), 1), (('speech', 'synthesis'), 1), (('synthesis', ','), 1), ((',', 'Speech'), 1), (('Speech', 'recognition'), 1), (('and', 'Machine'), 1), (('Machine', 'translation'), 1), (('translation', '.'), 1), (('Processing', 'ha'), 1), (('ha', 'a'), 1), (('of', 'application'), 2), (('application', 'in'), 1), (('the', 'Indian'), 1), (('Indian', 'context'), 1), (('context', '.'), 1), (('.', 'Most'), 1), (('Most', 'of'), 1), (('the', 'rural'), 1), (('rural', 'Indian'), 1), (('Indian', 'community'), 1), (('community', 'is'), 1), (('is', 'unable'), 1), (('unable', 'to'), 1), (('to', 'make'), 1), (('use', 'An'), 1), (('An', 'Evaluation'), 1), (('Evaluation', 'of'), 1), (('of', 'LOLITA'), 1), (('LOLITA', 'and'), 1), (('and', 'related'), 1), (('related', 'Natural'), 1), (('Processing', 'Systems'), 1), (('Systems', 'Paul'), 1), (('Paul', 'Callaghan'), 1), (('Callaghan', 'Submitted'), 1), (('Submitted', 'to'), 1), (('the', 'University'), 1), (('University', 'of'), 1), (('of', 'Durham'), 1), (('Durham', 'for'), 1), (('the', 'degree'), 1), (('degree', 'of'), 1), (('of', 'Ph.D.'), 1), (('Ph.D.', ','), 1), ((',', 'August'), 1), (('August', '1997'), 1), (('1997', '--'), 1), (('--', '--'), 9), (('--', '-'), 1), (('-', 'This'), 1), (('This', 'research'), 1), (('research', 'address'), 1), (('the', 'question'), 1), (('question', ','), 1), ((',', '``'), 1), (('``', 'how'), 1), (('how', 'do'), 1), (('do', 'we'), 1), (('we', 'evaluate'), 1), (('evaluate', 'system'), 1), (('system', 'like'), 1), (('like', 'LOLITA'), 1), (('LOLITA', '?'), 1), (('?', \"''\"), 1), ((\"''\", 'LOLITA'), 1), (('LOLITA', 'is'), 1), (('the', 'Natural'), 1), (('Natural', 'Previous'), 1), (('Previous', 'work'), 1), (('work', 'demonstrated'), 1), (('demonstrated', 'that'), 1), (('that', 'Web'), 1), (('Web', 'count'), 1), (('count', 'can'), 1), (('to', 'approximate'), 1), (('approximate', 'bigram'), 1), (('bigram', 'counts'), 1), (('counts', ','), 1), ((',', 'suggesting'), 1), (('suggesting', 'that'), 1), (('that', 'Web-based'), 1), (('Web-based', 'frequency'), 1), (('frequency', 'should'), 1), (('should', 'be'), 1), (('be', 'useful'), 1), (('useful', 'for'), 1), (('wide', 'variety'), 1), (('variety', 'of'), 2), (('.', 'However'), 1), (('However', ','), 1), ((',', 'only'), 1), (('only', 'a'), 1), (('a', 'limited'), 1), (('limited', 'number'), 1), (('task', 'have'), 1), (('have', 'so'), 1), (('far', 'been'), 1), (('been', 'tested'), 1), (('tested', 'using'), 1), (('using', 'Web-scale'), 1), (('Web-scale', 'data'), 1), (('data', 'set'), 1), (('set', 'This'), 1), (('.', '16.1'), 1), (('16.1', 'Introduction'), 1), (('Introduction', 'This'), 1), (('chapter', 'focus'), 1), (('on', 'application'), 1), (('application', 'This'), 1), (('paper', 'describes'), 1), (('describes', 'a'), 1), (('language', 'system'), 1), (('which', 'improves'), 1), (('improves', 'it'), 1), (('it', 'own'), 1), (('own', 'performance'), 1), (('performance', 'through'), 1), (('through', 'learning'), 1), (('The', 'system'), 1), (('system', 'process'), 1), (('process', 'short'), 1), (('short', 'English'), 1), (('English', 'narrative'), 1), (('narrative', 'and'), 1), (('and', 'is'), 1), (('is', 'able'), 1), (('to', 'acquire'), 1), (('acquire', ','), 1), ((',', 'from'), 1), (('single', 'narrative'), 1), (('narrative', ','), 1), (('new', 'schema'), 1), (('schema', 'for'), 1), (('a', 'stereotypical'), 1), (('stereotypical', 'set'), 1), (('set', 'of'), 2), (('of', 'actions'), 1), (('actions', '.'), 1), (('During', 'the'), 1), (('the', 'understanding'), 1), (('understanding', 'process'), 1), (('process', ','), 1), (('the', 'system'), 1), (('system', 'attempt'), 1), (('attempt', 'We'), 1), (('We', 'classify'), 1), (('classify', 'and'), 1), (('and', 'review'), 1), (('review', 'current'), 1), (('current', 'approach'), 1), (('to', 'software'), 1), (('software', 'infrastructure'), 1), (('infrastructure', 'for'), 1), (('for', 'research'), 1), (('research', ','), 1), ((',', 'development'), 1), (('and', 'delivery'), 1), (('delivery', 'of'), 1), (('NLP', 'systems'), 1), (('The', 'task'), 1), (('task', 'Confidence'), 1), (('Confidence', 'measure'), 1), (('are', 'a'), 2), (('a', 'practical'), 1), (('practical', 'solution'), 1), (('solution', 'for'), 1), (('for', 'improving'), 1), (('improving', 'the'), 1), (('the', 'usefulness'), 1), (('usefulness', 'of'), 1), (('Processing', 'applications'), 1), (('.', 'Confidence'), 1), (('Confidence', 'estimation'), 1), (('estimation', 'is'), 1), (('a', 'generic'), 1), (('generic', 'machine'), 1), (('learning', 'approach'), 1), (('for', 'deriving'), 1), (('deriving', 'confidence'), 1), (('confidence', 'measures'), 1), (('measures', '.'), 1), (('We', 'give'), 1), (('give', 'an'), 1), (('overview', 'of'), 1), (('of', 'confidence'), 1), (('confidence', 'estimation'), 1), (('estimation', 'in'), 1), (('various', 'field'), 1), (('field', '!'), 1), (('!', 'lex-sign'), 3), (('lex-sign', 'sense-id'), 3), (('sense-id', ':'), 3), ((':', 'sense-id'), 3), (('sense-id', 'dictionary'), 1), (('dictionary', '?'), 1), (('?', '='), 3), (('=', '``'), 3), (('``', 'LDOCE'), 1), (('LDOCE', \"''\"), 1), ((\"''\", '!'), 2), (('sense-id', 'ldb-entry-no'), 1), (('ldb-entry-no', '?'), 1), (('``', '12364'), 1), (('12364', \"''\"), 1), (('sense-id', 'sense-no'), 1), (('sense-no', '?'), 1), (('``', '0'), 1), (('0', \"''\"), 1), ((\"''\", '.'), 1), (('.', 'When'), 1), (('When', 'loaded'), 1), (('loaded', 'into'), 1), (('into', 'the'), 2), (('the', 'LKB'), 2), (('LKB', ','), 1), ((',', '('), 1), (('(', '9'), 2), (('9', ')'), 2), ((')', 'will'), 1), (('be', 'expanded'), 1), (('expanded', 'into'), 1), (('a', 'fully-fledged'), 1), (('fully-fledged', 'representation'), 1), (('representation', 'for'), 1), (('the', 'transitive'), 1), (('transitive', 'use'), 1), (('of', 'experience'), 1), (('experience', ';'), 1), ((';', 'by'), 1), (('by', 'integrating'), 1), (('integrating', 'word-specific'), 1), (('word-specific', 'information'), 1), (('information', 'provided'), 1), (('provided', 'by'), 1), (('by', '('), 1), ((')', 'with'), 1), (('information', 'encoded'), 1), (('encoded', 'by'), 1), (('LKB', 'type'), 1), (('type', 'strict-trans-sign'), 1), (('strict-trans-sign', '.'), 1), (('.', 'Thus'), 1), (('Thus', ','), 1), ((',', 'although'), 1), (('although', 'neither'), 1), (('neither', 'LDOCE'), 1), (('LDOCE', ','), 1), ((',', 'LLCE'), 1), (('LLCE', 'or'), 1), (('or', 'the'), 1), (('the', 'earlier'), 1), (('earlier', 'subcategorised'), 1), (('subcategorised', 'lexicon'), 1), (('lexicon', 'contain'), 1), (('contain', 'all'), 1), (('about', 'psychological'), 1), (('psychological', 'verb'), 1), (('verb', 'defined'), 1), (('defined', 'in'), 1), (('in', 'Sanfilippo'), 1), (('Sanfilippo', '&'), 1), (('&', 'aposs'), 1), (('aposs', 'type'), 1), (('type', 'system'), 2), ((',', 'by'), 1), (('by', 'using'), 1), (('using', 'the'), 2), (('the', 'conjunction'), 1), (('conjunction', 'of'), 1), (('information', 'available'), 1), (('available', 'from'), 1), (('from', 'all'), 1), (('all', 'three'), 1), (('three', ','), 1), (('it', 'proved'), 1), (('proved', 'possible'), 1), (('possible', 'to'), 1), (('to', 'effectively'), 1), (('effectively', 'enrich'), 1), (('enrich', 'this'), 1), (('this', 'information'), 1), (('information', 'at'), 1), (('same', 'time'), 1), (('time', 'a'), 1), (('a', 'mapping'), 1), (('mapping', 'it'), 1), (('it', 'into'), 1), (('a', 'formal'), 1), (('formal', 'representation'), 1), (('representation', '.'), 1), (('.', '4.2.5'), 1), (('4.2.5', 'Towards'), 1), (('Towards', 'a'), 1), (('a', 'Multilingual'), 1), (('Multilingual', 'LKB'), 1), (('LKB', 'A'), 1), (('A', 'goal'), 1), (('goal', 'of'), 1), (('of', 'ACQUILEX'), 1), (('ACQUILEX', 'is'), 1), (('to', 'demonstrate'), 1), (('demonstrate', 'that'), 2), (('that', 'an'), 1), (('an', 'LKB'), 1), (('LKB', 'can'), 1), (('be', 'produced'), 1), (('produced', 'that'), 1), (('that', 'usefully'), 1), (('usefully', 'exploit'), 1), (('exploit', 'various'), 1), (('various', 'MRD'), 1), (('MRD', 'source'), 1), (('source', 'and'), 1), (('and', 'integrates'), 1), (('integrates', 'multilingual'), 1), (('multilingual', 'information'), 1), (('The', 'use'), 1), (('a', 'common'), 2), (('common', 'LRL'), 1), (('LRL', 'with'), 1), (('common', 'type'), 1), (('make', 'it'), 1), (('it', 'possi'), 1), (('possi', '...'), 1), (('...', 'We'), 1), (('the', 'Stanford'), 1), (('Stanford', 'CoreNLP'), 1), (('CoreNLP', 'toolkit'), 1), (('toolkit', ','), 1), ((',', 'an'), 1), (('an', 'extensible'), 1), (('extensible', 'pipeline'), 1), (('pipeline', 'that'), 1), (('that', 'provides'), 1), (('provides', 'core'), 1), (('core', 'natural'), 1), (('natural', 'lan-guage'), 1), (('lan-guage', 'analysis'), 1), (('This', 'toolkit'), 1), (('toolkit', 'is'), 1), (('is', 'quite'), 1), (('quite', 'widely'), 1), (('widely', 'used'), 1), (('used', ','), 1), ((',', 'both'), 1), (('both', 'in'), 1), (('research', 'NLP'), 1), (('NLP', 'community'), 1), (('community', 'and'), 1), (('and', 'also'), 1), (('also', 'among'), 1), (('among', 'commercial'), 1), (('commercial', 'and'), 1), (('and', 'govern-ment'), 1), (('govern-ment', 'user'), 1), (('user', 'of'), 1), (('of', 'open'), 1), (('open', 'source'), 1), (('source', 'NLP'), 1), (('NLP', 'technol-ogy'), 1), (('technol-ogy', '.'), 1), (('We', 'suggest'), 1), (('suggest', 'Gaussian'), 1), (('Gaussian', 'Processes'), 1), (('Processes', '('), 1), (('(', 'GPs'), 1), (('GPs', ')'), 1), (('a', 'powerful'), 1), (('powerful', 'mod-elling'), 1), (('mod-elling', 'framework'), 1), (('framework', 'incorporating'), 1), (('incorporating', 'kernel'), 1), (('kernel', 'and'), 1), (('and', 'Bayesian'), 1), (('Bayesian', 'inference'), 1), (('inference', ','), 1), (('are', 'recognised'), 1), (('recognised', 'a'), 1), (('a', 'state-of-the-art'), 1), (('state-of-the-art', 'for'), 1), (('for', 'many'), 1), (('many', 'machine'), 1), (('learning', 'tasks'), 1), (('.', ':'), 1), ((':', 'A'), 1), (('A', 'fundamental'), 1), (('fundamental', 'issue'), 1), (('issue', 'in'), 1), (('the', 'prerequisite'), 1), (('prerequisite', 'of'), 1), (('an', 'enormous'), 1), (('enormous', 'quantity'), 1), (('quantity', 'of'), 1), (('of', 'preprogrammed'), 1), (('preprogrammed', 'knowledge'), 1), (('knowledge', 'concerning'), 1), (('concerning', 'both'), 1), (('both', 'the'), 1), (('the', 'language'), 1), (('the', 'domain'), 1), (('domain', 'under'), 1), (('under', 'examination'), 1), (('examination', '.'), 1), (('.', 'Manual'), 1), (('Manual', 'acquisition'), 1), (('acquisition', 'of'), 1), (('this', 'knowledge'), 1), (('knowledge', 'is'), 1), (('is', 'tedious'), 1), (('tedious', 'and'), 1), (('and', 'error'), 1), (('error', 'prone'), 1), (('prone', '.'), 1), (('.', 'Development'), 1), (('Development', 'of'), 1), (('an', 'automated'), 1), (('automated', 'acquisition'), 1), (('acquisition', '``'), 1), (('``', 'that'), 1), (('that', 'support'), 1), (('support', 'sophisticated'), 1), (('sophisticated', 'natural'), 1), (('processing', 'while'), 1), (('while', 'significantly'), 1), (('significantly', 'simplifying'), 1), (('simplifying', 'the'), 1), (('the', 'interface'), 1), (('interface', 'between'), 1), (('between', 'domain-specific'), 1), (('domain-specific', 'knowledge'), 1), (('knowledge', 'and'), 1), (('and', 'general'), 1), (('general', 'linguis-'), 1), (('linguis-', 'tic'), 1), (('tic', 'resources'), 1), (('paper', 'present'), 2), (('present', 'the'), 1), (('the', 'result'), 1), (('result', 'of'), 1), (('our', 'experience'), 2), (('experience', 'in'), 1), (('in', 'designing'), 1), (('designing', 'and'), 1), (('and', 'using'), 1), (('the', 'upper'), 1), (('upper', 'model'), 1), (('a', 'variety'), 1), (('application', 'over'), 1), (('past', '5'), 1), (('5', 'year'), 1), (('year', 'into'), 1), (('same', 'or'), 1), (('or', 'neighboring'), 1), (('neighboring', 'map'), 1), (('map', 'nodes'), 1), (('nodes', '.'), 1), (('.', 'Nodes'), 1), (('Nodes', 'may'), 1), (('may', 'thus'), 1), (('thus', 'be'), 1), (('be', 'viewed'), 1), (('viewed', 'a'), 1), (('word', 'categories'), 1), (('categories', '.'), 1), (('.', 'Although'), 1), (('Although', 'no'), 1), (('no', 'a'), 1), (('a', 'priori'), 1), (('priori', 'information'), 1), (('about', 'class'), 1), (('class', 'is'), 1), (('is', 'given'), 1), (('given', ','), 1), ((',', 'during'), 1), (('the', 'self-organizing'), 1), (('self-organizing', 'process'), 1), (('process', 'a'), 1), (('a', 'model'), 1), (('word', 'class'), 1), (('class', 'emerges'), 1), (('emerges', '.'), 1), (('The', 'central'), 1), (('central', 'topic'), 1), (('topic', 'of'), 1), (('the', 'thesis'), 1), (('thesis', 'is'), 1), (('the', 'SOM'), 1), (('SOM', 'in'), 1), (('The', 'approach'), 1), (('a', 'workbench'), 1), (('workbench', 'built'), 1), (('built', 'by'), 1), (('by', 'Priberam'), 1), (('Priberam', 'Informática'), 1), (('Informática', 'for'), 1), (('the', 'company'), 1), (('company', '’'), 1), (('’', 's'), 1), (('s', 'natural'), 1), (('processing', 'technology'), 1), (('technology', '.'), 1), (('This', 'workbench'), 1), (('workbench', 'includes'), 1), (('a', 'set'), 1), (('linguistic', 'resource'), 1), (('resource', 'and'), 1), (('software', 'tool'), 1), (('tool', 'that'), 1), (('that', 'have'), 1), (('been', 'applied'), 1), (('applied', 'in'), 1), (('a', 'considerable'), 1), (('considerable', 'number'), 1), (('of', 'practical'), 1), (('practical', 'purposes'), 1), (('purposes', ','), 1), ((',', 'covering'), 1), (('covering', 'Abstract—Natural'), 1), (('Abstract—Natural', 'Language'), 1), (('an', 'effective'), 1), (('effective', 'approach'), 2), (('for', 'bringing'), 1), (('bringing', 'improvement'), 1), (('in', 'educational'), 1), (('educational', 'setting'), 1), (('setting', '.'), 1), (('.', 'Implementing'), 1), (('Implementing', 'NLP'), 1), (('NLP', 'involves'), 1), (('involves', 'initiating'), 1), (('initiating', 'the'), 1), (('of', 'learning'), 1), (('learning', 'through'), 1), (('through', 'the'), 1), (('natural', 'acquisition'), 1), (('acquisition', 'in'), 1), (('the', 'educational'), 1), (('educational', 'systems'), 1), (('It', 'is'), 1), (('is', 'based'), 1), (('on', 'effective'), 1), (('for', 'providing'), 1), (('a', 'solution'), 1), (('solution', 'ABSTRACT'), 1), ((':', 'After'), 1), (('After', 'twenty'), 1), (('twenty', 'year'), 1), (('year', 'of'), 1), (('of', 'disfavor'), 1), (('disfavor', ','), 1), (('a', 'technology'), 1), (('technology', 'ha'), 1), (('ha', 'returned'), 1), (('returned', 'which'), 1), (('which', 'imitates'), 1), (('imitates', 'the'), 1), (('the', 'brain'), 1), (('brain', '.'), 1), (('language', 'experiment'), 1), (('experiment', '('), 1), (('(', 'Sejnowski'), 1), (('Sejnowski', '&'), 1), (('&', 'Rosenberg'), 1), (('Rosenberg', ':'), 1), ((':', '1986'), 1), (('1986', ')'), 1), ((')', 'demonstrate'), 1), (('that', 'neural'), 1), (('network', 'computing'), 1), (('computing', 'architecture'), 1), (('architecture', 'can'), 1), (('can', 'learn'), 1), (('learn', 'from'), 1), (('from', 'actual'), 1), (('actual', 'spoken'), 1), (('spoken', 'language'), 1), (('language', ','), 1), ((',', 'observe'), 1), (('observe', 'rule'), 1), (('rule', 'of'), 1), (('of', 'pronunciation'), 1), (('pronunciation', 'Text'), 1), (('Text', 'statistic'), 1), (('statistic', 'are'), 1), (('are', 'frequently'), 1), (('frequently', 'used'), 1), (('in', 'stylometry'), 1), (('stylometry', 'and'), 1), (('and', 'cryptography'), 1), (('cryptography', 'studies'), 1), (('studies', '.'), 1), (('some', 'text'), 1), (('text', 'statistic'), 1), (('statistic', 'tool'), 1), (('tool', 'are'), 1), (('are', 'developed'), 1), (('developed', 'in'), 1), (('in', 'ISO'), 1), (('ISO', 'Prolog'), 1), (('Prolog', 'for'), 1), (('.', 'Details'), 1), (('Details', 'are'), 1), (('are', 'given'), 1), (('given', 'on'), 1), (('the', 'usage'), 1), (('usage', 'of'), 1), (('of', '21'), 1), (('21', 'user-callable'), 1), (('user-callable', 'predicates'), 1), (('predicates', '.'), 1), (('.', 'Logic'), 1), (('Logic', 'and'), 1), (('and', 'limitation'), 1), (('limitation', 'of'), 1), (('the', 'program'), 1), (('program', 'are'), 1), (('are', 'also'), 1), (('also', 'discussed'), 1), (('discussed', 'We'), 1), (('We', 'summarize'), 1), (('summarize', 'our'), 1), (('experience', 'using'), 1), (('using', 'FrameNet'), 1), (('FrameNet', 'in'), 2), (('two', 'rather'), 1), (('rather', 'different'), 1), (('different', 'project'), 1), (('project', 'in'), 1), (('We', 'conclude'), 1), (('conclude', 'that'), 1), (('that', 'NLP'), 1), (('NLP', 'can'), 1), (('can', 'benefit'), 1), (('benefit', 'from'), 1), (('from', 'FrameNet'), 1), (('different', 'ways'), 1), (('ways', ','), 1), (('but', 'we'), 1), (('we', 'sketch'), 1), (('sketch', 'some'), 1), (('some', 'problem'), 1), (('problem', 'that'), 1), (('be', 'overcome'), 1), (('overcome', '.'), 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCanVrO5k1c0",
        "outputId": "9d38a942-76d8-401d-fe27-c69cd88d7bee"
      },
      "source": [
        "#1.2\r\n",
        "for key, value in bigram_data_count.items():\r\n",
        "  print(\"probability of {0} is {1}\".format(key, value/Single_Word_Dict[key[0]]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "probability of ('Abstract', 'not') is 0.6153846153846154\n",
            "probability of ('not', 'found') is 0.5714285714285714\n",
            "probability of ('found', 'describe') is 0.125\n",
            "probability of ('describe', 'a') is 0.5555555555555556\n",
            "probability of ('a', 'method') is 0.017094017094017096\n",
            "probability of ('method', 'for') is 0.42857142857142855\n",
            "probability of ('for', 'statistical') is 0.017857142857142856\n",
            "probability of ('statistical', 'modeling') is 0.14285714285714285\n",
            "probability of ('modeling', 'based') is 0.3333333333333333\n",
            "probability of ('based', 'on') is 0.625\n",
            "probability of ('on', 'maximum') is 0.043478260869565216\n",
            "probability of ('maximum', 'entropy') is 1.0\n",
            "probability of ('entropy', '.') is 0.5\n",
            "probability of ('.', 'We') is 0.0958904109589041\n",
            "probability of ('We', 'present') is 0.08333333333333333\n",
            "probability of ('present', 'a') is 0.7142857142857143\n",
            "probability of ('a', 'maximum-likelihood') is 0.008547008547008548\n",
            "probability of ('maximum-likelihood', 'approach') is 1.0\n",
            "probability of ('approach', 'for') is 0.29411764705882354\n",
            "probability of ('for', 'automatically') is 0.017857142857142856\n",
            "probability of ('automatically', 'constructing') is 0.3333333333333333\n",
            "probability of ('constructing', 'maximum') is 1.0\n",
            "probability of ('entropy', 'model') is 0.5\n",
            "probability of ('model', 'and') is 0.1\n",
            "probability of ('and', 'describe') is 0.00847457627118644\n",
            "probability of ('describe', 'how') is 0.1111111111111111\n",
            "probability of ('how', 'to') is 0.2\n",
            "probability of ('to', 'implement') is 0.013513513513513514\n",
            "probability of ('implement', 'this') is 1.0\n",
            "probability of ('this', 'approach') is 0.045454545454545456\n",
            "probability of ('approach', 'efficiently') is 0.058823529411764705\n",
            "probability of ('efficiently', ',') is 1.0\n",
            "probability of (',', 'using') is 0.006578947368421052\n",
            "probability of ('using', 'a') is 0.09090909090909091\n",
            "probability of ('a', 'example') is 0.008547008547008548\n",
            "probability of ('example', 'several') is 1.0\n",
            "probability of ('several', 'problem') is 0.25\n",
            "probability of ('problem', 'in') is 0.25\n",
            "probability of ('in', 'natural') is 0.18666666666666668\n",
            "probability of ('natural', 'language') is 0.953125\n",
            "probability of ('language', 'processing') is 0.6263736263736264\n",
            "probability of ('processing', '.') is 0.171875\n",
            "probability of ('.', 'Scaling') is 0.00684931506849315\n",
            "probability of ('Scaling', 'conditional') is 1.0\n",
            "probability of ('conditional', 'random') is 1.0\n",
            "probability of ('random', 'field') is 1.0\n",
            "probability of ('field', 'for') is 0.125\n",
            "probability of ('for', 'natural') is 0.07142857142857142\n",
            "probability of ('processing', 'Terms') is 0.015625\n",
            "probability of ('Terms', 'and') is 1.0\n",
            "probability of ('and', 'Conditions') is 0.01694915254237288\n",
            "probability of ('Conditions', ':') is 1.0\n",
            "probability of (':', 'Terms') is 0.05263157894736842\n",
            "probability of (':', 'Copyright') is 0.05263157894736842\n",
            "probability of ('Copyright', 'in') is 1.0\n",
            "probability of ('in', 'work') is 0.013333333333333334\n",
            "probability of ('work', 'deposited') is 0.125\n",
            "probability of ('deposited', 'in') is 1.0\n",
            "probability of ('in', 'Minerva') is 0.013333333333333334\n",
            "probability of ('Minerva', 'Access') is 1.0\n",
            "probability of ('Access', 'is') is 1.0\n",
            "probability of ('is', 'retained') is 0.018518518518518517\n",
            "probability of ('retained', 'by') is 1.0\n",
            "probability of ('by', 'the') is 0.16666666666666666\n",
            "probability of ('the', 'The') is 0.005434782608695652\n",
            "probability of ('The', 'paper') is 0.07692307692307693\n",
            "probability of ('paper', 'address') is 0.047619047619047616\n",
            "probability of ('address', 'the') is 1.0\n",
            "probability of ('the', 'issue') is 0.005434782608695652\n",
            "probability of ('issue', 'of') is 0.4\n",
            "probability of ('of', 'cooperation') is 0.005681818181818182\n",
            "probability of ('cooperation', 'between') is 0.5\n",
            "probability of ('between', 'linguistics') is 0.25\n",
            "probability of ('linguistics', 'and') is 0.4\n",
            "probability of ('and', 'natural') is 0.00847457627118644\n",
            "probability of ('processing', '(') is 0.1875\n",
            "probability of ('(', 'NLP') is 0.43137254901960786\n",
            "probability of ('NLP', ')') is 0.4772727272727273\n",
            "probability of (')', ',') is 0.19148936170212766\n",
            "probability of (',', 'in') is 0.02631578947368421\n",
            "probability of ('in', 'general') is 0.02666666666666667\n",
            "probability of ('general', ',') is 0.3333333333333333\n",
            "probability of (',', 'and') is 0.125\n",
            "probability of ('and', 'between') is 0.00847457627118644\n",
            "probability of ('and', 'machine') is 0.03389830508474576\n",
            "probability of ('machine', 'translation') is 0.21428571428571427\n",
            "probability of ('translation', '(') is 0.25\n",
            "probability of ('(', 'MT') is 0.0196078431372549\n",
            "probability of ('MT', ')') is 1.0\n",
            "probability of ('in', 'particular') is 0.02666666666666667\n",
            "probability of ('particular', '.') is 0.2\n",
            "probability of ('.', 'It') is 0.03424657534246575\n",
            "probability of ('It', 'focus') is 0.2\n",
            "probability of ('focus', 'on') is 0.9\n",
            "probability of ('on', 'just') is 0.043478260869565216\n",
            "probability of ('just', 'one') is 1.0\n",
            "probability of ('one', 'direction') is 0.2\n",
            "probability of ('direction', 'of') is 1.0\n",
            "probability of ('of', 'such') is 0.005681818181818182\n",
            "probability of ('such', 'cooperation') is 0.25\n",
            "probability of ('cooperation', ',') is 0.5\n",
            "probability of (',', 'namely') is 0.006578947368421052\n",
            "probability of ('namely', 'application') is 1.0\n",
            "probability of ('application', 'of') is 0.5454545454545454\n",
            "probability of ('of', 'linguistics') is 0.005681818181818182\n",
            "probability of ('linguistics', 'to') is 0.2\n",
            "probability of ('to', 'NLP') is 0.013513513513513514\n",
            "probability of ('NLP', ',') is 0.022727272727272728\n",
            "probability of (',', 'virtually') is 0.006578947368421052\n",
            "probability of ('virtually', 'In') is 1.0\n",
            "probability of ('In', 'most') is 0.058823529411764705\n",
            "probability of ('most', 'natural') is 0.5\n",
            "probability of ('processing', 'applications') is 0.015625\n",
            "probability of ('applications', ',') is 0.2\n",
            "probability of (',', 'Description') is 0.013157894736842105\n",
            "probability of ('Description', 'Logics') is 1.0\n",
            "probability of ('Logics', 'have') is 0.5\n",
            "probability of ('have', 'been') is 0.5\n",
            "probability of ('been', 'used') is 0.23076923076923078\n",
            "probability of ('used', 'to') is 0.36363636363636365\n",
            "probability of ('to', 'encode') is 0.013513513513513514\n",
            "probability of ('encode', 'in') is 0.5\n",
            "probability of ('in', 'a') is 0.08\n",
            "probability of ('a', 'knowledge') is 0.017094017094017096\n",
            "probability of ('knowledge', 'base') is 0.16666666666666666\n",
            "probability of ('base', 'some') is 0.5\n",
            "probability of ('some', 'syntactic') is 0.2\n",
            "probability of ('syntactic', ',') is 0.16666666666666666\n",
            "probability of (',', 'semantic') is 0.013157894736842105\n",
            "probability of ('semantic', ',') is 0.125\n",
            "probability of ('and', 'pragmatic') is 0.00847457627118644\n",
            "probability of ('pragmatic', 'element') is 1.0\n",
            "probability of ('element', 'needed') is 0.5\n",
            "probability of ('needed', 'to') is 1.0\n",
            "probability of ('to', 'drive') is 0.02702702702702703\n",
            "probability of ('drive', 'the') is 1.0\n",
            "probability of ('the', 'semantic') is 0.010869565217391304\n",
            "probability of ('semantic', 'interpretation') is 0.25\n",
            "probability of ('interpretation', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'the') is 0.06779661016949153\n",
            "probability of ('the', 'natural') is 0.010869565217391304\n",
            "probability of ('language', 'generation') is 0.01098901098901099\n",
            "probability of ('generation', 'processes') is 0.5\n",
            "probability of ('processes', '.') is 1.0\n",
            "probability of ('.', 'More') is 0.00684931506849315\n",
            "probability of ('More', 'recently') is 1.0\n",
            "probability of ('recently', ',') is 0.5\n",
            "probability of ('been', 'We') is 0.07692307692307693\n",
            "probability of ('We', 'propose') is 0.08333333333333333\n",
            "probability of ('propose', 'a') is 1.0\n",
            "probability of ('a', 'unified') is 0.008547008547008548\n",
            "probability of ('unified', 'neural') is 1.0\n",
            "probability of ('neural', 'network') is 1.0\n",
            "probability of ('network', 'architecture') is 0.4\n",
            "probability of ('architecture', 'and') is 0.2\n",
            "probability of ('and', 'learning') is 0.01694915254237288\n",
            "probability of ('learning', 'algorithm') is 0.05\n",
            "probability of ('algorithm', 'that') is 1.0\n",
            "probability of ('that', 'can') is 0.02631578947368421\n",
            "probability of ('can', 'be') is 0.625\n",
            "probability of ('be', 'applied') is 0.038461538461538464\n",
            "probability of ('applied', 'to') is 0.5\n",
            "probability of ('to', 'various') is 0.013513513513513514\n",
            "probability of ('various', 'natural') is 0.125\n",
            "probability of ('processing', 'task') is 0.015625\n",
            "probability of ('task', 'including') is 0.09090909090909091\n",
            "probability of ('including', 'part-of-speech') is 0.125\n",
            "probability of ('part-of-speech', 'tagging') is 0.5\n",
            "probability of ('tagging', ',') is 0.5\n",
            "probability of (',', 'chunking') is 0.006578947368421052\n",
            "probability of ('chunking', ',') is 1.0\n",
            "probability of (',', 'named') is 0.013157894736842105\n",
            "probability of ('named', 'entity') is 1.0\n",
            "probability of ('entity', 'recognition') is 0.5\n",
            "probability of ('recognition', ',') is 0.3333333333333333\n",
            "probability of ('and', 'semantic') is 0.01694915254237288\n",
            "probability of ('semantic', 'role') is 0.125\n",
            "probability of ('role', 'labeling') is 0.25\n",
            "probability of ('labeling', '.') is 1.0\n",
            "probability of ('.', 'This') is 0.10273972602739725\n",
            "probability of ('This', 'versatility') is 0.034482758620689655\n",
            "probability of ('versatility', 'is') is 1.0\n",
            "probability of ('is', 'achieved') is 0.018518518518518517\n",
            "probability of ('achieved', 'by') is 1.0\n",
            "probability of ('by', 'trying') is 0.05555555555555555\n",
            "probability of ('trying', 'to') is 1.0\n",
            "probability of ('to', 'avoid') is 0.02702702702702703\n",
            "probability of ('avoid', 'task') is 0.3333333333333333\n",
            "probability of ('task', 'Natural') is 0.09090909090909091\n",
            "probability of ('Natural', 'Language') is 0.6551724137931034\n",
            "probability of ('Language', 'Processing') is 0.8695652173913043\n",
            "probability of ('Processing', 'The') is 0.05\n",
            "probability of ('The', 'subject') is 0.038461538461538464\n",
            "probability of ('subject', 'of') is 0.5\n",
            "probability of ('of', 'Natural') is 0.017045454545454544\n",
            "probability of ('Processing', 'can') is 0.05\n",
            "probability of ('be', 'considered') is 0.038461538461538464\n",
            "probability of ('considered', 'in') is 1.0\n",
            "probability of ('in', 'both') is 0.02666666666666667\n",
            "probability of ('both', 'broad') is 0.2\n",
            "probability of ('broad', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'narrow') is 0.00847457627118644\n",
            "probability of ('narrow', 'senses') is 1.0\n",
            "probability of ('senses', '.') is 0.5\n",
            "probability of ('.', 'In') is 0.0547945205479452\n",
            "probability of ('In', 'the') is 0.11764705882352941\n",
            "probability of ('the', 'broad') is 0.005434782608695652\n",
            "probability of ('broad', 'sense') is 0.3333333333333333\n",
            "probability of ('sense', ',') is 0.2\n",
            "probability of (',', 'it') is 0.02631578947368421\n",
            "probability of ('it', 'cover') is 0.07692307692307693\n",
            "probability of ('cover', 'processing') is 1.0\n",
            "probability of ('processing', 'issue') is 0.015625\n",
            "probability of ('issue', 'at') is 0.2\n",
            "probability of ('at', 'all') is 0.14285714285714285\n",
            "probability of ('all', 'level') is 0.14285714285714285\n",
            "probability of ('level', 'of') is 0.5\n",
            "probability of ('of', 'natural') is 0.07386363636363637\n",
            "probability of ('language', 'understanding') is 0.02197802197802198\n",
            "probability of ('understanding', ',') is 0.16666666666666666\n",
            "probability of (',', 'including') is 0.006578947368421052\n",
            "probability of ('including', 'speech') is 0.125\n",
            "probability of ('speech', 'recognition') is 0.375\n",
            "probability of (',', 'syntactic') is 0.013157894736842105\n",
            "probability of ('syntactic', 'and') is 0.16666666666666666\n",
            "probability of ('semantic', 'analysis') is 0.125\n",
            "probability of ('analysis', 'of') is 0.2727272727272727\n",
            "probability of ('of', 'sentence') is 0.005681818181818182\n",
            "probability of ('sentence', 'Robots') is 0.3333333333333333\n",
            "probability of ('Robots', 'that') is 1.0\n",
            "probability of ('that', 'interact') is 0.02631578947368421\n",
            "probability of ('interact', 'with') is 1.0\n",
            "probability of ('with', 'human') is 0.047619047619047616\n",
            "probability of ('human', 'face-to-face') is 0.25\n",
            "probability of ('face-to-face', 'using') is 0.5\n",
            "probability of ('using', 'natural') is 0.18181818181818182\n",
            "probability of ('language', 'need') is 0.01098901098901099\n",
            "probability of ('need', 'to') is 0.3333333333333333\n",
            "probability of ('to', 'be') is 0.0945945945945946\n",
            "probability of ('be', 'responsive') is 0.038461538461538464\n",
            "probability of ('responsive', 'to') is 1.0\n",
            "probability of ('to', 'the') is 0.08108108108108109\n",
            "probability of ('the', 'way') is 0.016304347826086956\n",
            "probability of ('way', 'human') is 0.4\n",
            "probability of ('human', 'use') is 0.25\n",
            "probability of ('use', 'language') is 0.08333333333333333\n",
            "probability of ('language', 'in') is 0.03296703296703297\n",
            "probability of ('in', 'those') is 0.013333333333333334\n",
            "probability of ('those', 'situations') is 0.5\n",
            "probability of ('situations', '.') is 1.0\n",
            "probability of ('a', 'psychologicallyinspired') is 0.008547008547008548\n",
            "probability of ('psychologicallyinspired', 'natural') is 1.0\n",
            "probability of ('processing', 'system') is 0.046875\n",
            "probability of ('system', 'for') is 0.10526315789473684\n",
            "probability of ('for', 'robot') is 0.017857142857142856\n",
            "probability of ('robot', 'which') is 1.0\n",
            "probability of ('which', 'performs') is 0.2222222222222222\n",
            "probability of ('performs', 'incremental') is 0.5\n",
            "probability of ('incremental', 'semantic') is 1.0\n",
            "probability of ('interpretation', 'of') is 0.3333333333333333\n",
            "probability of ('of', 'spoken') is 0.005681818181818182\n",
            "probability of ('spoken', 'utterance') is 0.25\n",
            "probability of ('utterance', 'Natural') is 1.0\n",
            "probability of ('Natural', 'language') is 0.3103448275862069\n",
            "probability of ('language', 'are') is 0.03296703296703297\n",
            "probability of ('are', 'language') is 0.038461538461538464\n",
            "probability of ('language', 'spoken') is 0.01098901098901099\n",
            "probability of ('spoken', 'by') is 0.25\n",
            "probability of ('by', 'humans') is 0.05555555555555555\n",
            "probability of ('humans', '.') is 1.0\n",
            "probability of ('.', 'Currently') is 0.00684931506849315\n",
            "probability of ('Currently', 'we') is 1.0\n",
            "probability of ('we', 'are') is 0.058823529411764705\n",
            "probability of ('are', 'not') is 0.15384615384615385\n",
            "probability of ('not', 'yet') is 0.07142857142857142\n",
            "probability of ('yet', 'at') is 0.5\n",
            "probability of ('at', 'the') is 0.5714285714285714\n",
            "probability of ('the', 'point') is 0.005434782608695652\n",
            "probability of ('point', 'where') is 0.25\n",
            "probability of ('where', 'these') is 0.5\n",
            "probability of ('these', 'language') is 0.2\n",
            "probability of ('in', 'all') is 0.013333333333333334\n",
            "probability of ('all', 'of') is 0.14285714285714285\n",
            "probability of ('of', 'their') is 0.005681818181818182\n",
            "probability of ('their', 'unprocessed') is 0.25\n",
            "probability of ('unprocessed', 'form') is 1.0\n",
            "probability of ('form', 'can') is 0.2\n",
            "probability of ('be', 'understood') is 0.038461538461538464\n",
            "probability of ('understood', 'by') is 0.5\n",
            "probability of ('by', 'computers') is 0.05555555555555555\n",
            "probability of ('computers', '.') is 1.0\n",
            "probability of ('.', 'Natural') is 0.04794520547945205\n",
            "probability of ('processing', 'is') is 0.046875\n",
            "probability of ('is', 'the') is 0.14814814814814814\n",
            "probability of ('the', 'collection') is 0.005434782608695652\n",
            "probability of ('collection', 'of') is 0.5\n",
            "probability of ('of', 'technique') is 0.005681818181818182\n",
            "probability of ('technique', 'employed') is 0.08333333333333333\n",
            "probability of ('employed', 'to') is 0.5\n",
            "probability of ('to', 'try') is 0.013513513513513514\n",
            "probability of ('try', 'and') is 1.0\n",
            "probability of ('and', 'accomplish') is 0.00847457627118644\n",
            "probability of ('accomplish', 'that') is 1.0\n",
            "probability of ('that', 'goal') is 0.02631578947368421\n",
            "probability of ('goal', '.') is 0.3333333333333333\n",
            "probability of ('.', 'The') is 0.1506849315068493\n",
            "probability of ('The', 'field') is 0.038461538461538464\n",
            "probability of ('field', 'of') is 0.25\n",
            "probability of ('natural', 'ABSTRACT') is 0.015625\n",
            "probability of ('ABSTRACT', ':') is 0.75\n",
            "probability of (':', 'Ambiguity') is 0.05263157894736842\n",
            "probability of ('Ambiguity', 'can') is 1.0\n",
            "probability of ('be', 'referred') is 0.038461538461538464\n",
            "probability of ('referred', 'a') is 1.0\n",
            "probability of ('a', 'the') is 0.008547008547008548\n",
            "probability of ('the', 'ability') is 0.005434782608695652\n",
            "probability of ('ability', 'of') is 1.0\n",
            "probability of ('of', 'having') is 0.011363636363636364\n",
            "probability of ('having', 'more') is 0.5\n",
            "probability of ('more', 'than') is 0.2222222222222222\n",
            "probability of ('than', 'one') is 0.5\n",
            "probability of ('one', 'meaning') is 0.2\n",
            "probability of ('meaning', 'or') is 0.5\n",
            "probability of ('or', 'being') is 0.058823529411764705\n",
            "probability of ('being', 'understood') is 0.5\n",
            "probability of ('understood', 'in') is 0.5\n",
            "probability of ('in', 'more') is 0.013333333333333334\n",
            "probability of ('one', 'way') is 0.2\n",
            "probability of ('way', '.') is 0.2\n",
            "probability of ('are', 'ambiguous') is 0.038461538461538464\n",
            "probability of ('ambiguous', ',') is 1.0\n",
            "probability of (',', 'so') is 0.006578947368421052\n",
            "probability of ('so', 'computer') is 0.3333333333333333\n",
            "probability of ('computer', 'are') is 0.3333333333333333\n",
            "probability of ('not', 'able') is 0.07142857142857142\n",
            "probability of ('able', 'to') is 1.0\n",
            "probability of ('to', 'understand') is 0.013513513513513514\n",
            "probability of ('understand', 'language') is 1.0\n",
            "probability of ('language', 'the') is 0.01098901098901099\n",
            "probability of ('way', 'people') is 0.2\n",
            "probability of ('people', 'do') is 1.0\n",
            "probability of ('do', '.') is 0.3333333333333333\n",
            "probability of ('Processing', '(') is 0.55\n",
            "probability of (')', 'is') is 0.1276595744680851\n",
            "probability of ('is', 'concerned') is 0.018518518518518517\n",
            "probability of ('concerned', 'with') is 1.0\n",
            "probability of ('with', 'the') is 0.2857142857142857\n",
            "probability of ('the', 'development') is 0.021739130434782608\n",
            "probability of ('development', 'Introduction') is 0.09090909090909091\n",
            "probability of ('Introduction', 'Statistical') is 0.2\n",
            "probability of ('Statistical', 'natural') is 1.0\n",
            "probability of ('(', 'SNLP') is 0.0196078431372549\n",
            "probability of ('SNLP', ')') is 0.5\n",
            "probability of ('is', 'a') is 0.16666666666666666\n",
            "probability of ('a', 'field') is 0.008547008547008548\n",
            "probability of ('field', 'lying') is 0.125\n",
            "probability of ('lying', 'in') is 1.0\n",
            "probability of ('in', 'the') is 0.13333333333333333\n",
            "probability of ('the', 'intersection') is 0.005434782608695652\n",
            "probability of ('intersection', 'of') is 1.0\n",
            "probability of ('processing', 'and') is 0.015625\n",
            "probability of ('machine', 'learning') is 0.6428571428571429\n",
            "probability of ('learning', '.') is 0.15\n",
            "probability of ('.', 'SNLP') is 0.00684931506849315\n",
            "probability of ('SNLP', 'di') is 0.5\n",
            "probability of ('di', '#') is 1.0\n",
            "probability of ('#', 'ers') is 1.0\n",
            "probability of ('ers', 'from') is 1.0\n",
            "probability of ('from', 'traditional') is 0.05555555555555555\n",
            "probability of ('traditional', 'natural') is 0.5\n",
            "probability of ('processing', 'in') is 0.015625\n",
            "probability of ('in', 'that') is 0.013333333333333334\n",
            "probability of ('that', 'instead') is 0.02631578947368421\n",
            "probability of ('instead', 'of') is 1.0\n",
            "probability of ('having', 'a') is 0.5\n",
            "probability of ('a', 'linguist') is 0.008547008547008548\n",
            "probability of ('linguist', 'manually') is 1.0\n",
            "probability of ('manually', 'construct') is 0.5\n",
            "probability of ('construct', 'some') is 1.0\n",
            "probability of ('some', 'model') is 0.2\n",
            "probability of ('model', 'of') is 0.3\n",
            "probability of ('of', 'a') is 0.05113636363636364\n",
            "probability of ('a', 'given') is 0.008547008547008548\n",
            "probability of ('given', 'linguistic') is 0.25\n",
            "probability of ('linguistic', 'text') is 0.14285714285714285\n",
            "probability of ('text', 'directly') is 0.09090909090909091\n",
            "probability of ('directly', '(') is 1.0\n",
            "probability of ('(', 'rather') is 0.0196078431372549\n",
            "probability of ('rather', 'than') is 0.6666666666666666\n",
            "probability of ('than', 'e.g') is 0.25\n",
            "probability of ('e.g', '.') is 1.0\n",
            "probability of ('.', 'title') is 0.00684931506849315\n",
            "probability of ('title', 'and') is 1.0\n",
            "probability of ('and', 'abstracts') is 0.00847457627118644\n",
            "probability of ('abstracts', ')') is 1.0\n",
            "probability of ('and', 'suggests') is 0.00847457627118644\n",
            "probability of ('suggests', 'appropriate') is 1.0\n",
            "probability of ('appropriate', 'approach') is 1.0\n",
            "probability of ('approach', 'to') is 0.17647058823529413\n",
            "probability of ('to', 'doing') is 0.013513513513513514\n",
            "probability of ('doing', 'this') is 1.0\n",
            "probability of ('this', ',') is 0.045454545454545456\n",
            "probability of (',', 'with') is 0.006578947368421052\n",
            "probability of ('with', 'a') is 0.23809523809523808\n",
            "probability of ('a', 'focus') is 0.02564102564102564\n",
            "probability of ('on', 'the') is 0.2608695652173913\n",
            "probability of ('the', 'role') is 0.010869565217391304\n",
            "probability of ('role', 'of') is 0.5\n",
            "probability of ('paper', 'also') is 0.047619047619047616\n",
            "probability of ('also', 'comment') is 0.2\n",
            "probability of ('comment', 'on') is 1.0\n",
            "probability of ('on', 'possible') is 0.043478260869565216\n",
            "probability of ('possible', 'connection') is 0.5\n",
            "probability of ('connection', 'with') is 1.0\n",
            "probability of ('with', 'data') is 0.047619047619047616\n",
            "probability of ('data', 'and') is 0.16666666666666666\n",
            "probability of ('and', 'knowledge') is 0.00847457627118644\n",
            "probability of ('knowledge', 'retrieval') is 0.08333333333333333\n",
            "probability of ('retrieval', ',') is 0.1111111111111111\n",
            "probability of ('and', 'concludes') is 0.00847457627118644\n",
            "probability of ('concludes', 'by') is 1.0\n",
            "probability of ('by', 'emphasizing') is 0.05555555555555555\n",
            "probability of ('emphasizing', 'the') is 1.0\n",
            "probability of ('the', 'importance') is 0.005434782608695652\n",
            "probability of ('importance', 'of') is 1.0\n",
            "probability of ('of', 'rigorous') is 0.005681818181818182\n",
            "probability of ('rigorous', 'ABSTRACT') is 1.0\n",
            "probability of (':', 'Language') is 0.05263157894736842\n",
            "probability of ('Language', 'is') is 0.043478260869565216\n",
            "probability of ('is', 'way') is 0.018518518518518517\n",
            "probability of ('way', 'of') is 0.2\n",
            "probability of ('of', 'communicating') is 0.005681818181818182\n",
            "probability of ('communicating', 'your') is 1.0\n",
            "probability of ('your', 'word') is 1.0\n",
            "probability of ('word', 'Language') is 0.07692307692307693\n",
            "probability of ('Language', 'help') is 0.08695652173913043\n",
            "probability of ('help', 'in') is 0.5\n",
            "probability of ('in', 'understanding') is 0.013333333333333334\n",
            "probability of ('understanding', 'the') is 0.16666666666666666\n",
            "probability of ('the', 'world') is 0.010869565217391304\n",
            "probability of ('world', ',') is 0.5\n",
            "probability of (',', 'we') is 0.06578947368421052\n",
            "probability of ('we', 'get') is 0.058823529411764705\n",
            "probability of ('get', 'a') is 1.0\n",
            "probability of ('a', 'better') is 0.008547008547008548\n",
            "probability of ('better', 'insight') is 0.5\n",
            "probability of ('insight', 'of') is 1.0\n",
            "probability of ('of', 'the') is 0.14772727272727273\n",
            "probability of ('world', '.') is 0.5\n",
            "probability of ('.', 'Language') is 0.00684931506849315\n",
            "probability of ('help', 'speaker') is 0.5\n",
            "probability of ('speaker', 'to') is 0.5\n",
            "probability of ('be', 'a') is 0.07692307692307693\n",
            "probability of ('a', 'vague') is 0.008547008547008548\n",
            "probability of ('vague', 'or') is 1.0\n",
            "probability of ('or', 'a') is 0.11764705882352941\n",
            "probability of ('a', 'precise') is 0.008547008547008548\n",
            "probability of ('precise', 'a') is 1.0\n",
            "probability of ('a', 'they') is 0.008547008547008548\n",
            "probability of ('they', 'like') is 1.0\n",
            "probability of ('like', '.') is 0.5\n",
            "probability of ('.', 'NLP') is 0.00684931506849315\n",
            "probability of ('NLP', 'Stands') is 0.022727272727272728\n",
            "probability of ('Stands', 'for') is 1.0\n",
            "probability of ('language', 'processing..') is 0.01098901098901099\n",
            "probability of ('processing..', 'Natural') is 1.0\n",
            "probability of ('are', 'those') is 0.038461538461538464\n",
            "probability of ('those', 'language') is 0.5\n",
            "probability of ('language', 'that') is 0.02197802197802198\n",
            "probability of ('that', 'are') is 0.05263157894736842\n",
            "probability of ('are', 'spoken') is 0.038461538461538464\n",
            "probability of ('spoken', 'We') is 0.25\n",
            "probability of ('We', 'report') is 0.041666666666666664\n",
            "probability of ('report', 'experiment') is 0.3333333333333333\n",
            "probability of ('experiment', 'on') is 0.5\n",
            "probability of ('the', 'use') is 0.016304347826086956\n",
            "probability of ('use', 'of') is 0.6666666666666666\n",
            "probability of ('of', 'standard') is 0.005681818181818182\n",
            "probability of ('standard', 'natural') is 0.3333333333333333\n",
            "probability of (')', 'tool') is 0.02127659574468085\n",
            "probability of ('tool', 'for') is 0.3333333333333333\n",
            "probability of ('for', 'the') is 0.125\n",
            "probability of ('the', 'analysis') is 0.005434782608695652\n",
            "probability of ('of', 'music') is 0.017045454545454544\n",
            "probability of ('music', 'lyrics') is 0.25\n",
            "probability of ('lyrics', '.') is 1.0\n",
            "probability of ('.', 'A') is 0.0136986301369863\n",
            "probability of ('A', 'significant') is 0.25\n",
            "probability of ('significant', 'amount') is 0.3333333333333333\n",
            "probability of ('amount', 'of') is 1.0\n",
            "probability of ('music', 'audio') is 0.25\n",
            "probability of ('audio', 'ha') is 1.0\n",
            "probability of ('ha', 'lyrics') is 0.08333333333333333\n",
            "probability of ('.', 'Lyrics') is 0.00684931506849315\n",
            "probability of ('Lyrics', 'encode') is 1.0\n",
            "probability of ('encode', 'an') is 0.5\n",
            "probability of ('an', 'important') is 0.05\n",
            "probability of ('important', 'part') is 0.3333333333333333\n",
            "probability of ('part', 'of') is 0.6666666666666666\n",
            "probability of ('the', 'semantics') is 0.005434782608695652\n",
            "probability of ('semantics', 'of') is 1.0\n",
            "probability of ('a', 'song') is 0.008547008547008548\n",
            "probability of ('song', ',') is 1.0\n",
            "probability of (',', 'therefore') is 0.006578947368421052\n",
            "probability of ('therefore', 'their') is 1.0\n",
            "probability of ('their', 'analysis') is 0.25\n",
            "probability of ('analysis', 'complement') is 0.09090909090909091\n",
            "probability of ('complement', 'that') is 1.0\n",
            "probability of ('that', 'of') is 0.02631578947368421\n",
            "probability of ('of', 'acoustic') is 0.005681818181818182\n",
            "probability of ('acoustic', 'and') is 1.0\n",
            "probability of ('and', 'cultural') is 0.00847457627118644\n",
            "probability of ('cultural', 'this') is 1.0\n",
            "probability of ('this', 'paper') is 0.36363636363636365\n",
            "probability of ('paper', ',') is 0.23809523809523808\n",
            "probability of ('we', 'will') is 0.11764705882352941\n",
            "probability of ('will', 'describe') is 0.1111111111111111\n",
            "probability of ('a', 'simple') is 0.008547008547008548\n",
            "probability of ('simple', 'rule-based') is 1.0\n",
            "probability of ('rule-based', 'approach') is 1.0\n",
            "probability of ('to', 'automated') is 0.013513513513513514\n",
            "probability of ('automated', 'learning') is 0.2\n",
            "probability of ('learning', 'of') is 0.05\n",
            "probability of ('of', 'linguistic') is 0.022727272727272728\n",
            "probability of ('linguistic', 'knowledge') is 0.2857142857142857\n",
            "probability of ('knowledge', '.') is 0.08333333333333333\n",
            "probability of ('This', 'approach') is 0.034482758620689655\n",
            "probability of ('approach', 'ha') is 0.058823529411764705\n",
            "probability of ('ha', 'been') is 0.25\n",
            "probability of ('been', 'shown') is 0.07692307692307693\n",
            "probability of ('shown', 'for') is 0.5\n",
            "probability of ('for', 'a') is 0.10714285714285714\n",
            "probability of ('a', 'number') is 0.02564102564102564\n",
            "probability of ('number', 'of') is 1.0\n",
            "probability of ('of', 'task') is 0.017045454545454544\n",
            "probability of ('task', 'to') is 0.09090909090909091\n",
            "probability of ('to', 'capture') is 0.02702702702702703\n",
            "probability of ('capture', 'information') is 0.3333333333333333\n",
            "probability of ('information', 'in') is 0.05263157894736842\n",
            "probability of ('a', 'clearer') is 0.008547008547008548\n",
            "probability of ('clearer', 'and') is 1.0\n",
            "probability of ('and', 'more') is 0.01694915254237288\n",
            "probability of ('more', 'direct') is 0.1111111111111111\n",
            "probability of ('direct', 'fashion') is 1.0\n",
            "probability of ('fashion', 'without') is 1.0\n",
            "probability of ('without', 'a') is 1.0\n",
            "probability of ('a', 'compromise') is 0.008547008547008548\n",
            "probability of ('compromise', 'in') is 1.0\n",
            "probability of ('in', 'performance') is 0.013333333333333334\n",
            "probability of ('performance', '.') is 0.5\n",
            "probability of ('a', 'detailed') is 0.008547008547008548\n",
            "probability of ('detailed', 'case') is 1.0\n",
            "probability of ('case', 'study') is 0.5\n",
            "probability of ('study', 'of') is 0.6666666666666666\n",
            "probability of ('of', 'this') is 0.017045454545454544\n",
            "probability of ('this', 'learning') is 0.045454545454545456\n",
            "probability of ('learning', 'method') is 0.05\n",
            "probability of ('method', 'applied') is 0.14285714285714285\n",
            "probability of ('to', 'part') is 0.013513513513513514\n",
            "probability of ('of', 'speech') is 0.005681818181818182\n",
            "probability of ('speech', 'tagging') is 0.125\n",
            "probability of ('tagging', 'This') is 0.5\n",
            "probability of ('This', 'paper') is 0.3448275862068966\n",
            "probability of ('paper', 'focus') is 0.047619047619047616\n",
            "probability of ('on', 'connectionist') is 0.043478260869565216\n",
            "probability of ('connectionist', 'model') is 1.0\n",
            "probability of ('model', 'in') is 0.2\n",
            "probability of ('We', 'briefly') is 0.041666666666666664\n",
            "probability of ('briefly', 'present') is 0.3333333333333333\n",
            "probability of ('present', 'and') is 0.14285714285714285\n",
            "probability of ('and', 'discus') is 0.01694915254237288\n",
            "probability of ('discus', 'several') is 0.6666666666666666\n",
            "probability of ('several', 'aspect') is 0.25\n",
            "probability of ('aspect', 'of') is 1.0\n",
            "probability of ('of', 'high') is 0.011363636363636364\n",
            "probability of ('high', 'level') is 0.6666666666666666\n",
            "probability of ('level', 'task') is 0.25\n",
            "probability of ('task', 'which') is 0.09090909090909091\n",
            "probability of ('which', 'recently') is 0.1111111111111111\n",
            "probability of ('recently', 'have') is 0.5\n",
            "probability of ('been', 'approached') is 0.07692307692307693\n",
            "probability of ('approached', 'with') is 1.0\n",
            "probability of ('with', 'connectionism') is 0.047619047619047616\n",
            "probability of ('connectionism', ',') is 1.0\n",
            "probability of (',', 'either') is 0.006578947368421052\n",
            "probability of ('either', 'with') is 1.0\n",
            "probability of ('with', 'localist') is 0.047619047619047616\n",
            "probability of ('localist', 'or') is 1.0\n",
            "probability of ('or', 'parallel') is 0.058823529411764705\n",
            "probability of ('parallel', 'distributed') is 0.5\n",
            "probability of ('distributed', 'processing') is 1.0\n",
            "probability of ('processing', 'models') is 0.015625\n",
            "probability of ('models', '.') is 1.0\n",
            "probability of ('.', 'Several') is 0.00684931506849315\n",
            "probability of ('Several', 'interesting') is 1.0\n",
            "probability of ('interesting', 'architecture') is 1.0\n",
            "probability of ('architecture', 'process') is 0.2\n",
            "probability of ('process', 'of') is 0.36363636363636365\n",
            "probability of ('of', 'language') is 0.022727272727272728\n",
            "probability of ('understanding', '.') is 0.16666666666666666\n",
            "probability of ('This', 'is') is 0.06896551724137931\n",
            "probability of ('a', 'new') is 0.03418803418803419\n",
            "probability of ('new', 'approach') is 0.16666666666666666\n",
            "probability of ('approach', 'in') is 0.058823529411764705\n",
            "probability of ('processing', 'based') is 0.015625\n",
            "probability of ('the', 'deterministic') is 0.005434782608695652\n",
            "probability of ('deterministic', 'chaotic') is 1.0\n",
            "probability of ('chaotic', 'behavior') is 1.0\n",
            "probability of ('behavior', 'of') is 1.0\n",
            "probability of ('of', 'dynamical') is 0.005681818181818182\n",
            "probability of ('dynamical', 'systems') is 1.0\n",
            "probability of ('systems', '.') is 1.0\n",
            "probability of ('.', '1') is 0.0547945205479452\n",
            "probability of ('1', 'this') is 0.2222222222222222\n",
            "probability of ('paper', '(') is 0.047619047619047616\n",
            "probability of ('(', 'see') is 0.0196078431372549\n",
            "probability of ('see', '[') is 1.0\n",
            "probability of ('[', 'Schank') is 0.2\n",
            "probability of ('Schank', '86') is 1.0\n",
            "probability of ('86', ']') is 1.0\n",
            "probability of (']', 'for') is 0.4\n",
            "probability of ('a', 'theoretical') is 0.008547008547008548\n",
            "probability of ('theoretical', 'discussion') is 1.0\n",
            "probability of ('discussion', 'and') is 0.5\n",
            "probability of ('and', '[') is 0.01694915254237288\n",
            "probability of ('[', 'Kass') is 0.2\n",
            "probability of ('Kass', '86') is 1.0\n",
            "probability of (']', 'and') is 0.2\n",
            "probability of ('[', 'Leake') is 0.2\n",
            "probability of ('Leake', 'and') is 1.0\n",
            "probability of ('and', 'Owens') is 0.00847457627118644\n",
            "probability of ('Owens', '86') is 1.0\n",
            "probability of ('for', 'brief') is 0.017857142857142856\n",
            "probability of ('brief', 'discussion') is 0.3333333333333333\n",
            "probability of ('discussion', 'of') is 0.5\n",
            "probability of ('a', 'program') is 0.008547008547008548\n",
            "probability of ('program', 'built') is 0.3333333333333333\n",
            "probability of ('built', 'around') is 0.5\n",
            "probability of ('around', 'these') is 1.0\n",
            "probability of ('these', '.principles') is 0.2\n",
            "probability of ('.principles', ')') is 1.0\n",
            "probability of (')', ';') is 0.02127659574468085\n",
            "probability of (';', 'the') is 0.3333333333333333\n",
            "probability of ('the', 'goal') is 0.005434782608695652\n",
            "probability of ('goal', 'here') is 0.3333333333333333\n",
            "probability of ('here', 'is') is 0.5\n",
            "probability of ('is', 'simply') is 0.018518518518518517\n",
            "probability of ('simply', 'to') is 1.0\n",
            "probability of ('to', 'point') is 0.013513513513513514\n",
            "probability of ('point', 'out') is 0.5\n",
            "probability of ('out', 'how') is 0.3333333333333333\n",
            "probability of ('how', 'our') is 0.2\n",
            "probability of ('our', 'interest') is 0.25\n",
            "probability of ('interest', 'in') is 1.0\n",
            "probability of ('processing', 'ha') is 0.015625\n",
            "probability of ('ha', 'led') is 0.08333333333333333\n",
            "probability of ('led', 'u') is 1.0\n",
            "probability of ('u', 'naturally') is 0.5\n",
            "probability of ('naturally', ',') is 0.5\n",
            "probability of ('and', 'indeed') is 0.00847457627118644\n",
            "probability of ('indeed', 'inevitably') is 1.0\n",
            "probability of ('inevitably', 'Objectives') is 1.0\n",
            "probability of ('Objectives', 'To') is 1.0\n",
            "probability of ('To', 'provide') is 1.0\n",
            "probability of ('provide', 'an') is 1.0\n",
            "probability of ('an', 'overview') is 0.1\n",
            "probability of ('overview', 'and') is 0.5\n",
            "probability of ('and', 'tutorial') is 0.00847457627118644\n",
            "probability of ('tutorial', 'of') is 0.5\n",
            "probability of (')', 'and') is 0.1276595744680851\n",
            "probability of ('and', 'modern') is 0.00847457627118644\n",
            "probability of ('modern', 'NLP-system') is 1.0\n",
            "probability of ('NLP-system', 'design') is 1.0\n",
            "probability of ('design', '.') is 0.25\n",
            "probability of ('.', 'Target') is 0.00684931506849315\n",
            "probability of ('Target', 'audience') is 1.0\n",
            "probability of ('audience', 'This') is 1.0\n",
            "probability of ('This', 'tutorial') is 0.034482758620689655\n",
            "probability of ('tutorial', 'target') is 0.5\n",
            "probability of ('target', 'the') is 1.0\n",
            "probability of ('the', 'medical') is 0.005434782608695652\n",
            "probability of ('medical', 'informatics') is 1.0\n",
            "probability of ('informatics', 'generalist') is 1.0\n",
            "probability of ('generalist', 'who') is 1.0\n",
            "probability of ('who', 'ha') is 1.0\n",
            "probability of ('ha', 'limited') is 0.08333333333333333\n",
            "probability of ('limited', 'acquaintance') is 0.25\n",
            "probability of ('acquaintance', 'with') is 1.0\n",
            "probability of ('the', 'principle') is 0.005434782608695652\n",
            "probability of ('principle', 'behind') is 0.5\n",
            "probability of ('behind', 'NLP') is 1.0\n",
            "probability of ('NLP', 'and/or') is 0.022727272727272728\n",
            "probability of ('and/or', 'limited') is 1.0\n",
            "probability of ('limited', 'knowledge') is 0.25\n",
            "probability of ('knowledge', 'of') is 0.08333333333333333\n",
            "probability of ('the', 'current') is 0.021739130434782608\n",
            "probability of ('current', 'state') is 0.2857142857142857\n",
            "probability of ('state', 'This') is 0.2\n",
            "probability of ('paper', 'briefly') is 0.047619047619047616\n",
            "probability of ('briefly', 'describes') is 0.3333333333333333\n",
            "probability of ('describes', 'the') is 0.5\n",
            "probability of ('current', 'implementation') is 0.14285714285714285\n",
            "probability of ('implementation', 'status') is 0.5\n",
            "probability of ('status', 'of') is 1.0\n",
            "probability of ('of', 'an') is 0.017045454545454544\n",
            "probability of ('an', 'intelligent') is 0.05\n",
            "probability of ('intelligent', 'information') is 0.5\n",
            "probability of ('information', 'retrieval') is 0.21052631578947367\n",
            "probability of ('retrieval', 'system') is 0.2222222222222222\n",
            "probability of ('system', ',') is 0.15789473684210525\n",
            "probability of (',', 'MARIE') is 0.006578947368421052\n",
            "probability of ('MARIE', ',') is 1.0\n",
            "probability of (',', 'that') is 0.006578947368421052\n",
            "probability of ('that', 'employ') is 0.02631578947368421\n",
            "probability of ('employ', 'natural') is 1.0\n",
            "probability of ('processing', 'techniques') is 0.015625\n",
            "probability of ('techniques', '.') is 1.0\n",
            "probability of ('.', 'Descriptive') is 0.00684931506849315\n",
            "probability of ('Descriptive', 'caption') is 1.0\n",
            "probability of ('caption', 'are') is 1.0\n",
            "probability of ('are', 'used') is 0.038461538461538464\n",
            "probability of ('to', 'iden-') is 0.013513513513513514\n",
            "probability of ('iden-', 'tify') is 1.0\n",
            "probability of ('tify', 'photographic') is 1.0\n",
            "probability of ('photographic', 'image') is 1.0\n",
            "probability of ('image', 'concerning') is 0.3333333333333333\n",
            "probability of ('concerning', 'various') is 0.5\n",
            "probability of ('various', 'military') is 0.125\n",
            "probability of ('military', 'projects') is 1.0\n",
            "probability of ('projects', '.') is 1.0\n",
            "probability of ('The', 'caption') is 0.038461538461538464\n",
            "probability of ('are', 'parsed') is 0.038461538461538464\n",
            "probability of ('parsed', 'based') is 1.0\n",
            "probability of ('based', 'and') is 0.125\n",
            "probability of ('and', 'literature') is 0.00847457627118644\n",
            "probability of ('literature', 'resources') is 1.0\n",
            "probability of ('resources', '.') is 1.0\n",
            "probability of ('We', 'describe') is 0.16666666666666666\n",
            "probability of ('describe', 'here') is 0.1111111111111111\n",
            "probability of ('here', 'a') is 0.5\n",
            "probability of ('a', 'system') is 0.008547008547008548\n",
            "probability of ('for', 'agent') is 0.017857142857142856\n",
            "probability of ('agent', 'directed') is 1.0\n",
            "probability of ('directed', 'natural') is 1.0\n",
            "probability of ('processing', 'to') is 0.046875\n",
            "probability of ('to', 'extract') is 0.04054054054054054\n",
            "probability of ('extract', 'information') is 0.25\n",
            "probability of ('information', 'from') is 0.10526315789473684\n",
            "probability of ('from', 'journal') is 0.05555555555555555\n",
            "probability of ('journal', 'articles') is 1.0\n",
            "probability of ('articles', '.') is 1.0\n",
            "probability of ('.', 'An') is 0.00684931506849315\n",
            "probability of ('An', 'interface') is 0.5\n",
            "probability of ('interface', 'wa') is 0.5\n",
            "probability of ('wa', 'developed') is 1.0\n",
            "probability of ('developed', 'to') is 0.2\n",
            "probability of ('to', 'permit') is 0.013513513513513514\n",
            "probability of ('permit', 'curation') is 1.0\n",
            "probability of ('curation', 'of') is 1.0\n",
            "probability of ('the', 'NLP') is 0.005434782608695652\n",
            "probability of ('NLP', 'result') is 0.022727272727272728\n",
            "probability of ('result', 'and') is 0.125\n",
            "probability of ('and', 'deposition') is 0.00847457627118644\n",
            "probability of ('deposition', 'of') is 1.0\n",
            "probability of ('of', 'accepted') is 0.005681818181818182\n",
            "probability of ('accepted', 'result') is 1.0\n",
            "probability of ('result', 'into') is 0.125\n",
            "probability of ('into', 'a') is 0.5\n",
            "probability of ('base', '.') is 0.5\n",
            "probability of ('.', 'Motivation') is 0.00684931506849315\n",
            "probability of ('Motivation', ':') is 1.0\n",
            "probability of (':', 'The') is 0.05263157894736842\n",
            "probability of ('The', 'advent') is 0.038461538461538464\n",
            "probability of ('advent', 'of') is 1.0\n",
            "probability of ('high', 'to') is 0.3333333333333333\n",
            "probability of ('to', 'evaluation') is 0.013513513513513514\n",
            "probability of ('evaluation', 'in') is 0.25\n",
            "probability of ('in', 'speech') is 0.013333333333333334\n",
            "probability of ('speech', 'processing') is 0.125\n",
            "probability of ('.', 'Part') is 0.00684931506849315\n",
            "probability of ('Part', '2') is 1.0\n",
            "probability of ('2', 'survey') is 1.0\n",
            "probability of ('survey', 'significant') is 0.5\n",
            "probability of ('significant', 'evaluation') is 0.3333333333333333\n",
            "probability of ('evaluation', 'work') is 0.125\n",
            "probability of ('work', 'done') is 0.125\n",
            "probability of ('done', 'so') is 1.0\n",
            "probability of ('so', 'far') is 0.6666666666666666\n",
            "probability of ('far', ',') is 0.5\n",
            "probability of (',', 'for') is 0.006578947368421052\n",
            "probability of ('for', 'instance') is 0.017857142857142856\n",
            "probability of ('instance', 'in') is 1.0\n",
            "probability of ('in', 'machine') is 0.013333333333333334\n",
            "probability of ('translation', ',') is 0.5\n",
            "probability of ('discus', 'the') is 0.3333333333333333\n",
            "probability of ('the', 'particular') is 0.005434782608695652\n",
            "probability of ('particular', 'problem') is 0.2\n",
            "probability of ('problem', 'of') is 0.25\n",
            "probability of ('of', 'generic') is 0.005681818181818182\n",
            "probability of ('generic', 'system') is 0.3333333333333333\n",
            "probability of ('system', 'evaluation') is 0.05263157894736842\n",
            "probability of ('evaluation', '.') is 0.125\n",
            "probability of ('The', 'conclusion') is 0.038461538461538464\n",
            "probability of ('conclusion', 'is') is 1.0\n",
            "probability of ('is', 'that') is 0.018518518518518517\n",
            "probability of ('that', 'evaluation') is 0.02631578947368421\n",
            "probability of ('evaluation', 'strategy') is 0.125\n",
            "probability of ('strategy', 'and') is 1.0\n",
            "probability of ('and', 'technique') is 0.00847457627118644\n",
            "probability of ('technique', 'for') is 0.25\n",
            "probability of ('for', 'NLP') is 0.017857142857142856\n",
            "probability of ('NLP', 'need') is 0.022727272727272728\n",
            "probability of ('need', 'much') is 0.16666666666666666\n",
            "probability of ('much', 'more') is 0.5\n",
            "probability of ('more', 'development') is 0.1111111111111111\n",
            "probability of ('development', ',') is 0.09090909090909091\n",
            "probability of ('particular', 'similar') is 0.2\n",
            "probability of ('similar', 'to') is 0.5\n",
            "probability of ('human', 'intuitively') is 0.25\n",
            "probability of ('intuitively', 'do') is 0.5\n",
            "probability of ('do', 'in') is 0.3333333333333333\n",
            "probability of ('in', 'order') is 0.013333333333333334\n",
            "probability of ('order', 'to') is 1.0\n",
            "probability of ('to', 'eliminate') is 0.013513513513513514\n",
            "probability of ('eliminate', 'noisy') is 1.0\n",
            "probability of ('noisy', 'content') is 1.0\n",
            "probability of ('content', '.') is 1.0\n",
            "probability of ('In', 'this') is 0.47058823529411764\n",
            "probability of ('we', 'describe') is 0.11764705882352941\n",
            "probability of ('a', 'combination') is 0.008547008547008548\n",
            "probability of ('combination', 'of') is 1.0\n",
            "probability of ('of', 'HTML') is 0.005681818181818182\n",
            "probability of ('HTML', 'DOM') is 1.0\n",
            "probability of ('DOM', 'analysis') is 1.0\n",
            "probability of ('analysis', 'and') is 0.09090909090909091\n",
            "probability of ('and', 'Natural') is 0.01694915254237288\n",
            "probability of (')', 'technique') is 0.0425531914893617\n",
            "probability of ('for', 'automated') is 0.017857142857142856\n",
            "probability of ('automated', 'extraction') is 0.2\n",
            "probability of ('extraction', 'of') is 0.5\n",
            "probability of ('of', 'main') is 0.005681818181818182\n",
            "probability of ('main', 'article') is 1.0\n",
            "probability of ('article', 'with') is 0.3333333333333333\n",
            "probability of ('with', 'associated') is 0.047619047619047616\n",
            "probability of ('associated', 'image') is 1.0\n",
            "probability of ('image', 'from') is 0.3333333333333333\n",
            "probability of ('from', 'web') is 0.05555555555555555\n",
            "probability of ('web', 'pages') is 1.0\n",
            "probability of ('pages', '.') is 1.0\n",
            "probability of ('.', 'Abstract') is 0.02054794520547945\n",
            "probability of ('Abstract', '--') is 0.07692307692307693\n",
            "probability of ('--', 'Natural') is 0.07142857142857142\n",
            "probability of ('Processing', 'is') is 0.05\n",
            "probability of ('a', 'theoretically') is 0.008547008547008548\n",
            "probability of ('theoretically', 'motivated') is 1.0\n",
            "probability of ('motivated', 'range') is 1.0\n",
            "probability of ('range', 'of') is 1.0\n",
            "probability of ('of', 'computational') is 0.005681818181818182\n",
            "probability of ('computational', 'technique') is 0.2\n",
            "probability of ('for', 'analysing') is 0.017857142857142856\n",
            "probability of ('analysing', 'and') is 1.0\n",
            "probability of ('and', 'representing') is 0.00847457627118644\n",
            "probability of ('representing', 'naturally') is 1.0\n",
            "probability of ('naturally', 'occurring') is 0.5\n",
            "probability of ('occurring', 'text') is 1.0\n",
            "probability of ('text', 'at') is 0.09090909090909091\n",
            "probability of ('at', 'one') is 0.14285714285714285\n",
            "probability of ('one', 'or') is 0.2\n",
            "probability of ('or', 'more') is 0.058823529411764705\n",
            "probability of ('more', 'level') is 0.1111111111111111\n",
            "probability of ('linguistic', 'analysis') is 0.14285714285714285\n",
            "probability of ('analysis', 'for') is 0.09090909090909091\n",
            "probability of ('the', 'purpose') is 0.010869565217391304\n",
            "probability of ('purpose', 'of') is 0.6666666666666666\n",
            "probability of ('of', 'achieving') is 0.005681818181818182\n",
            "probability of ('achieving', 'human-like') is 1.0\n",
            "probability of ('human-like', 'language') is 1.0\n",
            "probability of ('processing', 'for') is 0.03125\n",
            "probability of ('a', 'range') is 0.008547008547008548\n",
            "probability of ('task', 'This') is 0.09090909090909091\n",
            "probability of ('paper', 'review') is 0.09523809523809523\n",
            "probability of ('review', 'the') is 0.5\n",
            "probability of ('the', 'process') is 0.021739130434782608\n",
            "probability of ('process', 'involved') is 0.09090909090909091\n",
            "probability of ('involved', 'in') is 1.0\n",
            "probability of ('in', 'Natural') is 0.02666666666666667\n",
            "probability of (')', '.') is 0.0851063829787234\n",
            "probability of ('It', 'then') is 0.2\n",
            "probability of ('then', 'demonstrates') is 0.2\n",
            "probability of ('demonstrates', 'the') is 1.0\n",
            "probability of ('the', 'various') is 0.005434782608695652\n",
            "probability of ('various', 'kind') is 0.125\n",
            "probability of ('kind', 'of') is 1.0\n",
            "probability of ('of', 'choice') is 0.005681818181818182\n",
            "probability of ('choice', 'that') is 1.0\n",
            "probability of ('that', 'need') is 0.05263157894736842\n",
            "probability of ('need', 'be') is 0.16666666666666666\n",
            "probability of ('be', 'taken') is 0.038461538461538464\n",
            "probability of ('taken', 'during') is 0.5\n",
            "probability of ('during', 'the') is 0.6666666666666666\n",
            "probability of ('the', 'execution') is 0.005434782608695652\n",
            "probability of ('execution', 'of') is 0.5\n",
            "probability of ('the', 'word') is 0.016304347826086956\n",
            "probability of ('word', 'morphology') is 0.07692307692307693\n",
            "probability of ('morphology', ',') is 1.0\n",
            "probability of (',', 'the') is 0.019736842105263157\n",
            "probability of ('the', 'syntactic') is 0.005434782608695652\n",
            "probability of ('syntactic', 'text') is 0.16666666666666666\n",
            "probability of ('text', 'analysis') is 0.18181818181818182\n",
            "probability of ('analysis', ',') is 0.18181818181818182\n",
            "probability of (',', 'or') is 0.013157894736842105\n",
            "probability of ('or', 'text') is 0.058823529411764705\n",
            "probability of ('text', 'generation') is 0.09090909090909091\n",
            "probability of ('generation', 'components') is 0.5\n",
            "probability of ('components', '.') is 1.0\n",
            "probability of ('It', 'compare') is 0.2\n",
            "probability of ('compare', 'the') is 0.5\n",
            "probability of ('the', 'time') is 0.005434782608695652\n",
            "probability of ('time', 'complexity') is 0.5\n",
            "probability of ('complexity', 'This') is 0.25\n",
            "probability of ('This', 'article') is 0.06896551724137931\n",
            "probability of ('article', 'focus') is 0.3333333333333333\n",
            "probability of ('the', 'derivation') is 0.005434782608695652\n",
            "probability of ('derivation', 'of') is 1.0\n",
            "probability of ('of', 'large') is 0.005681818181818182\n",
            "probability of ('large', 'lexicon') is 0.5\n",
            "probability of ('lexicon', 'for') is 0.5\n",
            "probability of ('describe', 'the') is 0.2222222222222222\n",
            "probability of ('development', 'of') is 0.2727272727272727\n",
            "probability of ('a', 'dictionary') is 0.008547008547008548\n",
            "probability of ('dictionary', 'support') is 0.5\n",
            "probability of ('support', 'environment') is 0.3333333333333333\n",
            "probability of ('environment', 'linking') is 0.5\n",
            "probability of ('linking', 'a') is 1.0\n",
            "probability of ('a', 'restructured') is 0.008547008547008548\n",
            "probability of ('restructured', 'version') is 1.0\n",
            "probability of ('version', 'of') is 1.0\n",
            "probability of ('the', 'Longman') is 0.005434782608695652\n",
            "probability of ('Longman', 'Dictionary') is 1.0\n",
            "probability of ('Dictionary', 'of') is 1.0\n",
            "probability of ('of', 'Contemporary') is 0.005681818181818182\n",
            "probability of ('Contemporary', 'English') is 1.0\n",
            "probability of ('English', 'to') is 0.5\n",
            "probability of ('to', 'natural') is 0.02702702702702703\n",
            "probability of ('processing', 'systems') is 0.03125\n",
            "probability of ('The', 'process') is 0.038461538461538464\n",
            "probability of ('process', 'We') is 0.09090909090909091\n",
            "probability of ('We', 'introduce') is 0.041666666666666664\n",
            "probability of ('introduce', 'a') is 1.0\n",
            "probability of ('for', 'analyzing') is 0.017857142857142856\n",
            "probability of ('analyzing', 'the') is 0.5\n",
            "probability of ('the', 'complexity') is 0.005434782608695652\n",
            "probability of ('complexity', 'of') is 0.5\n",
            "probability of ('processing', 'tasks') is 0.015625\n",
            "probability of ('tasks', ',') is 0.4444444444444444\n",
            "probability of ('and', 'for') is 0.01694915254237288\n",
            "probability of ('for', 'predicting') is 0.017857142857142856\n",
            "probability of ('predicting', 'the') is 1.0\n",
            "probability of ('the', 'difficulty') is 0.005434782608695652\n",
            "probability of ('difficulty', 'new') is 1.0\n",
            "probability of ('new', 'NLP') is 0.16666666666666666\n",
            "probability of ('NLP', 'tasks') is 0.06818181818181818\n",
            "probability of ('tasks', '.') is 0.4444444444444444\n",
            "probability of ('.', 'Our') is 0.0136986301369863\n",
            "probability of ('Our', 'complexity') is 0.5\n",
            "probability of ('complexity', 'measure') is 0.25\n",
            "probability of ('measure', 'are') is 1.0\n",
            "probability of ('are', 'derived') is 0.038461538461538464\n",
            "probability of ('derived', 'from') is 1.0\n",
            "probability of ('from', 'the') is 0.05555555555555555\n",
            "probability of ('the', 'Kolmogorov') is 0.005434782608695652\n",
            "probability of ('Kolmogorov', 'complexity') is 1.0\n",
            "probability of ('a', 'class') is 0.008547008547008548\n",
            "probability of ('class', 'of') is 0.25\n",
            "probability of ('of', 'automaton') is 0.005681818181818182\n",
            "probability of ('automaton', '—') is 1.0\n",
            "probability of ('—', 'meaning') is 1.0\n",
            "probability of ('meaning', 'automata') is 0.5\n",
            "probability of ('automata', ',') is 1.0\n",
            "probability of (',', 'whose') is 0.006578947368421052\n",
            "probability of ('whose', 'purpose') is 1.0\n",
            "probability of ('purpose', 'is') is 0.3333333333333333\n",
            "probability of ('is', 'to') is 0.1111111111111111\n",
            "probability of ('extract', 'relevant') is 0.25\n",
            "probability of ('relevant', 'piece') is 0.3333333333333333\n",
            "probability of ('piece', ',') is 1.0\n",
            "probability of (',', 'sounds') is 0.006578947368421052\n",
            "probability of ('sounds', ',') is 1.0\n",
            "probability of (',', 'text') is 0.02631578947368421\n",
            "probability of ('text', 'and') is 0.09090909090909091\n",
            "probability of ('and', 'motion') is 0.00847457627118644\n",
            "probability of ('motion', '.') is 1.0\n",
            "probability of ('The', 'technique') is 0.038461538461538464\n",
            "probability of ('technique', 'developed') is 0.08333333333333333\n",
            "probability of ('developed', 'from') is 0.2\n",
            "probability of ('from', 'deep') is 0.05555555555555555\n",
            "probability of ('deep', 'learning') is 1.0\n",
            "probability of ('learning', 'research') is 0.05\n",
            "probability of ('research', 'have') is 0.08333333333333333\n",
            "probability of ('have', 'already') is 0.08333333333333333\n",
            "probability of ('already', 'been') is 1.0\n",
            "probability of ('been', 'impacting') is 0.07692307692307693\n",
            "probability of ('impacting', 'the') is 1.0\n",
            "probability of ('the', 'research') is 0.010869565217391304\n",
            "probability of ('research', 'of') is 0.08333333333333333\n",
            "probability of ('language', 'process') is 0.01098901098901099\n",
            "probability of ('process', '.') is 0.09090909090909091\n",
            "probability of ('the', 'recent') is 0.005434782608695652\n",
            "probability of ('recent', 'research') is 0.5\n",
            "probability of ('research', 'on') is 0.08333333333333333\n",
            "probability of ('on', 'deep') is 0.043478260869565216\n",
            "probability of ('learning', ',') is 0.1\n",
            "probability of ('it', 'application') is 0.07692307692307693\n",
            "probability of ('application', 'and') is 0.09090909090909091\n",
            "probability of ('and', 'recent') is 0.00847457627118644\n",
            "probability of ('recent', 'development') is 0.3333333333333333\n",
            "probability of ('development', 'in') is 0.18181818181818182\n",
            "probability of ('1', 'This') is 0.1111111111111111\n",
            "probability of ('is', 'an') is 0.07407407407407407\n",
            "probability of ('an', 'author-produced') is 0.05\n",
            "probability of ('author-produced', 'version') is 1.0\n",
            "probability of ('a', 'paper') is 0.008547008547008548\n",
            "probability of ('paper', 'published') is 0.047619047619047616\n",
            "probability of ('published', 'in') is 1.0\n",
            "probability of ('in', 'The') is 0.013333333333333334\n",
            "probability of ('The', 'Abstract—Natural') is 0.038461538461538464\n",
            "probability of ('Abstract—Natural', 'language') is 0.5\n",
            "probability of ('the', 'application') is 0.02717391304347826\n",
            "probability of ('of', 'automated') is 0.005681818181818182\n",
            "probability of ('automated', 'parsing') is 0.2\n",
            "probability of ('parsing', 'and') is 0.25\n",
            "probability of ('learning', 'technique') is 0.1\n",
            "probability of ('technique', 'to') is 0.16666666666666666\n",
            "probability of ('to', 'analyze') is 0.013513513513513514\n",
            "probability of ('analyze', 'standard') is 1.0\n",
            "probability of ('standard', 'text') is 0.3333333333333333\n",
            "probability of ('text', '.') is 0.09090909090909091\n",
            "probability of ('.', 'Applications') is 0.00684931506849315\n",
            "probability of ('Applications', 'of') is 1.0\n",
            "probability of ('of', 'NLP') is 0.022727272727272728\n",
            "probability of ('NLP', 'to') is 0.045454545454545456\n",
            "probability of ('to', 'requirement') is 0.013513513513513514\n",
            "probability of ('requirement', 'engineering') is 0.25\n",
            "probability of ('engineering', 'include') is 0.5\n",
            "probability of ('include', 'extraction') is 0.5\n",
            "probability of ('of', 'ontology') is 0.005681818181818182\n",
            "probability of ('ontology', 'from') is 0.5\n",
            "probability of ('from', 'a') is 0.16666666666666666\n",
            "probability of ('a', 'requirement') is 0.008547008547008548\n",
            "probability of ('requirement', 'specification') is 0.25\n",
            "probability of ('specification', ',') is 1.0\n",
            "probability of ('and', 'use') is 0.01694915254237288\n",
            "probability of ('to', 'verify') is 0.013513513513513514\n",
            "probability of ('verify', 'the') is 1.0\n",
            "probability of ('the', 'consistency') is 0.005434782608695652\n",
            "probability of ('consistency', 'statistical') is 1.0\n",
            "probability of ('statistical', 'baseline') is 0.14285714285714285\n",
            "probability of ('baseline', 'including') is 1.0\n",
            "probability of ('including', ':') is 0.125\n",
            "probability of (':', 'the') is 0.10526315789473684\n",
            "probability of ('the', 'forgiving') is 0.005434782608695652\n",
            "probability of ('forgiving', 'nature') is 1.0\n",
            "probability of ('nature', 'but') is 1.0\n",
            "probability of ('but', 'broad') is 0.16666666666666666\n",
            "probability of ('broad', 'coverage') is 0.3333333333333333\n",
            "probability of ('coverage', 'of') is 1.0\n",
            "probability of ('the', 'typical') is 0.005434782608695652\n",
            "probability of ('typical', 'retrieval') is 1.0\n",
            "probability of ('retrieval', 'task') is 0.1111111111111111\n",
            "probability of ('task', ';') is 0.09090909090909091\n",
            "probability of ('the', 'lack') is 0.005434782608695652\n",
            "probability of ('lack', 'of') is 1.0\n",
            "probability of ('of', 'good') is 0.005681818181818182\n",
            "probability of ('good', 'weighting') is 0.5\n",
            "probability of ('weighting', 'scheme') is 0.5\n",
            "probability of ('scheme', 'for') is 1.0\n",
            "probability of ('for', 'compound') is 0.017857142857142856\n",
            "probability of ('compound', 'index') is 0.3333333333333333\n",
            "probability of ('index', 'terms') is 1.0\n",
            "probability of ('terms', ';') is 1.0\n",
            "probability of (';', 'and') is 0.16666666666666666\n",
            "probability of ('the', 'implicit') is 0.005434782608695652\n",
            "probability of ('implicit', 'linguistic') is 0.5\n",
            "probability of ('linguistic', 'processing') is 0.14285714285714285\n",
            "probability of ('processing', 'inherent') is 0.015625\n",
            "probability of ('inherent', 'in') is 1.0\n",
            "probability of ('the', 'statistical') is 0.005434782608695652\n",
            "probability of ('statistical', 'methods') is 0.14285714285714285\n",
            "probability of ('methods', '.') is 1.0\n",
            "probability of ('processing', 'technique') is 0.03125\n",
            "probability of ('technique', 'may') is 0.08333333333333333\n",
            "probability of ('may', 'be') is 0.3333333333333333\n",
            "probability of ('be', 'more') is 0.038461538461538464\n",
            "probability of ('more', 'important') is 0.1111111111111111\n",
            "probability of ('important', 'Work') is 0.3333333333333333\n",
            "probability of ('Work', 'in') is 1.0\n",
            "probability of ('in', 'computational') is 0.02666666666666667\n",
            "probability of ('computational', 'linguistics') is 0.4\n",
            "probability of ('linguistics', 'began') is 0.2\n",
            "probability of ('began', 'very') is 1.0\n",
            "probability of ('very', 'soon') is 0.5\n",
            "probability of ('soon', 'after') is 1.0\n",
            "probability of ('after', 'the') is 1.0\n",
            "probability of ('the', 'first') is 0.005434782608695652\n",
            "probability of ('first', 'computer') is 0.3333333333333333\n",
            "probability of ('computer', '(') is 0.3333333333333333\n",
            "probability of ('(', 'Booth') is 0.0196078431372549\n",
            "probability of ('Booth', ',') is 1.0\n",
            "probability of (',', 'Brandwood') is 0.006578947368421052\n",
            "probability of ('Brandwood', 'and') is 1.0\n",
            "probability of ('and', 'Cleave') is 0.00847457627118644\n",
            "probability of ('Cleave', '1958') is 1.0\n",
            "probability of ('1958', ')') is 1.0\n",
            "probability of (',', 'yet') is 0.006578947368421052\n",
            "probability of ('yet', 'in') is 0.5\n",
            "probability of ('the', 'intervening') is 0.005434782608695652\n",
            "probability of ('intervening', 'four') is 1.0\n",
            "probability of ('four', 'decade') is 1.0\n",
            "probability of ('decade', 'there') is 1.0\n",
            "probability of ('there', 'ha') is 1.0\n",
            "probability of ('been', 'a') is 0.07692307692307693\n",
            "probability of ('a', 'pervasive') is 0.008547008547008548\n",
            "probability of ('pervasive', 'feeling') is 1.0\n",
            "probability of ('feeling', 'that') is 1.0\n",
            "probability of ('that', 'progress') is 0.02631578947368421\n",
            "probability of ('progress', 'in') is 1.0\n",
            "probability of ('in', 'computer') is 0.013333333333333334\n",
            "probability of ('computer', 'understanding') is 0.3333333333333333\n",
            "probability of ('understanding', 'of') is 0.3333333333333333\n",
            "probability of ('language', 'ha') is 0.01098901098901099\n",
            "probability of ('ha', 'not') is 0.08333333333333333\n",
            "probability of ('not', 'been') is 0.07142857142857142\n",
            "probability of ('been', 'commensurate') is 0.07692307692307693\n",
            "probability of ('commensurate', 'the') is 1.0\n",
            "probability of ('the', 'voice') is 0.005434782608695652\n",
            "probability of ('voice', 'recognition') is 1.0\n",
            "probability of ('recognition', 'for') is 0.08333333333333333\n",
            "probability of ('a', 'natural') is 0.017094017094017096\n",
            "probability of ('language', '(') is 0.01098901098901099\n",
            "probability of ('(', 'Tamil') is 0.0196078431372549\n",
            "probability of ('Tamil', ')') is 1.0\n",
            "probability of (')', 'by') is 0.02127659574468085\n",
            "probability of ('by', 'combining') is 0.05555555555555555\n",
            "probability of ('combining', 'the') is 1.0\n",
            "probability of ('the', 'digital') is 0.005434782608695652\n",
            "probability of ('digital', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'mathematical') is 0.00847457627118644\n",
            "probability of ('mathematical', 'knowledge') is 0.5\n",
            "probability of ('knowledge', 'using') is 0.08333333333333333\n",
            "probability of ('using', 'MFCC') is 0.09090909090909091\n",
            "probability of ('MFCC', 'and') is 1.0\n",
            "probability of ('and', 'DTW') is 0.00847457627118644\n",
            "probability of ('DTW', 'to') is 1.0\n",
            "probability of ('extract', 'and') is 0.25\n",
            "probability of ('and', 'match') is 0.00847457627118644\n",
            "probability of ('match', 'the') is 1.0\n",
            "probability of ('the', 'feature') is 0.005434782608695652\n",
            "probability of ('feature', 'to') is 0.5\n",
            "probability of ('to', 'improve') is 0.02702702702702703\n",
            "probability of ('improve', 'the') is 0.5\n",
            "probability of ('the', 'accuracy') is 0.005434782608695652\n",
            "probability of ('accuracy', 'for') is 1.0\n",
            "probability of ('for', 'better') is 0.017857142857142856\n",
            "probability of ('better', 'performance') is 0.5\n",
            "probability of ('Abstract', ':') is 0.15384615384615385\n",
            "probability of (':', 'Testing') is 0.05263157894736842\n",
            "probability of ('Testing', 'against') is 1.0\n",
            "probability of ('against', 'natural') is 1.0\n",
            "probability of ('language', 'requirement') is 0.01098901098901099\n",
            "probability of ('requirement', 'is') is 0.25\n",
            "probability of ('the', 'standard') is 0.005434782608695652\n",
            "probability of ('standard', 'approach') is 0.3333333333333333\n",
            "probability of ('for', 'system') is 0.017857142857142856\n",
            "probability of ('system', 'and') is 0.05263157894736842\n",
            "probability of ('and', 'acceptance') is 0.00847457627118644\n",
            "probability of ('acceptance', 'testing') is 1.0\n",
            "probability of ('testing', '.') is 1.0\n",
            "probability of ('This', 'test') is 0.034482758620689655\n",
            "probability of ('test', 'is') is 0.5\n",
            "probability of ('is', 'often') is 0.018518518518518517\n",
            "probability of ('often', 'performed') is 0.5\n",
            "probability of ('performed', 'by') is 1.0\n",
            "probability of ('by', 'an') is 0.05555555555555555\n",
            "probability of ('an', 'independent') is 0.05\n",
            "probability of ('independent', 'test') is 0.5\n",
            "probability of ('test', 'organization') is 0.5\n",
            "probability of ('organization', 'unfamiliar') is 1.0\n",
            "probability of ('unfamiliar', 'with') is 1.0\n",
            "probability of ('application', 'area') is 0.09090909090909091\n",
            "probability of ('area', '.') is 0.5\n",
            "probability of ('The', 'only') is 0.038461538461538464\n",
            "probability of ('only', 'thing') is 0.5\n",
            "probability of ('thing', 'the') is 1.0\n",
            "probability of ('the', 'tester') is 0.005434782608695652\n",
            "probability of ('tester', 'have') is 1.0\n",
            "probability of ('have', 'to') is 0.08333333333333333\n",
            "probability of ('to', 'go') is 0.013513513513513514\n",
            "probability of ('go', 'by') is 0.5\n",
            "probability of ('by', 'are') is 0.05555555555555555\n",
            "probability of ('are', 'the') is 0.07692307692307693\n",
            "probability of ('the', 'written') is 0.005434782608695652\n",
            "probability of ('written', 'requirements') is 1.0\n",
            "probability of ('requirements', '.') is 1.0\n",
            "probability of ('.', 'So') is 0.0136986301369863\n",
            "probability of ('So', 'Abstract') is 0.5\n",
            "probability of ('found', 'conversational') is 0.125\n",
            "probability of ('conversational', 'partners') is 1.0\n",
            "probability of ('partners', '.') is 1.0\n",
            "probability of ('.', 'But') is 0.00684931506849315\n",
            "probability of ('But', 'it') is 1.0\n",
            "probability of ('it', 'also') is 0.07692307692307693\n",
            "probability of ('also', 'provides') is 0.2\n",
            "probability of ('provides', 'u') is 0.5\n",
            "probability of ('u', 'with') is 0.5\n",
            "probability of ('with', 'information') is 0.047619047619047616\n",
            "probability of ('information', 'about') is 0.15789473684210525\n",
            "probability of ('about', 'being') is 0.3333333333333333\n",
            "probability of ('being', 'creative') is 0.5\n",
            "probability of ('creative', ',') is 1.0\n",
            "probability of (',', 'making') is 0.006578947368421052\n",
            "probability of ('making', 'associations') is 1.0\n",
            "probability of ('associations', ',') is 1.0\n",
            "probability of (',', 'storytelling') is 0.006578947368421052\n",
            "probability of ('storytelling', 'and') is 1.0\n",
            "probability of ('and', 'language') is 0.00847457627118644\n",
            "probability of ('language', 'use') is 0.01098901098901099\n",
            "probability of ('use', '.') is 0.08333333333333333\n",
            "probability of ('.', 'Many') is 0.00684931506849315\n",
            "probability of ('Many', 'more') is 0.3333333333333333\n",
            "probability of ('more', 'subtlety') is 0.1111111111111111\n",
            "probability of ('subtlety', 'in') is 1.0\n",
            "probability of ('in', 'face-to-face') is 0.013333333333333334\n",
            "probability of ('face-to-face', 'and') is 0.5\n",
            "probability of ('and', 'multiparty') is 0.00847457627118644\n",
            "probability of ('multiparty', 'interaction') is 1.0\n",
            "probability of ('interaction', 'can') is 1.0\n",
            "probability of ('be', 'added') is 0.038461538461538464\n",
            "probability of ('added', ',') is 1.0\n",
            "probability of (',', 'such') is 0.013157894736842105\n",
            "probability of ('such', 'a') is 0.75\n",
            "probability of ('a', 'using') is 0.008547008547008548\n",
            "probability of ('using', 'humor') is 0.09090909090909091\n",
            "probability of ('humor', 'to') is 1.0\n",
            "probability of ('to', 'persuade') is 0.013513513513513514\n",
            "probability of ('persuade', 'and') is 1.0\n",
            "probability of ('and', 'dominate') is 0.00847457627118644\n",
            "probability of ('dominate', ',') is 1.0\n",
            "probability of (',', 'to') is 0.02631578947368421\n",
            "probability of ('to', 'soften') is 0.013513513513513514\n",
            "probability of ('soften', 'or') is 1.0\n",
            "probability of ('or', 'avoid') is 0.058823529411764705\n",
            "probability of ('avoid', 'a') is 0.3333333333333333\n",
            "probability of ('a', 'face') is 0.008547008547008548\n",
            "probability of ('face', 'threatening') is 1.0\n",
            "probability of ('threatening', 'act') is 1.0\n",
            "probability of ('act', 'Abstract') is 1.0\n",
            "probability of ('found', 'In') is 0.25\n",
            "probability of ('In', 'recent') is 0.058823529411764705\n",
            "probability of ('recent', 'years') is 0.16666666666666666\n",
            "probability of ('years', ',') is 0.5\n",
            "probability of (',', 'machine') is 0.02631578947368421\n",
            "probability of ('learning', '(') is 0.05\n",
            "probability of ('(', 'ML') is 0.0392156862745098\n",
            "probability of ('ML', ')') is 1.0\n",
            "probability of (')', 'ha') is 0.02127659574468085\n",
            "probability of ('used', 'more') is 0.09090909090909091\n",
            "probability of ('more', 'and') is 0.1111111111111111\n",
            "probability of ('more', 'to') is 0.1111111111111111\n",
            "probability of ('to', 'solve') is 0.02702702702702703\n",
            "probability of ('solve', 'complex') is 0.5\n",
            "probability of ('complex', 'task') is 1.0\n",
            "probability of ('task', 'in') is 0.09090909090909091\n",
            "probability of ('in', 'different') is 0.02666666666666667\n",
            "probability of ('different', 'disciplines') is 0.25\n",
            "probability of ('disciplines', ',') is 1.0\n",
            "probability of (',', 'ranging') is 0.006578947368421052\n",
            "probability of ('ranging', 'from') is 1.0\n",
            "probability of ('from', 'Data') is 0.05555555555555555\n",
            "probability of ('Data', 'Mining') is 1.0\n",
            "probability of ('Mining', 'to') is 1.0\n",
            "probability of ('to', 'Information') is 0.013513513513513514\n",
            "probability of ('Information', 'We') is 0.3333333333333333\n",
            "probability of ('We', 'argue') is 0.041666666666666664\n",
            "probability of ('argue', 'that') is 0.6666666666666666\n",
            "probability of ('that', 'manual') is 0.02631578947368421\n",
            "probability of ('manual', 'and') is 0.5\n",
            "probability of ('and', 'automatic') is 0.00847457627118644\n",
            "probability of ('automatic', 'thesaurus') is 1.0\n",
            "probability of ('thesaurus', 'are') is 0.2\n",
            "probability of ('are', 'alternative') is 0.038461538461538464\n",
            "probability of ('alternative', 'resource') is 1.0\n",
            "probability of ('resource', 'for') is 0.3333333333333333\n",
            "probability of ('the', 'same') is 0.016304347826086956\n",
            "probability of ('same', 'NLP') is 0.3333333333333333\n",
            "probability of ('This', 'involves') is 0.034482758620689655\n",
            "probability of ('involves', 'the') is 0.5\n",
            "probability of ('the', 'radical') is 0.005434782608695652\n",
            "probability of ('radical', 'step') is 1.0\n",
            "probability of ('step', 'of') is 1.0\n",
            "probability of ('of', 'interpreting') is 0.005681818181818182\n",
            "probability of ('interpreting', 'manual') is 1.0\n",
            "probability of ('manual', 'thesaurus') is 0.5\n",
            "probability of ('thesaurus', 'a') is 0.2\n",
            "probability of ('a', 'classification') is 0.008547008547008548\n",
            "probability of ('classification', 'of') is 1.0\n",
            "probability of ('of', 'word') is 0.005681818181818182\n",
            "probability of ('word', 'rather') is 0.07692307692307693\n",
            "probability of ('than', 'word') is 0.25\n",
            "probability of ('word', 'senses') is 0.07692307692307693\n",
            "probability of ('senses', ':') is 0.5\n",
            "probability of ('the', 'case') is 0.005434782608695652\n",
            "probability of ('case', 'for') is 0.5\n",
            "probability of ('for', 'this') is 0.017857142857142856\n",
            "probability of ('this', 'is') is 0.045454545454545456\n",
            "probability of ('is', 'made') is 0.018518518518518517\n",
            "probability of ('made', '.') is 0.3333333333333333\n",
            "probability of ('The', 'range') is 0.038461538461538464\n",
            "probability of ('of', 'role') is 0.005681818181818182\n",
            "probability of ('role', 'for') is 0.25\n",
            "probability of ('for', 'thesaurus') is 0.017857142857142856\n",
            "probability of ('thesaurus', 'within') is 0.2\n",
            "probability of ('within', 'NLP') is 0.4\n",
            "probability of ('NLP', 'is') is 0.022727272727272728\n",
            "probability of ('is', 'briefly') is 0.018518518518518517\n",
            "probability of ('briefly', 'presented') is 0.3333333333333333\n",
            "probability of ('presented', 'and') is 0.5\n",
            "probability of ('the', 'WASPS') is 0.005434782608695652\n",
            "probability of ('WASPS', 'thesaurus') is 1.0\n",
            "probability of ('thesaurus', 'is') is 0.2\n",
            "probability of ('is', 'introduced') is 0.018518518518518517\n",
            "probability of ('introduced', '.') is 0.5\n",
            "probability of ('.', 'Thesaurus') is 0.00684931506849315\n",
            "probability of ('Thesaurus', 'evaluation') is 1.0\n",
            "probability of ('evaluation', 'is') is 0.125\n",
            "probability of ('is', 'now') is 0.018518518518518517\n",
            "probability of ('now', 'becoming') is 1.0\n",
            "probability of ('becoming', 'urgent') is 1.0\n",
            "probability of ('urgent', '.') is 1.0\n",
            "probability of ('A', 'range') is 0.25\n",
            "probability of ('of', 'evaluation') is 0.005681818181818182\n",
            "probability of ('evaluation', 'strategies') is 0.125\n",
            "probability of ('strategies', ',') is 1.0\n",
            "probability of (',', 'all') is 0.013157894736842105\n",
            "probability of ('all', 'embedded') is 0.14285714285714285\n",
            "probability of ('embedded', 'within') is 1.0\n",
            "probability of (',', 'is') is 0.006578947368421052\n",
            "probability of ('is', 'proposed') is 0.018518518518518517\n",
            "probability of ('proposed', '.') is 1.0\n",
            "probability of ('.', 'Introduction') is 0.00684931506849315\n",
            "probability of ('Introduction', 'Patterns') is 0.2\n",
            "probability of ('Patterns', 'in') is 0.5\n",
            "probability of ('in', 'music') is 0.013333333333333334\n",
            "probability of ('music', 'have') is 0.25\n",
            "probability of ('been', 'the') is 0.07692307692307693\n",
            "probability of ('the', 'object') is 0.005434782608695652\n",
            "probability of ('object', 'of') is 0.5\n",
            "probability of ('of', 'intensive') is 0.005681818181818182\n",
            "probability of ('intensive', 'study') is 1.0\n",
            "probability of ('study', 'in') is 0.3333333333333333\n",
            "probability of ('the', 'past') is 0.010869565217391304\n",
            "probability of ('past', 'years') is 0.5\n",
            "probability of ('years', '.') is 0.5\n",
            "probability of ('.', '\\\\One') is 0.00684931506849315\n",
            "probability of ('\\\\One', 'of') is 1.0\n",
            "probability of ('of', 'analyzing') is 0.005681818181818182\n",
            "probability of ('analyzing', 'musical') is 0.5\n",
            "probability of ('musical', 'structure') is 0.3333333333333333\n",
            "probability of ('structure', 'and') is 0.25\n",
            "probability of ('and', 'form') is 0.00847457627118644\n",
            "probability of ('form', 'is') is 0.2\n",
            "probability of ('to', 'discover') is 0.013513513513513514\n",
            "probability of ('discover', 'the') is 1.0\n",
            "probability of ('the', 'pattern') is 0.010869565217391304\n",
            "probability of ('pattern', 'that') is 0.2\n",
            "probability of ('are', 'explicit') is 0.038461538461538464\n",
            "probability of ('explicit', 'or') is 1.0\n",
            "probability of ('or', 'implicit') is 0.058823529411764705\n",
            "probability of ('implicit', 'in') is 0.5\n",
            "probability of ('in', 'musical') is 0.013333333333333334\n",
            "probability of ('musical', 'works') is 0.3333333333333333\n",
            "probability of ('works', \"''\") is 1.0\n",
            "probability of (\"''\", 'Simon') is 0.2\n",
            "probability of ('Simon', '[') is 1.0\n",
            "probability of ('[', '13') is 0.2\n",
            "probability of ('13', ']') is 1.0\n",
            "probability of (']', '.') is 0.2\n",
            "probability of ('.', 'Patterns') is 0.00684931506849315\n",
            "probability of ('Patterns', 'comprise') is 0.5\n",
            "probability of ('comprise', 'periodicity') is 1.0\n",
            "probability of ('periodicity', ',') is 1.0\n",
            "probability of (',', 'make') is 0.013157894736842105\n",
            "probability of ('make', 'use') is 0.5\n",
            "probability of ('of', 'alphabets') is 0.005681818181818182\n",
            "probability of ('alphabets', ',') is 1.0\n",
            "probability of (',', 'can') is 0.006578947368421052\n",
            "probability of ('be', 'compound') is 0.038461538461538464\n",
            "probability of ('compound', '(') is 0.3333333333333333\n",
            "probability of ('(', 'made') is 0.0196078431372549\n",
            "probability of ('made', 'up') is 0.3333333333333333\n",
            "probability of ('up', 'of') is 1.0\n",
            "probability of ('of', 'subpatterns') is 0.005681818181818182\n",
            "probability of ('subpatterns', ')') is 1.0\n",
            "probability of ('and', 'posse') is 0.00847457627118644\n",
            "probability of ('posse', 'phrase') is 1.0\n",
            "probability of ('phrase', 'structure') is 1.0\n",
            "probability of ('structure', 'with') is 0.25\n",
            "probability of ('with', 'various') is 0.047619047619047616\n",
            "probability of ('various', 'form') is 0.125\n",
            "probability of ('form', 'of') is 0.2\n",
            "probability of ('of', 'punctuation') is 0.005681818181818182\n",
            "probability of ('punctuation', '.') is 1.0\n",
            "probability of ('.', 'Traditionally') is 0.00684931506849315\n",
            "probability of ('Traditionally', ',') is 1.0\n",
            "probability of (',', 'composer') is 0.006578947368421052\n",
            "probability of ('composer', 'have') is 1.0\n",
            "probability of ('have', 'employed') is 0.08333333333333333\n",
            "probability of ('employed', 'pattern') is 0.5\n",
            "probability of ('pattern', 'propagation') is 0.4\n",
            "probability of ('propagation', 'intuitively') is 0.5\n",
            "probability of ('intuitively', ',') is 0.5\n",
            "probability of (',', 'but') is 0.013157894736842105\n",
            "probability of ('but', 'algorithmic') is 0.16666666666666666\n",
            "probability of ('algorithmic', 'composition') is 1.0\n",
            "probability of ('composition', 'technique') is 0.5\n",
            "probability of ('technique', 'allow') is 0.08333333333333333\n",
            "probability of ('allow', 'the') is 1.0\n",
            "probability of ('propagation', 'to') is 0.5\n",
            "probability of ('be', 'formalized') is 0.038461538461538464\n",
            "probability of ('formalized', ',') is 1.0\n",
            "probability of (',', 'albeit') is 0.006578947368421052\n",
            "probability of ('albeit', 'a') is 1.0\n",
            "probability of ('a', 'high') is 0.008547008547008548\n",
            "probability of ('level', '.') is 0.25\n",
            "probability of ('.', 'During') is 0.0136986301369863\n",
            "probability of ('During', 'composition') is 0.5\n",
            "probability of ('composition', ',') is 0.5\n",
            "probability of ('all', 'the') is 0.2857142857142857\n",
            "probability of ('the', 'musical') is 0.005434782608695652\n",
            "probability of ('musical', 'pattern') is 0.3333333333333333\n",
            "probability of ('pattern', 'evolve') is 0.2\n",
            "probability of ('evolve', 'according') is 1.0\n",
            "probability of ('according', 'to') is 1.0\n",
            "probability of ('the', 'rule') is 0.005434782608695652\n",
            "probability of ('rule', 'and') is 0.5\n",
            "probability of ('and', 'constraint') is 0.00847457627118644\n",
            "probability of ('constraint', 'specied') is 0.5\n",
            "probability of ('specied', 'at') is 1.0\n",
            "probability of ('the', 'design') is 0.016304347826086956\n",
            "probability of ('design', 'stage') is 0.25\n",
            "probability of ('stage', '.') is 1.0\n",
            "probability of ('In', 'jazz') is 0.058823529411764705\n",
            "probability of ('jazz', 'improvisation') is 1.0\n",
            "probability of ('improvisation', ',') is 1.0\n",
            "probability of ('the', 'musician') is 0.005434782608695652\n",
            "probability of ('musician', 'invents') is 1.0\n",
            "probability of ('invents', 'a') is 1.0\n",
            "probability of ('a', 'solo') is 0.008547008547008548\n",
            "probability of ('solo', 'guided') is 0.5\n",
            "probability of ('guided', 'by') is 1.0\n",
            "probability of ('by', 'a') is 0.05555555555555555\n",
            "probability of ('a', 'progression') is 0.008547008547008548\n",
            "probability of ('progression', 'of') is 0.5\n",
            "probability of ('of', 'chord') is 0.005681818181818182\n",
            "probability of ('chord', '(') is 1.0\n",
            "probability of ('(', 'the') is 0.0196078431372549\n",
            "probability of ('the', 'changes') is 0.005434782608695652\n",
            "probability of ('changes', ')') is 1.0\n",
            "probability of ('.', 'One') is 0.0136986301369863\n",
            "probability of ('One', 'approach') is 0.5\n",
            "probability of ('approach', '[') is 0.058823529411764705\n",
            "probability of ('[', '1') is 0.2\n",
            "probability of ('1', ']') is 0.1111111111111111\n",
            "probability of (']', 'to') is 0.2\n",
            "probability of ('to', 'learn') is 0.013513513513513514\n",
            "probability of ('learn', 'improvising') is 0.5\n",
            "probability of ('improvising', 'is') is 1.0\n",
            "probability of ('to', 'memorize') is 0.013513513513513514\n",
            "probability of ('memorize', 'pattern') is 1.0\n",
            "probability of ('pattern', '(') is 0.2\n",
            "probability of ('(', 'short') is 0.0196078431372549\n",
            "probability of ('short', 'chunk') is 0.5\n",
            "probability of ('chunk', 'of') is 1.0\n",
            "probability of ('music', ')') is 0.25\n",
            "probability of (')', 'that') is 0.0425531914893617\n",
            "probability of ('that', 't') is 0.05263157894736842\n",
            "probability of ('t', 'sub-progressions') is 0.5\n",
            "probability of ('sub-progressions', ',') is 1.0\n",
            "probability of ('and', 'to') is 0.00847457627118644\n",
            "probability of ('to', 'concatenate') is 0.013513513513513514\n",
            "probability of ('concatenate', 'them') is 1.0\n",
            "probability of ('them', 'to') is 0.25\n",
            "probability of ('to', 'form') is 0.013513513513513514\n",
            "probability of ('form', 'a') is 0.4\n",
            "probability of ('a', 'whole') is 0.017094017094017096\n",
            "probability of ('whole', 'solo') is 0.5\n",
            "probability of ('solo', 'that') is 0.5\n",
            "probability of ('t', 'a') is 0.5\n",
            "probability of ('whole', 'progression') is 0.5\n",
            "probability of ('progression', '.') is 0.5\n",
            "probability of ('One', 'Abstract') is 0.5\n",
            "probability of ('Abstract', 'Many') is 0.07692307692307693\n",
            "probability of ('Many', 'information') is 0.3333333333333333\n",
            "probability of ('retrieval', '(') is 0.1111111111111111\n",
            "probability of ('(', 'IR') is 0.0196078431372549\n",
            "probability of ('IR', ')') is 1.0\n",
            "probability of (')', 'system') is 0.02127659574468085\n",
            "probability of ('system', 'retrieve') is 0.05263157894736842\n",
            "probability of ('retrieve', 'relevant') is 1.0\n",
            "probability of ('relevant', 'document') is 0.6666666666666666\n",
            "probability of ('document', 'based') is 0.1111111111111111\n",
            "probability of ('on', 'exact') is 0.043478260869565216\n",
            "probability of ('exact', 'matching') is 1.0\n",
            "probability of ('matching', 'of') is 0.5\n",
            "probability of ('of', 'keywords') is 0.005681818181818182\n",
            "probability of ('keywords', 'between') is 1.0\n",
            "probability of ('between', 'a') is 0.125\n",
            "probability of ('a', 'query') is 0.008547008547008548\n",
            "probability of ('query', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'documents') is 0.00847457627118644\n",
            "probability of ('documents', '.') is 1.0\n",
            "probability of ('This', 'method') is 0.034482758620689655\n",
            "probability of ('method', 'degrades') is 0.14285714285714285\n",
            "probability of ('degrades', 'precision') is 1.0\n",
            "probability of ('precision', 'rate') is 1.0\n",
            "probability of ('rate', '.') is 0.5\n",
            "probability of ('In', 'order') is 0.058823529411764705\n",
            "probability of ('solve', 'the') is 0.5\n",
            "probability of ('the', 'problem') is 0.005434782608695652\n",
            "probability of ('problem', ',') is 0.25\n",
            "probability of ('we', 'collected') is 0.058823529411764705\n",
            "probability of ('collected', 'semantically') is 1.0\n",
            "probability of ('semantically', 'related') is 0.5\n",
            "probability of ('related', 'word') is 0.5\n",
            "probability of ('word', 'and') is 0.15384615384615385\n",
            "probability of ('and', 'assigned') is 0.00847457627118644\n",
            "probability of ('assigned', 'semantic') is 1.0\n",
            "probability of ('semantic', 'relationship') is 0.125\n",
            "probability of ('relationship', 'used') is 0.3333333333333333\n",
            "probability of ('used', 'in') is 0.45454545454545453\n",
            "probability of ('general', 'thesaurus') is 0.3333333333333333\n",
            "probability of ('thesaurus', 'and') is 0.2\n",
            "probability of ('and', 'a') is 0.00847457627118644\n",
            "probability of ('a', 'special') is 0.008547008547008548\n",
            "probability of ('special', 'relationship') is 0.5\n",
            "probability of ('relationship', 'called') is 0.3333333333333333\n",
            "probability of ('called', 'keyfact') is 0.5\n",
            "probability of ('keyfact', 'term') is 0.5\n",
            "probability of ('term', '(') is 0.5\n",
            "probability of ('(', 'FT') is 0.0196078431372549\n",
            "probability of ('FT', ')') is 1.0\n",
            "probability of (')', 'manually') is 0.02127659574468085\n",
            "probability of ('manually', '.') is 0.5\n",
            "probability of ('In', 'addition') is 0.058823529411764705\n",
            "probability of ('addition', 'to') is 1.0\n",
            "probability of ('semantic', 'knowledge') is 0.125\n",
            "probability of ('knowledge', ',') is 0.08333333333333333\n",
            "probability of ('we', 'automatically') is 0.058823529411764705\n",
            "probability of ('automatically', 'constructed') is 0.3333333333333333\n",
            "probability of ('constructed', 'statistic') is 1.0\n",
            "probability of ('statistic', 'knowledge') is 0.3333333333333333\n",
            "probability of ('knowledge', 'based') is 0.08333333333333333\n",
            "probability of ('the', 'concept') is 0.005434782608695652\n",
            "probability of ('concept', 'of') is 1.0\n",
            "probability of ('of', 'mutual') is 0.005681818181818182\n",
            "probability of ('mutual', 'information') is 1.0\n",
            "probability of ('information', '.') is 0.10526315789473684\n",
            "probability of ('.', 'Keyfact') is 0.0136986301369863\n",
            "probability of ('Keyfact', 'is') is 0.5\n",
            "probability of ('an', 'extended') is 0.05\n",
            "probability of ('extended', 'concept') is 1.0\n",
            "probability of ('of', 'keyword') is 0.005681818181818182\n",
            "probability of ('keyword', 'represented') is 1.0\n",
            "probability of ('represented', 'by') is 1.0\n",
            "probability of ('by', 'noun') is 0.05555555555555555\n",
            "probability of ('noun', 'and') is 0.5\n",
            "probability of ('and', 'compound') is 0.00847457627118644\n",
            "probability of ('compound', 'noun') is 0.3333333333333333\n",
            "probability of ('noun', '.') is 0.5\n",
            "probability of ('Keyfact', 'can') is 0.5\n",
            "probability of ('a', 'verb') is 0.008547008547008548\n",
            "probability of ('verb', 'and') is 0.5\n",
            "probability of ('and', 'an') is 0.00847457627118644\n",
            "probability of ('an', 'adjective') is 0.05\n",
            "probability of ('adjective', 'including') is 1.0\n",
            "probability of ('including', 'subject') is 0.125\n",
            "probability of ('subject', 'or') is 0.5\n",
            "probability of ('or', 'object') is 0.058823529411764705\n",
            "probability of ('object', 'term') is 0.5\n",
            "probability of ('term', '.') is 0.5\n",
            "probability of ('We', 'first') is 0.041666666666666664\n",
            "probability of ('first', 'retrieved') is 0.3333333333333333\n",
            "probability of ('retrieved', 'relevant') is 1.0\n",
            "probability of ('document', 'with') is 0.1111111111111111\n",
            "probability of ('with', 'original') is 0.047619047619047616\n",
            "probability of ('original', 'query') is 1.0\n",
            "probability of ('query', 'using') is 0.3333333333333333\n",
            "probability of ('using', 'tf') is 0.09090909090909091\n",
            "probability of ('tf', '*') is 1.0\n",
            "probability of ('*', 'idf') is 1.0\n",
            "probability of ('idf', 'weighting') is 1.0\n",
            "probability of ('weighting', 'formula') is 0.5\n",
            "probability of ('formula', 'and') is 1.0\n",
            "probability of ('and', 'then') is 0.03389830508474576\n",
            "probability of ('then', 'an') is 0.2\n",
            "probability of ('an', 'expanded') is 0.05\n",
            "probability of ('expanded', 'query') is 0.5\n",
            "probability of ('query', 'including') is 0.3333333333333333\n",
            "probability of ('including', 'keyfacts') is 0.125\n",
            "probability of ('keyfacts', 'is') is 1.0\n",
            "probability of ('is', 'used') is 0.018518518518518517\n",
            "probability of ('both', 'second') is 0.2\n",
            "probability of ('second', 'document') is 1.0\n",
            "probability of ('document', 'ranking') is 0.1111111111111111\n",
            "probability of ('ranking', 'and') is 1.0\n",
            "probability of ('and', 'word') is 0.00847457627118644\n",
            "probability of ('word', 'sense') is 0.23076923076923078\n",
            "probability of ('sense', 'disambiguating') is 0.2\n",
            "probability of ('disambiguating', '.') is 1.0\n",
            "probability of ('So', 'we') is 0.5\n",
            "probability of ('we', 'made') is 0.058823529411764705\n",
            "probability of ('made', 'an') is 0.3333333333333333\n",
            "probability of ('an', 'improvement') is 0.05\n",
            "probability of ('improvement', 'in') is 0.5\n",
            "probability of ('in', 'precision') is 0.013333333333333334\n",
            "probability of ('rate', 'using') is 0.5\n",
            "probability of ('using', 'keyfact') is 0.09090909090909091\n",
            "probability of ('keyfact', 'network') is 0.5\n",
            "probability of ('network', '.') is 0.2\n",
            "probability of ('paper', 'we') is 0.09523809523809523\n",
            "probability of ('we', 'argue') is 0.058823529411764705\n",
            "probability of ('that', 'questionanswering') is 0.02631578947368421\n",
            "probability of ('questionanswering', '(') is 1.0\n",
            "probability of ('(', 'QA') is 0.0196078431372549\n",
            "probability of ('QA', ')') is 0.3333333333333333\n",
            "probability of (')', 'over') is 0.02127659574468085\n",
            "probability of ('over', 'technical') is 0.2\n",
            "probability of ('technical', 'domain') is 1.0\n",
            "probability of ('domain', 'is') is 0.2\n",
            "probability of ('is', 'distinctly') is 0.018518518518518517\n",
            "probability of ('distinctly', 'different') is 1.0\n",
            "probability of ('different', 'from') is 0.25\n",
            "probability of ('from', 'TREC-based') is 0.05555555555555555\n",
            "probability of ('TREC-based', 'QA') is 1.0\n",
            "probability of ('QA', 'or') is 0.3333333333333333\n",
            "probability of ('or', 'Web-based') is 0.058823529411764705\n",
            "probability of ('Web-based', 'QA') is 0.5\n",
            "probability of ('QA', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'it') is 0.00847457627118644\n",
            "probability of ('it', 'can') is 0.07692307692307693\n",
            "probability of ('can', 'not') is 0.0625\n",
            "probability of ('not', 'benefit') is 0.07142857142857142\n",
            "probability of ('benefit', 'lom') is 0.5\n",
            "probability of ('lom', 'data-intensive') is 1.0\n",
            "probability of ('data-intensive', 'approach') is 1.0\n",
            "probability of ('approach', 'Universit') is 0.058823529411764705\n",
            "probability of ('Universit', '&') is 1.0\n",
            "probability of ('&', 'quot') is 0.3333333333333333\n",
            "probability of ('quot', ';') is 1.0\n",
            "probability of (';', 'at') is 0.16666666666666666\n",
            "probability of ('at', 'de') is 0.14285714285714285\n",
            "probability of ('de', 'Saarlandes') is 1.0\n",
            "probability of ('Saarlandes', 'Proceedings') is 1.0\n",
            "probability of ('Proceedings', 'of') is 1.0\n",
            "probability of ('the', 'Workshop') is 0.005434782608695652\n",
            "probability of ('Workshop', 'on') is 1.0\n",
            "probability of ('on', 'uni-hamburg.de') is 0.043478260869565216\n",
            "probability of ('uni-hamburg.de', 'Abstract') is 1.0\n",
            "probability of ('found', 'Abstract') is 0.25\n",
            "probability of ('found', 'SRI') is 0.125\n",
            "probability of ('SRI', 'ha') is 1.0\n",
            "probability of ('ha', 'developed') is 0.08333333333333333\n",
            "probability of ('developed', 'a') is 0.4\n",
            "probability of ('new', 'architecture') is 0.16666666666666666\n",
            "probability of ('architecture', 'for') is 0.2\n",
            "probability of ('for', 'integrating') is 0.017857142857142856\n",
            "probability of ('integrating', 'speech') is 0.5\n",
            "probability of ('speech', 'and') is 0.125\n",
            "probability of ('and', 'natural-language') is 0.00847457627118644\n",
            "probability of ('natural-language', 'processing') is 1.0\n",
            "probability of ('processing', 'that') is 0.015625\n",
            "probability of ('that', 'applies') is 0.02631578947368421\n",
            "probability of ('applies', 'linguistic') is 1.0\n",
            "probability of ('linguistic', 'constraint') is 0.14285714285714285\n",
            "probability of ('constraint', 'during') is 0.5\n",
            "probability of ('during', 'recognition') is 0.3333333333333333\n",
            "probability of ('recognition', 'by') is 0.08333333333333333\n",
            "probability of ('by', 'incrementally') is 0.05555555555555555\n",
            "probability of ('incrementally', 'expanding') is 1.0\n",
            "probability of ('expanding', 'the') is 1.0\n",
            "probability of ('the', 'state-transition') is 0.005434782608695652\n",
            "probability of ('state-transition', 'network') is 1.0\n",
            "probability of ('network', 'embodied') is 0.2\n",
            "probability of ('embodied', 'in') is 1.0\n",
            "probability of ('a', 'unification') is 0.008547008547008548\n",
            "probability of ('unification', 'grammar') is 1.0\n",
            "probability of ('grammar', '.') is 0.5\n",
            "probability of ('We', 'compare') is 0.041666666666666664\n",
            "probability of ('compare', 'this') is 0.5\n",
            "probability of ('this', 'dynamic-gralnlnar-network') is 0.045454545454545456\n",
            "probability of ('dynamic-gralnlnar-network', '(') is 1.0\n",
            "probability of ('(', 'DGN') is 0.0196078431372549\n",
            "probability of ('DGN', ')') is 1.0\n",
            "probability of (')', 'approach') is 0.02127659574468085\n",
            "probability of ('approach', 'This') is 0.11764705882352941\n",
            "probability of ('This', 'chapter') is 0.13793103448275862\n",
            "probability of ('chapter', 'considers') is 0.25\n",
            "probability of ('considers', 'the') is 1.0\n",
            "probability of ('the', 'revolution') is 0.005434782608695652\n",
            "probability of ('revolution', 'that') is 1.0\n",
            "probability of ('that', 'ha') is 0.02631578947368421\n",
            "probability of ('ha', 'taken') is 0.08333333333333333\n",
            "probability of ('taken', 'place') is 0.5\n",
            "probability of ('place', 'in') is 1.0\n",
            "probability of ('processing', 'research') is 0.015625\n",
            "probability of ('research', 'over') is 0.08333333333333333\n",
            "probability of ('over', 'the') is 0.8\n",
            "probability of ('the', 'last') is 0.021739130434782608\n",
            "probability of ('last', 'five') is 0.25\n",
            "probability of ('five', 'years') is 1.0\n",
            "probability of ('It', 'begin') is 0.2\n",
            "probability of ('begin', 'by') is 1.0\n",
            "probability of ('by', 'providing') is 0.05555555555555555\n",
            "probability of ('providing', 'a') is 1.0\n",
            "probability of ('a', 'brief') is 0.017094017094017096\n",
            "probability of ('brief', 'guide') is 0.3333333333333333\n",
            "probability of ('guide', 'to') is 1.0\n",
            "probability of ('the', 'structure') is 0.005434782608695652\n",
            "probability of ('structure', 'of') is 0.25\n",
            "probability of ('the', 'field') is 0.010869565217391304\n",
            "probability of ('field', 'and') is 0.125\n",
            "probability of ('then', 'present') is 0.2\n",
            "probability of ('a', 'caricature') is 0.008547008547008548\n",
            "probability of ('caricature', 'of') is 1.0\n",
            "probability of ('of', 'two') is 0.005681818181818182\n",
            "probability of ('two', 'competing') is 0.3333333333333333\n",
            "probability of ('competing', 'paradigm') is 1.0\n",
            "probability of ('paradigm', 'of') is 1.0\n",
            "probability of ('of', '1980s') is 0.005681818181818182\n",
            "probability of ('1980s', 'NLP') is 1.0\n",
            "probability of ('NLP', 'research') is 0.022727272727272728\n",
            "probability of ('research', 'and') is 0.16666666666666666\n",
            "probability of ('and', 'indicates') is 0.00847457627118644\n",
            "probability of ('indicates', 'the') is 1.0\n",
            "probability of ('the', 'reason') is 0.005434782608695652\n",
            "probability of ('reason', 'visual') is 1.0\n",
            "probability of ('visual', 'development') is 0.3333333333333333\n",
            "probability of ('development', 'environment') is 0.09090909090909091\n",
            "probability of ('environment', 'to') is 0.5\n",
            "probability of ('to', 'support') is 0.013513513513513514\n",
            "probability of ('support', 'the') is 0.3333333333333333\n",
            "probability of ('the', 'visual') is 0.005434782608695652\n",
            "probability of ('visual', 'assembly') is 0.3333333333333333\n",
            "probability of ('assembly', ',') is 1.0\n",
            "probability of (',', 'execution') is 0.006578947368421052\n",
            "probability of ('execution', 'and') is 0.5\n",
            "probability of ('and', 'analysis') is 0.00847457627118644\n",
            "probability of ('of', 'modular') is 0.005681818181818182\n",
            "probability of ('modular', 'natural') is 1.0\n",
            "probability of ('The', 'visual') is 0.038461538461538464\n",
            "probability of ('visual', 'model') is 0.3333333333333333\n",
            "probability of ('model', 'is') is 0.1\n",
            "probability of ('an', 'executable') is 0.05\n",
            "probability of ('executable', 'data') is 1.0\n",
            "probability of ('data', 'flow') is 0.16666666666666666\n",
            "probability of ('flow', 'program') is 1.0\n",
            "probability of ('program', 'graph') is 0.3333333333333333\n",
            "probability of ('graph', ',') is 0.5\n",
            "probability of (',', 'automatically') is 0.006578947368421052\n",
            "probability of ('automatically', 'synthesised') is 0.3333333333333333\n",
            "probability of ('synthesised', 'from') is 1.0\n",
            "probability of ('from', 'data') is 0.05555555555555555\n",
            "probability of ('data', 'dependency') is 0.16666666666666666\n",
            "probability of ('dependency', 'declaration') is 0.5\n",
            "probability of ('declaration', 'of') is 1.0\n",
            "probability of ('processing', 'modules') is 0.015625\n",
            "probability of ('modules', '.') is 1.0\n",
            "probability of ('The', 'graph') is 0.038461538461538464\n",
            "probability of ('graph', 'In') is 0.5\n",
            "probability of ('this', 'Chapter') is 0.045454545454545456\n",
            "probability of ('Chapter', 'the') is 1.0\n",
            "probability of ('the', 'basic') is 0.016304347826086956\n",
            "probability of ('basic', 'us') is 0.25\n",
            "probability of ('us', 'of') is 0.5\n",
            "probability of ('of', 'Description') is 0.011363636363636364\n",
            "probability of ('Logics', 'for') is 0.25\n",
            "probability of ('for', 'Natural') is 0.017857142857142856\n",
            "probability of ('Processing', 'will') is 0.05\n",
            "probability of ('will', 'be') is 0.4444444444444444\n",
            "probability of ('be', 'analysed') is 0.038461538461538464\n",
            "probability of ('analysed', ',') is 1.0\n",
            "probability of (',', 'together') is 0.006578947368421052\n",
            "probability of ('together', 'with') is 1.0\n",
            "probability of ('a', 'little') is 0.008547008547008548\n",
            "probability of ('little', 'bit') is 1.0\n",
            "probability of ('bit', 'of') is 1.0\n",
            "probability of ('of', 'history') is 0.011363636363636364\n",
            "probability of ('history', ',') is 0.25\n",
            "probability of ('Logics', 'in') is 0.25\n",
            "probability of ('state', 'of') is 0.6\n",
            "probability of ('the', 'art') is 0.016304347826086956\n",
            "probability of ('art', 'in') is 0.25\n",
            "probability of ('linguistics', 'will') is 0.2\n",
            "probability of ('be', 'pointed') is 0.038461538461538464\n",
            "probability of ('pointed', 'out') is 1.0\n",
            "probability of ('out', '.') is 0.3333333333333333\n",
            "probability of ('.', '18.1') is 0.00684931506849315\n",
            "probability of ('18.1', 'Introduction') is 1.0\n",
            "probability of ('Introduction', 'Since') is 0.2\n",
            "probability of ('Since', 'the') is 1.0\n",
            "probability of ('the', 'early') is 0.005434782608695652\n",
            "probability of ('early', 'day') is 1.0\n",
            "probability of ('day', 'We') is 1.0\n",
            "probability of ('We', 'applied') is 0.041666666666666664\n",
            "probability of ('applied', 'a') is 0.25\n",
            "probability of ('a', 'structure') is 0.008547008547008548\n",
            "probability of ('structure', 'learning') is 0.25\n",
            "probability of ('learning', 'model') is 0.05\n",
            "probability of ('model', ',') is 0.1\n",
            "probability of (',', 'Max-Margin') is 0.006578947368421052\n",
            "probability of ('Max-Margin', 'Structure') is 1.0\n",
            "probability of ('Structure', '(') is 1.0\n",
            "probability of ('(', 'MMS') is 0.0196078431372549\n",
            "probability of ('MMS', ')') is 1.0\n",
            "probability of (')', 'tasks') is 0.06382978723404255\n",
            "probability of (',', 'where') is 0.006578947368421052\n",
            "probability of ('where', 'the') is 0.5\n",
            "probability of ('the', 'aim') is 0.005434782608695652\n",
            "probability of ('aim', 'is') is 1.0\n",
            "probability of ('capture', 'the') is 0.6666666666666666\n",
            "probability of ('the', 'latent') is 0.005434782608695652\n",
            "probability of ('latent', 'relationship') is 1.0\n",
            "probability of ('relationship', 'within') is 0.3333333333333333\n",
            "probability of ('within', 'the') is 0.2\n",
            "probability of ('the', 'output') is 0.005434782608695652\n",
            "probability of ('output', 'language') is 0.3333333333333333\n",
            "probability of ('language', 'domain') is 0.01098901098901099\n",
            "probability of ('domain', '.') is 0.2\n",
            "probability of ('We', 'formulate') is 0.041666666666666664\n",
            "probability of ('formulate', 'this') is 1.0\n",
            "probability of ('this', 'model') is 0.045454545454545456\n",
            "probability of ('model', 'a') is 0.1\n",
            "probability of ('a', 'an') is 0.017094017094017096\n",
            "probability of ('an', 'extension') is 0.05\n",
            "probability of ('extension', 'of') is 1.0\n",
            "probability of ('of', 'multi–class') is 0.005681818181818182\n",
            "probability of ('multi–class', 'Support') is 1.0\n",
            "probability of ('Support', 'Vector') is 1.0\n",
            "probability of ('Vector', 'Machine') is 1.0\n",
            "probability of ('Machine', '(') is 0.3333333333333333\n",
            "probability of ('(', 'SVM') is 0.0196078431372549\n",
            "probability of ('SVM', ')') is 1.0\n",
            "probability of ('and', 'present') is 0.00847457627118644\n",
            "probability of ('a', '-mation') is 0.008547008547008548\n",
            "probability of ('-mation', 'Infrastructure') is 1.0\n",
            "probability of ('Infrastructure', ',') is 1.0\n",
            "probability of (',', 'digital') is 0.013157894736842105\n",
            "probability of ('digital', 'libraries') is 0.3333333333333333\n",
            "probability of ('libraries', ',') is 1.0\n",
            "probability of (',', 'networked') is 0.006578947368421052\n",
            "probability of ('networked', 'services') is 1.0\n",
            "probability of ('services', ',') is 1.0\n",
            "probability of ('digital', 'convergence') is 0.3333333333333333\n",
            "probability of ('convergence', 'or') is 1.0\n",
            "probability of ('or', 'intelligent') is 0.058823529411764705\n",
            "probability of ('intelligent', 'agents') is 0.5\n",
            "probability of ('agents', '.') is 1.0\n",
            "probability of ('This', 'attention') is 0.034482758620689655\n",
            "probability of ('attention', 'is') is 1.0\n",
            "probability of ('is', 'moving') is 0.018518518518518517\n",
            "probability of ('moving', 'natural') is 1.0\n",
            "probability of ('processing', 'along') is 0.015625\n",
            "probability of ('along', 'the') is 1.0\n",
            "probability of ('the', 'critical') is 0.005434782608695652\n",
            "probability of ('critical', 'path') is 1.0\n",
            "probability of ('path', 'for') is 1.0\n",
            "probability of ('for', 'all') is 0.017857142857142856\n",
            "probability of ('all', 'kind') is 0.14285714285714285\n",
            "probability of ('of', 'novel') is 0.005681818181818182\n",
            "probability of ('novel', 'applications') is 1.0\n",
            "probability of ('applications', '.') is 0.8\n",
            "probability of ('article', 'will') is 0.3333333333333333\n",
            "probability of ('will', 'mention') is 0.1111111111111111\n",
            "probability of ('mention', 'a') is 1.0\n",
            "probability of ('of', 'successful') is 0.005681818181818182\n",
            "probability of ('successful', 'application') is 1.0\n",
            "probability of ('NLP', 'Over') is 0.022727272727272728\n",
            "probability of ('Over', 'the') is 1.0\n",
            "probability of ('last', 'few') is 0.25\n",
            "probability of ('few', 'years') is 1.0\n",
            "probability of (',', 'a') is 0.03289473684210526\n",
            "probability of ('of', 'area') is 0.005681818181818182\n",
            "probability of ('area', 'of') is 0.5\n",
            "probability of ('processing', 'have') is 0.015625\n",
            "probability of ('have', 'begun') is 0.08333333333333333\n",
            "probability of ('begun', 'applying') is 1.0\n",
            "probability of ('applying', 'graph-based') is 0.5\n",
            "probability of ('graph-based', 'techniques') is 1.0\n",
            "probability of ('.', 'These') is 0.00684931506849315\n",
            "probability of ('These', 'include') is 1.0\n",
            "probability of ('include', ',') is 0.5\n",
            "probability of (',', 'among') is 0.006578947368421052\n",
            "probability of ('among', 'others') is 0.3333333333333333\n",
            "probability of ('others', ',') is 1.0\n",
            "probability of ('text', 'summarization') is 0.09090909090909091\n",
            "probability of ('summarization', ',') is 1.0\n",
            "probability of ('syntactic', 'parsing') is 0.3333333333333333\n",
            "probability of ('parsing', ',') is 0.5\n",
            "probability of (',', 'word') is 0.013157894736842105\n",
            "probability of ('sense', 'disambiguation') is 0.4\n",
            "probability of ('disambiguation', ',') is 0.5\n",
            "probability of (',', 'ontology') is 0.006578947368421052\n",
            "probability of ('ontology', 'construction') is 0.5\n",
            "probability of ('construction', ',') is 1.0\n",
            "probability of (',', 'sentiment') is 0.006578947368421052\n",
            "probability of ('sentiment', 'and') is 1.0\n",
            "probability of ('and', 'subjectivity') is 0.00847457627118644\n",
            "probability of ('subjectivity', 'analysis') is 1.0\n",
            "probability of ('text', 'clustering') is 0.09090909090909091\n",
            "probability of ('clustering', 'In') is 1.0\n",
            "probability of ('In', 'Natural') is 0.058823529411764705\n",
            "probability of (',', 'research') is 0.006578947368421052\n",
            "probability of ('research', 'result') is 0.25\n",
            "probability of ('result', 'from') is 0.125\n",
            "probability of ('from', 'software') is 0.05555555555555555\n",
            "probability of ('software', 'engineering') is 0.25\n",
            "probability of ('engineering', 'and') is 0.5\n",
            "probability of ('and', 'software') is 0.01694915254237288\n",
            "probability of ('software', 'technology') is 0.25\n",
            "probability of ('technology', 'have') is 0.3333333333333333\n",
            "probability of ('have', 'often') is 0.08333333333333333\n",
            "probability of ('often', 'been') is 0.5\n",
            "probability of ('been', 'neglected') is 0.07692307692307693\n",
            "probability of ('neglected', '.') is 1.0\n",
            "probability of ('.', 'of') is 0.00684931506849315\n",
            "probability of ('of', 'kernelized') is 0.011363636363636364\n",
            "probability of ('kernelized', 'sorting') is 0.5\n",
            "probability of ('sorting', 'to') is 1.0\n",
            "probability of ('to', 'increase') is 0.013513513513513514\n",
            "probability of ('increase', 'it') is 1.0\n",
            "probability of ('it', 'robustness') is 0.07692307692307693\n",
            "probability of ('robustness', 'and') is 1.0\n",
            "probability of ('and', 'performance') is 0.00847457627118644\n",
            "probability of ('performance', 'on') is 0.25\n",
            "probability of ('on', 'several') is 0.043478260869565216\n",
            "probability of ('several', 'Natural') is 0.25\n",
            "probability of ('tasks', ':') is 0.1111111111111111\n",
            "probability of (':', 'document') is 0.05263157894736842\n",
            "probability of ('document', 'matching') is 0.1111111111111111\n",
            "probability of ('matching', 'from') is 0.5\n",
            "probability of ('from', 'parallel') is 0.05555555555555555\n",
            "probability of ('parallel', 'and') is 0.5\n",
            "probability of ('and', 'comparable') is 0.00847457627118644\n",
            "probability of ('comparable', 'corpora') is 1.0\n",
            "probability of ('corpora', ',') is 1.0\n",
            "probability of ('machine', 'transliteration') is 0.07142857142857142\n",
            "probability of ('transliteration', 'and') is 1.0\n",
            "probability of ('and', 'even') is 0.00847457627118644\n",
            "probability of ('even', 'image') is 0.5\n",
            "probability of ('image', 'processing') is 0.3333333333333333\n",
            "probability of ('.', 'Empirically') is 0.00684931506849315\n",
            "probability of ('Empirically', 'we') is 1.0\n",
            "probability of ('we', 'show') is 0.058823529411764705\n",
            "probability of ('show', 'that') is 1.0\n",
            "probability of ('that', ',') is 0.05263157894736842\n",
            "probability of (',', 'on') is 0.006578947368421052\n",
            "probability of ('on', 'these') is 0.043478260869565216\n",
            "probability of ('these', 'tasks') is 0.2\n",
            "probability of ('a', 'semi-supervised') is 0.008547008547008548\n",
            "probability of ('semi-supervised', 'variant') is 1.0\n",
            "probability of ('variant', 'of') is 1.0\n",
            "probability of ('kernelized', 'will') is 0.5\n",
            "probability of ('be', 'structured') is 0.038461538461538464\n",
            "probability of ('structured', '.') is 0.5\n",
            "probability of ('word', 'of') is 0.07692307692307693\n",
            "probability of ('of', 'statistical') is 0.005681818181818182\n",
            "probability of ('statistical', 'natural') is 0.14285714285714285\n",
            "probability of ('processing', ',') is 0.0625\n",
            "probability of ('we', 'need') is 0.058823529411764705\n",
            "probability of ('need', 'a') is 0.16666666666666666\n",
            "probability of ('a', 'sophisticated') is 0.008547008547008548\n",
            "probability of ('sophisticated', 'statistical') is 0.5\n",
            "probability of ('statistical', 'model') is 0.14285714285714285\n",
            "probability of ('basic', 'elements') is 0.25\n",
            "probability of ('elements', ',') is 1.0\n",
            "probability of ('a', 'word') is 0.017094017094017096\n",
            "probability of ('word', 'or') is 0.07692307692307693\n",
            "probability of ('or', 'phrases') is 0.058823529411764705\n",
            "probability of ('phrases', ',') is 1.0\n",
            "probability of ('be', 'combined') is 0.038461538461538464\n",
            "probability of ('combined', 'with') is 1.0\n",
            "probability of ('the', 'structural') is 0.005434782608695652\n",
            "probability of ('structural', 'modeling') is 1.0\n",
            "probability of ('modeling', 'such') is 0.3333333333333333\n",
            "probability of ('a', 'syntactic') is 0.008547008547008548\n",
            "probability of ('parsing', 'or') is 0.25\n",
            "probability of ('or', 'dependency') is 0.058823529411764705\n",
            "probability of ('dependency', 'analysis') is 0.5\n",
            "probability of ('analysis', '.') is 0.18181818181818182\n",
            "probability of ('.', 'Since') is 0.00684931506849315\n",
            "probability of ('basic', 'property') is 0.25\n",
            "probability of ('property', 'of') is 1.0\n",
            "probability of ('of', 'these') is 0.005681818181818182\n",
            "probability of ('these', 'element') is 0.2\n",
            "probability of ('element', 'In') is 0.5\n",
            "probability of ('a', 'framework') is 0.008547008547008548\n",
            "probability of ('framework', 'for') is 0.5\n",
            "probability of ('for', 'developing') is 0.017857142857142856\n",
            "probability of ('developing', 'probabilistic') is 1.0\n",
            "probability of ('probabilistic', 'classifier') is 0.5\n",
            "probability of ('classifier', 'in') is 1.0\n",
            "probability of ('Our', 'focus') is 0.5\n",
            "probability of ('focus', 'is') is 0.1\n",
            "probability of ('is', 'on') is 0.018518518518518517\n",
            "probability of ('on', 'formulating') is 0.043478260869565216\n",
            "probability of ('formulating', 'model') is 1.0\n",
            "probability of ('model', 'that') is 0.1\n",
            "probability of ('that', 'capture') is 0.02631578947368421\n",
            "probability of ('the', 'most') is 0.005434782608695652\n",
            "probability of ('most', 'important') is 0.5\n",
            "probability of ('important', 'interdependency') is 0.3333333333333333\n",
            "probability of ('interdependency', 'among') is 1.0\n",
            "probability of ('among', 'features') is 0.3333333333333333\n",
            "probability of ('features', ',') is 1.0\n",
            "probability of ('avoid', 'overfitting') is 0.3333333333333333\n",
            "probability of ('overfitting', 'the') is 1.0\n",
            "probability of ('the', 'data') is 0.010869565217391304\n",
            "probability of ('data', 'while') is 0.16666666666666666\n",
            "probability of ('while', 'also') is 0.3333333333333333\n",
            "probability of ('also', 'characterizing') is 0.2\n",
            "probability of ('characterizing', 'the') is 1.0\n",
            "probability of ('data', 'well') is 0.16666666666666666\n",
            "probability of ('well', '.') is 1.0\n",
            "probability of ('The', 'class') is 0.038461538461538464\n",
            "probability of ('class', 'Many') is 0.25\n",
            "probability of ('Many', 'Natural') is 0.3333333333333333\n",
            "probability of ('technique', 'have') is 0.08333333333333333\n",
            "probability of ('in', 'Information') is 0.013333333333333334\n",
            "probability of ('Information', 'Retrieval') is 0.3333333333333333\n",
            "probability of ('Retrieval', '.') is 1.0\n",
            "probability of ('The', 'result') is 0.038461538461538464\n",
            "probability of ('result', 'are') is 0.125\n",
            "probability of ('not', 'encouraging') is 0.07142857142857142\n",
            "probability of ('encouraging', '.') is 1.0\n",
            "probability of ('.', 'Simple') is 0.00684931506849315\n",
            "probability of ('Simple', 'method') is 1.0\n",
            "probability of ('method', '(') is 0.14285714285714285\n",
            "probability of ('(', 'stopwording') is 0.0196078431372549\n",
            "probability of ('stopwording', ',') is 1.0\n",
            "probability of (',', 'porter-style') is 0.006578947368421052\n",
            "probability of ('porter-style', 'stemming') is 1.0\n",
            "probability of ('stemming', ',') is 1.0\n",
            "probability of (',', 'etc') is 0.006578947368421052\n",
            "probability of ('etc', '.') is 1.0\n",
            "probability of ('.', ')') is 0.00684931506849315\n",
            "probability of (')', 'usually') is 0.02127659574468085\n",
            "probability of ('usually', 'yield') is 1.0\n",
            "probability of ('yield', 'significant') is 1.0\n",
            "probability of ('significant', 'improvements') is 0.3333333333333333\n",
            "probability of ('improvements', ',') is 1.0\n",
            "probability of (',', 'while') is 0.006578947368421052\n",
            "probability of ('while', 'higher-level') is 0.3333333333333333\n",
            "probability of ('higher-level', 'processing') is 1.0\n",
            "probability of ('(', 'chunking') is 0.0196078431372549\n",
            "probability of (',', 'parsing') is 0.006578947368421052\n",
            "probability of ('disambiguation', 'Abstract-') is 0.5\n",
            "probability of ('Abstract-', 'This') is 1.0\n",
            "probability of ('paper', 'explains') is 0.047619047619047616\n",
            "probability of ('explains', 'the') is 1.0\n",
            "probability of ('the', 'information') is 0.021739130434782608\n",
            "probability of ('retrieval', 'using') is 0.1111111111111111\n",
            "probability of ('for', 'Malayalam') is 0.017857142857142856\n",
            "probability of ('Malayalam', 'language') is 1.0\n",
            "probability of ('in', 'these') is 0.013333333333333334\n",
            "probability of ('these', 'basic') is 0.2\n",
            "probability of ('basic', 'in') is 0.25\n",
            "probability of ('the', 'state') is 0.016304347826086956\n",
            "probability of ('art', 'plan') is 0.5\n",
            "probability of ('plan', 'recognition') is 1.0\n",
            "probability of ('recognition', 'systems') is 0.16666666666666666\n",
            "probability of ('paper', 'will') is 0.09523809523809523\n",
            "probability of ('will', 'outline') is 0.2222222222222222\n",
            "probability of ('outline', 'the') is 1.0\n",
            "probability of ('the', 'relation') is 0.010869565217391304\n",
            "probability of ('relation', 'between') is 1.0\n",
            "probability of ('between', 'natural') is 0.25\n",
            "probability of ('and', 'plan') is 0.01694915254237288\n",
            "probability of ('recognition', '(') is 0.16666666666666666\n",
            "probability of ('(', 'PR') is 0.0392156862745098\n",
            "probability of ('PR', ')') is 0.5\n",
            "probability of (',', 'argue') is 0.013157894736842105\n",
            "probability of ('that', 'each') is 0.05263157894736842\n",
            "probability of ('each', 'of') is 1.0\n",
            "probability of ('of', 'them') is 0.011363636363636364\n",
            "probability of ('them', 'can') is 0.5\n",
            "probability of ('can', 'effectively') is 0.125\n",
            "probability of ('effectively', 'inform') is 0.6666666666666666\n",
            "probability of ('inform', 'the') is 1.0\n",
            "probability of ('the', 'other') is 0.010869565217391304\n",
            "probability of ('other', ',') is 1.0\n",
            "probability of ('then', 'focus') is 0.4\n",
            "probability of ('on', 'key') is 0.08695652173913043\n",
            "probability of ('key', 'recent') is 1.0\n",
            "probability of ('result', 'in') is 0.25\n",
            "probability of ('in', 'NLP') is 0.02666666666666667\n",
            "probability of ('NLP', 'and') is 0.045454545454545456\n",
            "probability of ('and', 'argue') is 0.01694915254237288\n",
            "probability of ('argue', 'for') is 0.3333333333333333\n",
            "probability of ('for', 'their') is 0.03571428571428571\n",
            "probability of ('their', 'applicability') is 0.5\n",
            "probability of ('applicability', 'to') is 1.0\n",
            "probability of ('to', 'PR') is 0.02702702702702703\n",
            "probability of ('PR', '.') is 0.5\n",
            "probability of ('1', 'in') is 0.1111111111111111\n",
            "probability of ('1', 'Information') is 0.1111111111111111\n",
            "probability of ('Information', 'retrieval') is 0.3333333333333333\n",
            "probability of ('retrieval', 'is') is 0.1111111111111111\n",
            "probability of ('of', 'finding') is 0.005681818181818182\n",
            "probability of ('finding', 'the') is 1.0\n",
            "probability of ('the', 'document') is 0.005434782608695652\n",
            "probability of ('document', 'in') is 0.1111111111111111\n",
            "probability of ('a', 'document') is 0.008547008547008548\n",
            "probability of ('document', 'collection') is 0.1111111111111111\n",
            "probability of ('collection', 'that') is 0.5\n",
            "probability of ('that', 'satisfies') is 0.02631578947368421\n",
            "probability of ('satisfies', 'the') is 1.0\n",
            "probability of ('information', 'need') is 0.05263157894736842\n",
            "probability of ('need', 'of') is 0.16666666666666666\n",
            "probability of ('the', 'user') is 0.005434782608695652\n",
            "probability of ('user', '.') is 0.5\n",
            "probability of ('The', 'document') is 0.07692307692307693\n",
            "probability of ('document', 'are') is 0.1111111111111111\n",
            "probability of ('are', 'natural') is 0.038461538461538464\n",
            "probability of ('language', 'constructs') is 0.01098901098901099\n",
            "probability of ('constructs', ',') is 1.0\n",
            "probability of ('the', 'motivation') is 0.005434782608695652\n",
            "probability of ('motivation', 'of') is 1.0\n",
            "probability of ('this', 'work') is 0.045454545454545456\n",
            "probability of ('work', 'is') is 0.125\n",
            "probability of ('to', 'investigate') is 0.013513513513513514\n",
            "probability of ('investigate', 'how') is 1.0\n",
            "probability of ('how', 'natural') is 0.2\n",
            "probability of ('processing', 'can') is 0.015625\n",
            "probability of ('be', 'used') is 0.11538461538461539\n",
            "probability of ('improve', 'of') is 0.5\n",
            "probability of ('of', 'logic') is 0.005681818181818182\n",
            "probability of ('logic', 'programming') is 0.8\n",
            "probability of ('programming', 'within') is 0.2\n",
            "probability of ('within', 'both') is 0.2\n",
            "probability of ('both', 'natural') is 0.2\n",
            "probability of ('language', 'research') is 0.01098901098901099\n",
            "probability of ('we', 'point') is 0.058823529411764705\n",
            "probability of ('out', 'opportunity') is 0.3333333333333333\n",
            "probability of ('opportunity', 'for') is 1.0\n",
            "probability of ('for', 'induction') is 0.017857142857142856\n",
            "probability of ('induction', 'of') is 1.0\n",
            "probability of ('knowledge', 'within') is 0.08333333333333333\n",
            "probability of ('within', 'logic') is 0.2\n",
            "probability of ('logic', '(') is 0.2\n",
            "probability of ('(', 'programming') is 0.0196078431372549\n",
            "probability of ('programming', ')') is 0.2\n",
            "probability of ('.', 'Keywords') is 0.00684931506849315\n",
            "probability of ('Keywords', ':') is 1.0\n",
            "probability of (':', 'inductive') is 0.05263157894736842\n",
            "probability of ('inductive', 'logic') is 1.0\n",
            "probability of ('programming', ',') is 0.4\n",
            "probability of (',', 'natural') is 0.006578947368421052\n",
            "probability of (',', 'logic') is 0.006578947368421052\n",
            "probability of ('1', 'Introduction') is 0.1111111111111111\n",
            "probability of ('Introduction', 'There') is 0.2\n",
            "probability of ('There', 'is') is 1.0\n",
            "probability of ('a', 'What') is 0.008547008547008548\n",
            "probability of ('What', 'is') is 1.0\n",
            "probability of ('a', 'statistical') is 0.017094017094017096\n",
            "probability of ('statistical', 'method') is 0.14285714285714285\n",
            "probability of ('method', 'and') is 0.14285714285714285\n",
            "probability of ('and', 'how') is 0.00847457627118644\n",
            "probability of ('how', 'can') is 0.2\n",
            "probability of ('can', 'it') is 0.0625\n",
            "probability of ('it', 'be') is 0.07692307692307693\n",
            "probability of (')', '?') is 0.02127659574468085\n",
            "probability of ('?', 'In') is 0.2\n",
            "probability of ('we', 'start') is 0.058823529411764705\n",
            "probability of ('start', 'from') is 1.0\n",
            "probability of ('a', 'definition') is 0.008547008547008548\n",
            "probability of ('definition', 'of') is 1.0\n",
            "probability of ('NLP', 'a') is 0.022727272727272728\n",
            "probability of ('a', 'concerned') is 0.008547008547008548\n",
            "probability of ('design', 'and') is 0.5\n",
            "probability of ('and', 'implementation') is 0.00847457627118644\n",
            "probability of ('implementation', 'of') is 0.5\n",
            "probability of ('of', 'effective') is 0.005681818181818182\n",
            "probability of ('effective', 'natural') is 0.25\n",
            "probability of ('language', 'input') is 0.01098901098901099\n",
            "probability of ('input', 'and') is 1.0\n",
            "probability of ('and', 'output') is 0.00847457627118644\n",
            "probability of ('output', 'component') is 0.3333333333333333\n",
            "probability of ('component', 'for') is 1.0\n",
            "probability of ('for', 'computational') is 0.017857142857142856\n",
            "probability of ('computational', 'systems') is 0.2\n",
            "probability of ('We', 'distinguish') is 0.041666666666666664\n",
            "probability of ('distinguish', 'three') is 1.0\n",
            "probability of ('three', 'In') is 0.3333333333333333\n",
            "probability of ('this', 'report') is 0.045454545454545456\n",
            "probability of ('report', ',') is 0.3333333333333333\n",
            "probability of (',', 'some') is 0.013157894736842105\n",
            "probability of ('some', 'collaborative') is 0.2\n",
            "probability of ('collaborative', 'work') is 1.0\n",
            "probability of ('work', 'between') is 0.125\n",
            "probability of ('between', 'the') is 0.125\n",
            "probability of ('of', 'Machine') is 0.005681818181818182\n",
            "probability of ('Machine', 'Learning') is 0.3333333333333333\n",
            "probability of ('Learning', '(') is 1.0\n",
            "probability of ('is', 'presented') is 0.018518518518518517\n",
            "probability of ('presented', '.') is 0.5\n",
            "probability of ('document', 'is') is 0.1111111111111111\n",
            "probability of ('is', 'structured') is 0.018518518518518517\n",
            "probability of ('structured', 'in') is 0.5\n",
            "probability of ('in', 'two') is 0.02666666666666667\n",
            "probability of ('two', 'parts') is 0.3333333333333333\n",
            "probability of ('parts', '.') is 1.0\n",
            "probability of ('The', 'first') is 0.038461538461538464\n",
            "probability of ('first', 'part') is 0.3333333333333333\n",
            "probability of ('part', 'includes') is 0.3333333333333333\n",
            "probability of ('includes', 'a') is 0.6666666666666666\n",
            "probability of ('a', 'superficial') is 0.008547008547008548\n",
            "probability of ('superficial', 'but') is 1.0\n",
            "probability of ('but', 'comprehensive') is 0.16666666666666666\n",
            "probability of ('comprehensive', 'survey') is 1.0\n",
            "probability of ('survey', 'covering') is 0.5\n",
            "probability of ('covering', 'the') is 0.5\n",
            "probability of ('state', '--') is 0.2\n",
            "probability of ('--', 'of') is 0.07142857142857142\n",
            "probability of ('of', '--') is 0.005681818181818182\n",
            "probability of ('--', 'the') is 0.07142857142857142\n",
            "probability of ('the', '--') is 0.005434782608695652\n",
            "probability of ('--', 'art') is 0.07142857142857142\n",
            "probability of ('art', 'of') is 0.25\n",
            "probability of ('of', 'machine') is 0.011363636363636364\n",
            "probability of ('learning', 'Abstract') is 0.05\n",
            "probability of ('Abstract', '.') is 0.07692307692307693\n",
            "probability of ('This', 'thesis') is 0.034482758620689655\n",
            "probability of ('thesis', 'examines') is 0.5\n",
            "probability of ('examines', 'the') is 1.0\n",
            "probability of ('technique', 'in') is 0.08333333333333333\n",
            "probability of ('in', 'various') is 0.02666666666666667\n",
            "probability of ('various', 'task') is 0.125\n",
            "probability of ('task', 'of') is 0.18181818181818182\n",
            "probability of (',', 'mainly') is 0.006578947368421052\n",
            "probability of ('mainly', 'for') is 1.0\n",
            "probability of ('the', 'task') is 0.005434782608695652\n",
            "probability of ('of', 'information') is 0.017045454545454544\n",
            "probability of ('information', 'extraction') is 0.10526315789473684\n",
            "probability of ('extraction', 'from') is 0.25\n",
            "probability of ('from', 'texts') is 0.05555555555555555\n",
            "probability of ('texts', '.') is 1.0\n",
            "probability of ('The', 'objective') is 0.038461538461538464\n",
            "probability of ('objective', 'are') is 1.0\n",
            "probability of ('the', 'improvement') is 0.010869565217391304\n",
            "probability of ('improvement', 'of') is 0.5\n",
            "probability of ('of', 'adaptability') is 0.005681818181818182\n",
            "probability of ('adaptability', 'of') is 1.0\n",
            "probability of ('extraction', 'system') is 0.25\n",
            "probability of ('system', 'to') is 0.10526315789473684\n",
            "probability of ('to', 'new') is 0.013513513513513514\n",
            "probability of ('new', 'thematic') is 0.16666666666666666\n",
            "probability of ('thematic', 'do-mains') is 1.0\n",
            "probability of ('do-mains', '(') is 1.0\n",
            "probability of ('(', 'or') is 0.0196078431372549\n",
            "probability of ('or', 'even') is 0.058823529411764705\n",
            "probability of ('even', 'This') is 0.5\n",
            "probability of ('chapter', 'examines') is 0.5\n",
            "probability of ('to', 'computerassisted') is 0.02702702702702703\n",
            "probability of ('computerassisted', 'language') is 1.0\n",
            "probability of ('language', 'learning') is 0.02197802197802198\n",
            "probability of ('learning', 'including') is 0.1\n",
            "probability of ('including', 'the') is 0.25\n",
            "probability of ('the', 'history') is 0.010869565217391304\n",
            "probability of ('history', 'of') is 0.5\n",
            "probability of ('of', 'work') is 0.011363636363636364\n",
            "probability of ('work', 'in') is 0.25\n",
            "probability of ('in', 'this') is 0.02666666666666667\n",
            "probability of ('this', 'field') is 0.09090909090909091\n",
            "probability of ('field', 'over') is 0.25\n",
            "probability of ('last', 'thirtyfive') is 0.5\n",
            "probability of ('thirtyfive', 'year') is 1.0\n",
            "probability of ('year', 'but') is 0.5\n",
            "probability of ('but', 'with') is 0.3333333333333333\n",
            "probability of ('on', 'current') is 0.08695652173913043\n",
            "probability of ('current', 'development') is 0.2857142857142857\n",
            "probability of ('development', 'and') is 0.2727272727272727\n",
            "probability of ('and', 'opportunities') is 0.01694915254237288\n",
            "probability of ('opportunities', '.') is 1.0\n",
            "probability of ('.', '36.1') is 0.00684931506849315\n",
            "probability of ('36.1', 'Traditional') is 1.0\n",
            "probability of ('Traditional', 'approach') is 1.0\n",
            "probability of ('approach', 'tointerpretation') is 0.058823529411764705\n",
            "probability of ('tointerpretation', 'in') is 1.0\n",
            "probability of ('processing', 'typically') is 0.015625\n",
            "probability of ('typically', 'fall') is 0.5\n",
            "probability of ('fall', 'into') is 1.0\n",
            "probability of ('into', 'one') is 0.16666666666666666\n",
            "probability of ('one', 'of') is 0.2\n",
            "probability of ('of', 'three') is 0.005681818181818182\n",
            "probability of ('three', 'classes') is 0.3333333333333333\n",
            "probability of ('classes', ':') is 1.0\n",
            "probability of (':', 'syntax-driven') is 0.05263157894736842\n",
            "probability of ('syntax-driven', ',') is 1.0\n",
            "probability of (',', 'semantics-driven') is 0.006578947368421052\n",
            "probability of ('semantics-driven', ',') is 1.0\n",
            "probability of ('or', 'frame/task') is 0.058823529411764705\n",
            "probability of ('frame/task', 'based') is 1.0\n",
            "probability of ('based', '.') is 0.125\n",
            "probability of ('.', 'Syntax-driven') is 0.00684931506849315\n",
            "probability of ('Syntax-driven', 'approach') is 1.0\n",
            "probability of ('approach', 'use') is 0.058823529411764705\n",
            "probability of ('use', 'a') is 0.08333333333333333\n",
            "probability of ('a', 'domain-independent') is 0.008547008547008548\n",
            "probability of ('domain-independent', 'grammar') is 1.0\n",
            "probability of ('grammar', 'to') is 0.5\n",
            "probability of ('the', 'interpretation') is 0.005434782608695652\n",
            "probability of ('interpretation', 'process') is 0.3333333333333333\n",
            "probability of ('process', 'and') is 0.09090909090909091\n",
            "probability of ('and', 'produce') is 0.00847457627118644\n",
            "probability of ('produce', 'a') is 1.0\n",
            "probability of ('a', 'global') is 0.017094017094017096\n",
            "probability of ('global', 'parse') is 0.5\n",
            "probability of ('parse', 'Natural') is 1.0\n",
            "probability of ('a', 'very') is 0.008547008547008548\n",
            "probability of ('very', 'large') is 0.5\n",
            "probability of ('large', 'and') is 0.5\n",
            "probability of ('and', 'diverse') is 0.00847457627118644\n",
            "probability of ('diverse', 'subtopic') is 1.0\n",
            "probability of ('subtopic', 'of') is 1.0\n",
            "probability of ('of', 'artificial') is 0.011363636363636364\n",
            "probability of ('artificial', 'intelligence') is 1.0\n",
            "probability of ('intelligence', '.') is 0.5\n",
            "probability of ('.', 'As') is 0.00684931506849315\n",
            "probability of ('As', 'a') is 1.0\n",
            "probability of ('a', 'result') is 0.008547008547008548\n",
            "probability of ('result', ',') is 0.125\n",
            "probability of (',', 'NLP') is 0.006578947368421052\n",
            "probability of ('NLP', 'itself') is 0.022727272727272728\n",
            "probability of ('itself', 'ha') is 1.0\n",
            "probability of ('ha', 'many') is 0.08333333333333333\n",
            "probability of ('many', 'subtopics') is 0.5\n",
            "probability of ('subtopics', 'including') is 1.0\n",
            "probability of ('including', 'optical') is 0.125\n",
            "probability of ('optical', 'character') is 1.0\n",
            "probability of ('character', 'recognition') is 1.0\n",
            "probability of ('text', 'to') is 0.09090909090909091\n",
            "probability of ('to', 'speech') is 0.013513513513513514\n",
            "probability of ('speech', 'translators') is 0.125\n",
            "probability of ('translators', ',') is 1.0\n",
            "probability of (',', 'foreign') is 0.006578947368421052\n",
            "probability of ('foreign', 'language') is 1.0\n",
            "probability of ('language', 'reading') is 0.01098901098901099\n",
            "probability of ('reading', 'and') is 1.0\n",
            "probability of ('and', 'writing') is 0.00847457627118644\n",
            "probability of ('writing', 'aids') is 1.0\n",
            "probability of ('aids', ',') is 1.0\n",
            "probability of ('and', 'speech') is 0.00847457627118644\n",
            "probability of ('recognition', 'Probabilistic') is 0.08333333333333333\n",
            "probability of ('Probabilistic', 'finite-state') is 1.0\n",
            "probability of ('finite-state', 'string') is 1.0\n",
            "probability of ('string', 'transducer') is 1.0\n",
            "probability of ('transducer', '(') is 1.0\n",
            "probability of ('(', 'FSTs') is 0.0196078431372549\n",
            "probability of ('FSTs', ')') is 0.5\n",
            "probability of (')', 'are') is 0.0425531914893617\n",
            "probability of ('are', 'extremely') is 0.038461538461538464\n",
            "probability of ('extremely', 'popular') is 1.0\n",
            "probability of ('popular', 'in') is 1.0\n",
            "probability of (',', 'due') is 0.006578947368421052\n",
            "probability of ('due', 'to') is 1.0\n",
            "probability of ('to', 'powerful') is 0.013513513513513514\n",
            "probability of ('powerful', 'generic') is 0.5\n",
            "probability of ('generic', 'method') is 0.3333333333333333\n",
            "probability of ('for', 'applying') is 0.017857142857142856\n",
            "probability of ('applying', ',') is 0.5\n",
            "probability of (',', 'composing') is 0.006578947368421052\n",
            "probability of ('composing', ',') is 1.0\n",
            "probability of ('learning', 'them') is 0.05\n",
            "probability of ('them', '.') is 0.25\n",
            "probability of ('.', 'Unfortunately') is 0.00684931506849315\n",
            "probability of ('Unfortunately', ',') is 1.0\n",
            "probability of (',', 'FSTs') is 0.006578947368421052\n",
            "probability of ('FSTs', 'are') is 0.5\n",
            "probability of ('not', 'a') is 0.07142857142857142\n",
            "probability of ('a', 'good') is 0.008547008547008548\n",
            "probability of ('good', 'fit') is 0.5\n",
            "probability of ('fit', 'for') is 1.0\n",
            "probability of ('for', 'much') is 0.017857142857142856\n",
            "probability of ('much', 'of') is 0.5\n",
            "probability of ('current', 'work') is 0.14285714285714285\n",
            "probability of ('work', 'on') is 0.125\n",
            "probability of ('on', 'probabilistic') is 0.043478260869565216\n",
            "probability of ('probabilistic', 'modeling') is 0.5\n",
            "probability of ('modeling', 'for') is 0.3333333333333333\n",
            "probability of ('for', 'machine') is 0.017857142857142856\n",
            "probability of ('machine', 'ABSTRACT') is 0.07142857142857142\n",
            "probability of ('ABSTRACT', '.') is 0.25\n",
            "probability of ('this', 'special') is 0.045454545454545456\n",
            "probability of ('special', 'issue') is 0.5\n",
            "probability of ('of', 'TAL') is 0.005681818181818182\n",
            "probability of ('TAL', ',') is 1.0\n",
            "probability of ('we', 'look') is 0.058823529411764705\n",
            "probability of ('look', 'at') is 1.0\n",
            "probability of ('the', 'fundamental') is 0.005434782608695652\n",
            "probability of ('fundamental', 'principle') is 0.5\n",
            "probability of ('principle', 'underlying') is 0.5\n",
            "probability of ('underlying', 'evaluation') is 1.0\n",
            "probability of ('We', 'adopt') is 0.041666666666666664\n",
            "probability of ('adopt', 'a') is 1.0\n",
            "probability of ('global', 'point') is 0.5\n",
            "probability of ('point', 'of') is 0.25\n",
            "probability of ('of', 'view') is 0.005681818181818182\n",
            "probability of ('view', 'that') is 1.0\n",
            "probability of ('that', 'go') is 0.02631578947368421\n",
            "probability of ('go', 'beyond') is 0.5\n",
            "probability of ('beyond', 'the') is 1.0\n",
            "probability of ('the', 'horizon') is 0.005434782608695652\n",
            "probability of ('horizon', 'of') is 1.0\n",
            "probability of ('a', 'single') is 0.02564102564102564\n",
            "probability of ('single', 'evaluation') is 0.3333333333333333\n",
            "probability of ('evaluation', 'campaign') is 0.125\n",
            "probability of ('campaign', 'or') is 1.0\n",
            "probability of ('a', 'particular') is 0.008547008547008548\n",
            "probability of ('particular', 'protocol') is 0.2\n",
            "probability of ('protocol', '.') is 1.0\n",
            "probability of ('.', 'After') is 0.00684931506849315\n",
            "probability of ('After', 'a') is 0.5\n",
            "probability of ('brief', 'review') is 0.3333333333333333\n",
            "probability of ('review', 'of') is 0.25\n",
            "probability of ('history', 'and') is 0.25\n",
            "probability of ('and', 'terminology') is 0.00847457627118644\n",
            "probability of ('terminology', 'Abstract') is 1.0\n",
            "probability of ('found', 'Natural') is 0.125\n",
            "probability of ('system', '(') is 0.05263157894736842\n",
            "probability of ('that', 'extract') is 0.02631578947368421\n",
            "probability of ('extract', 'clinical') is 0.25\n",
            "probability of ('clinical', 'information') is 1.0\n",
            "probability of ('from', 'textual') is 0.05555555555555555\n",
            "probability of ('textual', 'report') is 1.0\n",
            "probability of ('report', 'were') is 0.3333333333333333\n",
            "probability of ('were', 'shown') is 1.0\n",
            "probability of ('shown', 'to') is 0.5\n",
            "probability of ('be', 'effective') is 0.038461538461538464\n",
            "probability of ('effective', 'for') is 0.25\n",
            "probability of ('for', 'limited') is 0.017857142857142856\n",
            "probability of ('limited', 'domain') is 0.25\n",
            "probability of ('domain', 'and') is 0.4\n",
            "probability of ('for', 'particular') is 0.017857142857142856\n",
            "probability of ('particular', 'applications') is 0.2\n",
            "probability of ('.', 'Because') is 0.00684931506849315\n",
            "probability of ('Because', 'an') is 1.0\n",
            "probability of ('an', 'NLP') is 0.05\n",
            "probability of ('NLP', 'system') is 0.022727272727272728\n",
            "probability of ('system', 'typically') is 0.05263157894736842\n",
            "probability of ('typically', 'requires') is 0.5\n",
            "probability of ('requires', 'substantial') is 1.0\n",
            "probability of ('substantial', 'resource') is 1.0\n",
            "probability of ('resource', 'to') is 0.3333333333333333\n",
            "probability of ('to', 'develop') is 0.013513513513513514\n",
            "probability of ('develop', ',') is 1.0\n",
            "probability of ('it', 'is') is 0.15384615384615385\n",
            "probability of ('is', 'beneficial') is 0.018518518518518517\n",
            "probability of ('beneficial', 'if') is 1.0\n",
            "probability of ('if', 'it') is 1.0\n",
            "probability of ('is', 'designed') is 0.018518518518518517\n",
            "probability of ('designed', 'to') is 1.0\n",
            "probability of ('be', 'easily') is 0.038461538461538464\n",
            "probability of ('easily', 'fact') is 1.0\n",
            "probability of ('fact', 'form') is 0.5\n",
            "probability of ('a', 'link') is 0.008547008547008548\n",
            "probability of ('link', 'between') is 1.0\n",
            "probability of ('between', 'IE') is 0.125\n",
            "probability of ('IE', ',') is 1.0\n",
            "probability of ('a', 'recent') is 0.008547008547008548\n",
            "probability of ('Processing', ',') is 0.1\n",
            "probability of ('and', 'logic') is 0.00847457627118644\n",
            "probability of ('programming', 'with') is 0.2\n",
            "probability of ('with', 'Prolog') is 0.047619047619047616\n",
            "probability of ('Prolog', '.') is 0.5\n",
            "probability of ('1', 'We') is 0.1111111111111111\n",
            "probability of ('single', 'convolutional') is 0.3333333333333333\n",
            "probability of ('convolutional', 'neural') is 1.0\n",
            "probability of ('architecture', 'that') is 0.2\n",
            "probability of (',', 'given') is 0.006578947368421052\n",
            "probability of ('given', 'a') is 0.25\n",
            "probability of ('a', 'sentence') is 0.008547008547008548\n",
            "probability of ('sentence', ',') is 0.3333333333333333\n",
            "probability of (',', 'output') is 0.006578947368421052\n",
            "probability of ('output', 'a') is 0.3333333333333333\n",
            "probability of ('a', 'host') is 0.008547008547008548\n",
            "probability of ('host', 'of') is 1.0\n",
            "probability of ('processing', 'predictions') is 0.015625\n",
            "probability of ('predictions', ':') is 1.0\n",
            "probability of (':', 'part-of-speech') is 0.05263157894736842\n",
            "probability of ('part-of-speech', 'tags') is 0.5\n",
            "probability of ('tags', ',') is 1.0\n",
            "probability of (',', 'chunks') is 0.006578947368421052\n",
            "probability of ('chunks', ',') is 1.0\n",
            "probability of ('entity', 'tags') is 0.5\n",
            "probability of ('semantic', 'roles') is 0.125\n",
            "probability of ('roles', ',') is 1.0\n",
            "probability of (',', 'semantically') is 0.006578947368421052\n",
            "probability of ('semantically', 'similar') is 0.5\n",
            "probability of ('similar', 'word') is 0.5\n",
            "probability of ('the', 'likelihood') is 0.005434782608695652\n",
            "probability of ('likelihood', 'that') is 1.0\n",
            "probability of ('that', 'the') is 0.02631578947368421\n",
            "probability of ('the', 'sentence') is 0.005434782608695652\n",
            "probability of ('sentence', 'make') is 0.3333333333333333\n",
            "probability of ('make', 'sense') is 0.25\n",
            "probability of ('sense', '(') is 0.2\n",
            "probability of ('(', 'grammatically') is 0.0196078431372549\n",
            "probability of ('grammatically', 'We') is 1.0\n",
            "probability of ('We', 'developed') is 0.041666666666666664\n",
            "probability of ('a', 'prototype') is 0.008547008547008548\n",
            "probability of ('prototype', 'information') is 1.0\n",
            "probability of ('system', 'which') is 0.10526315789473684\n",
            "probability of ('which', 'us') is 0.1111111111111111\n",
            "probability of ('us', 'advanced') is 0.5\n",
            "probability of ('advanced', 'natural') is 1.0\n",
            "probability of ('to', 'enhance') is 0.013513513513513514\n",
            "probability of ('enhance', 'the') is 1.0\n",
            "probability of ('the', 'effectiveness') is 0.005434782608695652\n",
            "probability of ('effectiveness', 'of') is 1.0\n",
            "probability of ('of', 'traditional') is 0.005681818181818182\n",
            "probability of ('traditional', 'key-word') is 0.5\n",
            "probability of ('key-word', 'based') is 1.0\n",
            "probability of ('based', 'document') is 0.125\n",
            "probability of ('document', 'retrieval') is 0.1111111111111111\n",
            "probability of ('retrieval', '.') is 0.1111111111111111\n",
            "probability of ('The', 'backbone') is 0.038461538461538464\n",
            "probability of ('backbone', 'of') is 1.0\n",
            "probability of ('of', 'our') is 0.011363636363636364\n",
            "probability of ('our', 'system') is 0.25\n",
            "probability of ('system', 'is') is 0.05263157894736842\n",
            "probability of ('statistical', 'retrieval') is 0.14285714285714285\n",
            "probability of ('retrieval', 'engine') is 0.1111111111111111\n",
            "probability of ('engine', 'which') is 1.0\n",
            "probability of ('performs', 'automated') is 0.5\n",
            "probability of ('automated', 'indexing') is 0.2\n",
            "probability of ('indexing', 'Abstract') is 1.0\n",
            "probability of ('will', 'discus') is 0.1111111111111111\n",
            "probability of ('several', 'issue') is 0.25\n",
            "probability of ('issue', 'and') is 0.2\n",
            "probability of ('and', 'requirement') is 0.00847457627118644\n",
            "probability of ('requirement', 'for') is 0.25\n",
            "probability of ('for', 'enabling') is 0.017857142857142856\n",
            "probability of ('enabling', 'natural') is 1.0\n",
            "probability of ('to', 'become') is 0.013513513513513514\n",
            "probability of ('become', 'context-adaptive') is 1.0\n",
            "probability of ('context-adaptive', '.') is 1.0\n",
            "probability of ('.', 'Given') is 0.00684931506849315\n",
            "probability of ('Given', 'the') is 1.0\n",
            "probability of ('the', 'fact') is 0.005434782608695652\n",
            "probability of ('fact', 'that') is 0.5\n",
            "probability of ('that', 'emerging') is 0.02631578947368421\n",
            "probability of ('emerging', 'system') is 1.0\n",
            "probability of ('system', 'feature') is 0.05263157894736842\n",
            "probability of ('feature', 'speaker') is 0.5\n",
            "probability of ('speaker', 'independent') is 0.5\n",
            "probability of ('independent', 'continuous') is 0.5\n",
            "probability of ('continuous', 'speech') is 1.0\n",
            "probability of ('recognition', 'restricted') is 0.08333333333333333\n",
            "probability of ('restricted', 'to') is 1.0\n",
            "probability of ('to', 'individual') is 0.013513513513513514\n",
            "probability of ('individual', 'domain') is 1.0\n",
            "probability of ('and', 'are') is 0.01694915254237288\n",
            "probability of ('are', 'equipped') is 0.038461538461538464\n",
            "probability of ('equipped', 'with') is 1.0\n",
            "probability of ('with', 'syntactic') is 0.047619047619047616\n",
            "probability of ('syntactic', 'In') is 0.16666666666666666\n",
            "probability of ('In', 'Fall') is 0.058823529411764705\n",
            "probability of ('Fall', '2004') is 1.0\n",
            "probability of ('2004', 'I') is 1.0\n",
            "probability of ('I', 'introduced') is 1.0\n",
            "probability of ('introduced', 'a') is 0.5\n",
            "probability of ('new', 'course') is 0.16666666666666666\n",
            "probability of ('course', 'called') is 1.0\n",
            "probability of ('called', 'Applied') is 0.5\n",
            "probability of ('Applied', 'Natural') is 1.0\n",
            "probability of ('in', 'which') is 0.013333333333333334\n",
            "probability of ('which', 'student') is 0.1111111111111111\n",
            "probability of ('student', 'acquire') is 1.0\n",
            "probability of ('acquire', 'an') is 0.5\n",
            "probability of ('an', 'understanding') is 0.05\n",
            "probability of ('of', 'which') is 0.005681818181818182\n",
            "probability of ('which', 'text') is 0.1111111111111111\n",
            "probability of ('analysis', 'technique') is 0.09090909090909091\n",
            "probability of ('technique', 'are') is 0.08333333333333333\n",
            "probability of ('are', 'currently') is 0.038461538461538464\n",
            "probability of ('currently', 'feasible') is 1.0\n",
            "probability of ('feasible', 'for') is 1.0\n",
            "probability of ('for', 'practical') is 0.017857142857142856\n",
            "probability of ('practical', 'applications') is 0.3333333333333333\n",
            "probability of (':', 'Natural') is 0.05263157894736842\n",
            "probability of ('the', 'study') is 0.005434782608695652\n",
            "probability of ('of', 'mathematical') is 0.005681818181818182\n",
            "probability of ('mathematical', 'and') is 0.5\n",
            "probability of ('and', 'computational') is 0.00847457627118644\n",
            "probability of ('computational', 'modelling') is 0.2\n",
            "probability of ('modelling', 'of') is 1.0\n",
            "probability of ('of', 'various') is 0.005681818181818182\n",
            "probability of ('various', 'aspect') is 0.125\n",
            "probability of ('language', 'and') is 0.02197802197802198\n",
            "probability of ('a', 'wide') is 0.02564102564102564\n",
            "probability of ('wide', 'range') is 0.6666666666666666\n",
            "probability of ('of', 'systems') is 0.005681818181818182\n",
            "probability of ('language', 'is') is 0.01098901098901099\n",
            "probability of ('is', 'any') is 0.018518518518518517\n",
            "probability of ('any', 'language') is 1.0\n",
            "probability of ('that', 'arises') is 0.02631578947368421\n",
            "probability of ('arises', 'a') is 1.0\n",
            "probability of ('an', 'innate') is 0.05\n",
            "probability of ('innate', 'facility') is 1.0\n",
            "probability of ('facility', 'for') is 1.0\n",
            "probability of ('for', 'language') is 0.017857142857142856\n",
            "probability of ('language', 'possessed') is 0.01098901098901099\n",
            "probability of ('possessed', 'by') is 1.0\n",
            "probability of ('the', 'human') is 0.005434782608695652\n",
            "probability of ('human', 'intellect') is 0.25\n",
            "probability of ('intellect', ';') is 1.0\n",
            "probability of (';', 'it') is 0.16666666666666666\n",
            "probability of ('it', 'may') is 0.07692307692307693\n",
            "probability of ('may', 'Natural') is 0.3333333333333333\n",
            "probability of (',', 'which') is 0.006578947368421052\n",
            "probability of ('which', 'is') is 0.1111111111111111\n",
            "probability of ('a', 'branch') is 0.008547008547008548\n",
            "probability of ('branch', 'of') is 1.0\n",
            "probability of ('intelligence', ',') is 0.5\n",
            "probability of (',', 'includes') is 0.006578947368421052\n",
            "probability of ('includes', 'speech') is 0.3333333333333333\n",
            "probability of ('speech', 'synthesis') is 0.125\n",
            "probability of ('synthesis', ',') is 1.0\n",
            "probability of (',', 'Speech') is 0.006578947368421052\n",
            "probability of ('Speech', 'recognition') is 1.0\n",
            "probability of ('and', 'Machine') is 0.00847457627118644\n",
            "probability of ('Machine', 'translation') is 0.3333333333333333\n",
            "probability of ('translation', '.') is 0.25\n",
            "probability of ('Processing', 'ha') is 0.05\n",
            "probability of ('ha', 'a') is 0.08333333333333333\n",
            "probability of ('of', 'application') is 0.011363636363636364\n",
            "probability of ('application', 'in') is 0.09090909090909091\n",
            "probability of ('the', 'Indian') is 0.005434782608695652\n",
            "probability of ('Indian', 'context') is 0.5\n",
            "probability of ('context', '.') is 1.0\n",
            "probability of ('.', 'Most') is 0.00684931506849315\n",
            "probability of ('Most', 'of') is 1.0\n",
            "probability of ('the', 'rural') is 0.005434782608695652\n",
            "probability of ('rural', 'Indian') is 1.0\n",
            "probability of ('Indian', 'community') is 0.5\n",
            "probability of ('community', 'is') is 0.5\n",
            "probability of ('is', 'unable') is 0.018518518518518517\n",
            "probability of ('unable', 'to') is 1.0\n",
            "probability of ('to', 'make') is 0.013513513513513514\n",
            "probability of ('use', 'An') is 0.08333333333333333\n",
            "probability of ('An', 'Evaluation') is 0.5\n",
            "probability of ('Evaluation', 'of') is 1.0\n",
            "probability of ('of', 'LOLITA') is 0.005681818181818182\n",
            "probability of ('LOLITA', 'and') is 0.3333333333333333\n",
            "probability of ('and', 'related') is 0.00847457627118644\n",
            "probability of ('related', 'Natural') is 0.5\n",
            "probability of ('Processing', 'Systems') is 0.05\n",
            "probability of ('Systems', 'Paul') is 1.0\n",
            "probability of ('Paul', 'Callaghan') is 1.0\n",
            "probability of ('Callaghan', 'Submitted') is 1.0\n",
            "probability of ('Submitted', 'to') is 1.0\n",
            "probability of ('the', 'University') is 0.005434782608695652\n",
            "probability of ('University', 'of') is 1.0\n",
            "probability of ('of', 'Durham') is 0.005681818181818182\n",
            "probability of ('Durham', 'for') is 1.0\n",
            "probability of ('the', 'degree') is 0.005434782608695652\n",
            "probability of ('degree', 'of') is 1.0\n",
            "probability of ('of', 'Ph.D.') is 0.005681818181818182\n",
            "probability of ('Ph.D.', ',') is 1.0\n",
            "probability of (',', 'August') is 0.006578947368421052\n",
            "probability of ('August', '1997') is 1.0\n",
            "probability of ('1997', '--') is 1.0\n",
            "probability of ('--', '--') is 0.6428571428571429\n",
            "probability of ('--', '-') is 0.07142857142857142\n",
            "probability of ('-', 'This') is 1.0\n",
            "probability of ('This', 'research') is 0.034482758620689655\n",
            "probability of ('research', 'address') is 0.08333333333333333\n",
            "probability of ('the', 'question') is 0.005434782608695652\n",
            "probability of ('question', ',') is 1.0\n",
            "probability of (',', '``') is 0.006578947368421052\n",
            "probability of ('``', 'how') is 0.2\n",
            "probability of ('how', 'do') is 0.2\n",
            "probability of ('do', 'we') is 0.3333333333333333\n",
            "probability of ('we', 'evaluate') is 0.058823529411764705\n",
            "probability of ('evaluate', 'system') is 1.0\n",
            "probability of ('system', 'like') is 0.05263157894736842\n",
            "probability of ('like', 'LOLITA') is 0.5\n",
            "probability of ('LOLITA', '?') is 0.3333333333333333\n",
            "probability of ('?', \"''\") is 0.2\n",
            "probability of (\"''\", 'LOLITA') is 0.2\n",
            "probability of ('LOLITA', 'is') is 0.3333333333333333\n",
            "probability of ('the', 'Natural') is 0.005434782608695652\n",
            "probability of ('Natural', 'Previous') is 0.034482758620689655\n",
            "probability of ('Previous', 'work') is 1.0\n",
            "probability of ('work', 'demonstrated') is 0.125\n",
            "probability of ('demonstrated', 'that') is 1.0\n",
            "probability of ('that', 'Web') is 0.02631578947368421\n",
            "probability of ('Web', 'count') is 1.0\n",
            "probability of ('count', 'can') is 1.0\n",
            "probability of ('to', 'approximate') is 0.013513513513513514\n",
            "probability of ('approximate', 'bigram') is 1.0\n",
            "probability of ('bigram', 'counts') is 1.0\n",
            "probability of ('counts', ',') is 1.0\n",
            "probability of (',', 'suggesting') is 0.006578947368421052\n",
            "probability of ('suggesting', 'that') is 1.0\n",
            "probability of ('that', 'Web-based') is 0.02631578947368421\n",
            "probability of ('Web-based', 'frequency') is 0.5\n",
            "probability of ('frequency', 'should') is 1.0\n",
            "probability of ('should', 'be') is 1.0\n",
            "probability of ('be', 'useful') is 0.038461538461538464\n",
            "probability of ('useful', 'for') is 1.0\n",
            "probability of ('wide', 'variety') is 0.3333333333333333\n",
            "probability of ('variety', 'of') is 1.0\n",
            "probability of ('.', 'However') is 0.00684931506849315\n",
            "probability of ('However', ',') is 1.0\n",
            "probability of (',', 'only') is 0.006578947368421052\n",
            "probability of ('only', 'a') is 0.5\n",
            "probability of ('a', 'limited') is 0.008547008547008548\n",
            "probability of ('limited', 'number') is 0.25\n",
            "probability of ('task', 'have') is 0.09090909090909091\n",
            "probability of ('have', 'so') is 0.08333333333333333\n",
            "probability of ('far', 'been') is 0.5\n",
            "probability of ('been', 'tested') is 0.07692307692307693\n",
            "probability of ('tested', 'using') is 1.0\n",
            "probability of ('using', 'Web-scale') is 0.09090909090909091\n",
            "probability of ('Web-scale', 'data') is 1.0\n",
            "probability of ('data', 'set') is 0.16666666666666666\n",
            "probability of ('set', 'This') is 0.3333333333333333\n",
            "probability of ('.', '16.1') is 0.00684931506849315\n",
            "probability of ('16.1', 'Introduction') is 1.0\n",
            "probability of ('Introduction', 'This') is 0.2\n",
            "probability of ('chapter', 'focus') is 0.25\n",
            "probability of ('on', 'application') is 0.043478260869565216\n",
            "probability of ('application', 'This') is 0.09090909090909091\n",
            "probability of ('paper', 'describes') is 0.047619047619047616\n",
            "probability of ('describes', 'a') is 0.5\n",
            "probability of ('language', 'system') is 0.01098901098901099\n",
            "probability of ('which', 'improves') is 0.1111111111111111\n",
            "probability of ('improves', 'it') is 1.0\n",
            "probability of ('it', 'own') is 0.07692307692307693\n",
            "probability of ('own', 'performance') is 1.0\n",
            "probability of ('performance', 'through') is 0.25\n",
            "probability of ('through', 'learning') is 0.5\n",
            "probability of ('The', 'system') is 0.038461538461538464\n",
            "probability of ('system', 'process') is 0.05263157894736842\n",
            "probability of ('process', 'short') is 0.09090909090909091\n",
            "probability of ('short', 'English') is 0.5\n",
            "probability of ('English', 'narrative') is 0.5\n",
            "probability of ('narrative', 'and') is 0.5\n",
            "probability of ('and', 'is') is 0.00847457627118644\n",
            "probability of ('is', 'able') is 0.018518518518518517\n",
            "probability of ('to', 'acquire') is 0.013513513513513514\n",
            "probability of ('acquire', ',') is 0.5\n",
            "probability of (',', 'from') is 0.006578947368421052\n",
            "probability of ('single', 'narrative') is 0.3333333333333333\n",
            "probability of ('narrative', ',') is 0.5\n",
            "probability of ('new', 'schema') is 0.16666666666666666\n",
            "probability of ('schema', 'for') is 1.0\n",
            "probability of ('a', 'stereotypical') is 0.008547008547008548\n",
            "probability of ('stereotypical', 'set') is 1.0\n",
            "probability of ('set', 'of') is 0.6666666666666666\n",
            "probability of ('of', 'actions') is 0.005681818181818182\n",
            "probability of ('actions', '.') is 1.0\n",
            "probability of ('During', 'the') is 0.5\n",
            "probability of ('the', 'understanding') is 0.005434782608695652\n",
            "probability of ('understanding', 'process') is 0.16666666666666666\n",
            "probability of ('process', ',') is 0.09090909090909091\n",
            "probability of ('the', 'system') is 0.005434782608695652\n",
            "probability of ('system', 'attempt') is 0.05263157894736842\n",
            "probability of ('attempt', 'We') is 1.0\n",
            "probability of ('We', 'classify') is 0.041666666666666664\n",
            "probability of ('classify', 'and') is 1.0\n",
            "probability of ('and', 'review') is 0.00847457627118644\n",
            "probability of ('review', 'current') is 0.25\n",
            "probability of ('current', 'approach') is 0.14285714285714285\n",
            "probability of ('to', 'software') is 0.013513513513513514\n",
            "probability of ('software', 'infrastructure') is 0.25\n",
            "probability of ('infrastructure', 'for') is 1.0\n",
            "probability of ('for', 'research') is 0.017857142857142856\n",
            "probability of ('research', ',') is 0.08333333333333333\n",
            "probability of (',', 'development') is 0.006578947368421052\n",
            "probability of ('and', 'delivery') is 0.00847457627118644\n",
            "probability of ('delivery', 'of') is 1.0\n",
            "probability of ('NLP', 'systems') is 0.022727272727272728\n",
            "probability of ('The', 'task') is 0.038461538461538464\n",
            "probability of ('task', 'Confidence') is 0.09090909090909091\n",
            "probability of ('Confidence', 'measure') is 0.5\n",
            "probability of ('are', 'a') is 0.07692307692307693\n",
            "probability of ('a', 'practical') is 0.008547008547008548\n",
            "probability of ('practical', 'solution') is 0.3333333333333333\n",
            "probability of ('solution', 'for') is 0.5\n",
            "probability of ('for', 'improving') is 0.017857142857142856\n",
            "probability of ('improving', 'the') is 1.0\n",
            "probability of ('the', 'usefulness') is 0.005434782608695652\n",
            "probability of ('usefulness', 'of') is 1.0\n",
            "probability of ('Processing', 'applications') is 0.05\n",
            "probability of ('.', 'Confidence') is 0.00684931506849315\n",
            "probability of ('Confidence', 'estimation') is 0.5\n",
            "probability of ('estimation', 'is') is 0.5\n",
            "probability of ('a', 'generic') is 0.008547008547008548\n",
            "probability of ('generic', 'machine') is 0.3333333333333333\n",
            "probability of ('learning', 'approach') is 0.05\n",
            "probability of ('for', 'deriving') is 0.017857142857142856\n",
            "probability of ('deriving', 'confidence') is 1.0\n",
            "probability of ('confidence', 'measures') is 0.5\n",
            "probability of ('measures', '.') is 1.0\n",
            "probability of ('We', 'give') is 0.041666666666666664\n",
            "probability of ('give', 'an') is 1.0\n",
            "probability of ('overview', 'of') is 0.5\n",
            "probability of ('of', 'confidence') is 0.005681818181818182\n",
            "probability of ('confidence', 'estimation') is 0.5\n",
            "probability of ('estimation', 'in') is 0.5\n",
            "probability of ('various', 'field') is 0.125\n",
            "probability of ('field', '!') is 0.125\n",
            "probability of ('!', 'lex-sign') is 1.0\n",
            "probability of ('lex-sign', 'sense-id') is 1.0\n",
            "probability of ('sense-id', ':') is 0.5\n",
            "probability of (':', 'sense-id') is 0.15789473684210525\n",
            "probability of ('sense-id', 'dictionary') is 0.16666666666666666\n",
            "probability of ('dictionary', '?') is 0.5\n",
            "probability of ('?', '=') is 0.6\n",
            "probability of ('=', '``') is 1.0\n",
            "probability of ('``', 'LDOCE') is 0.2\n",
            "probability of ('LDOCE', \"''\") is 0.5\n",
            "probability of (\"''\", '!') is 0.4\n",
            "probability of ('sense-id', 'ldb-entry-no') is 0.16666666666666666\n",
            "probability of ('ldb-entry-no', '?') is 1.0\n",
            "probability of ('``', '12364') is 0.2\n",
            "probability of ('12364', \"''\") is 1.0\n",
            "probability of ('sense-id', 'sense-no') is 0.16666666666666666\n",
            "probability of ('sense-no', '?') is 1.0\n",
            "probability of ('``', '0') is 0.2\n",
            "probability of ('0', \"''\") is 1.0\n",
            "probability of (\"''\", '.') is 0.2\n",
            "probability of ('.', 'When') is 0.00684931506849315\n",
            "probability of ('When', 'loaded') is 1.0\n",
            "probability of ('loaded', 'into') is 1.0\n",
            "probability of ('into', 'the') is 0.3333333333333333\n",
            "probability of ('the', 'LKB') is 0.010869565217391304\n",
            "probability of ('LKB', ',') is 0.25\n",
            "probability of (',', '(') is 0.006578947368421052\n",
            "probability of ('(', '9') is 0.0392156862745098\n",
            "probability of ('9', ')') is 1.0\n",
            "probability of (')', 'will') is 0.02127659574468085\n",
            "probability of ('be', 'expanded') is 0.038461538461538464\n",
            "probability of ('expanded', 'into') is 0.5\n",
            "probability of ('a', 'fully-fledged') is 0.008547008547008548\n",
            "probability of ('fully-fledged', 'representation') is 1.0\n",
            "probability of ('representation', 'for') is 0.5\n",
            "probability of ('the', 'transitive') is 0.005434782608695652\n",
            "probability of ('transitive', 'use') is 1.0\n",
            "probability of ('of', 'experience') is 0.005681818181818182\n",
            "probability of ('experience', ';') is 0.3333333333333333\n",
            "probability of (';', 'by') is 0.16666666666666666\n",
            "probability of ('by', 'integrating') is 0.05555555555555555\n",
            "probability of ('integrating', 'word-specific') is 0.5\n",
            "probability of ('word-specific', 'information') is 1.0\n",
            "probability of ('information', 'provided') is 0.05263157894736842\n",
            "probability of ('provided', 'by') is 1.0\n",
            "probability of ('by', '(') is 0.05555555555555555\n",
            "probability of (')', 'with') is 0.02127659574468085\n",
            "probability of ('information', 'encoded') is 0.05263157894736842\n",
            "probability of ('encoded', 'by') is 1.0\n",
            "probability of ('LKB', 'type') is 0.25\n",
            "probability of ('type', 'strict-trans-sign') is 0.3333333333333333\n",
            "probability of ('strict-trans-sign', '.') is 1.0\n",
            "probability of ('.', 'Thus') is 0.00684931506849315\n",
            "probability of ('Thus', ',') is 1.0\n",
            "probability of (',', 'although') is 0.006578947368421052\n",
            "probability of ('although', 'neither') is 1.0\n",
            "probability of ('neither', 'LDOCE') is 1.0\n",
            "probability of ('LDOCE', ',') is 0.5\n",
            "probability of (',', 'LLCE') is 0.006578947368421052\n",
            "probability of ('LLCE', 'or') is 1.0\n",
            "probability of ('or', 'the') is 0.058823529411764705\n",
            "probability of ('the', 'earlier') is 0.005434782608695652\n",
            "probability of ('earlier', 'subcategorised') is 1.0\n",
            "probability of ('subcategorised', 'lexicon') is 1.0\n",
            "probability of ('lexicon', 'contain') is 0.5\n",
            "probability of ('contain', 'all') is 1.0\n",
            "probability of ('about', 'psychological') is 0.3333333333333333\n",
            "probability of ('psychological', 'verb') is 1.0\n",
            "probability of ('verb', 'defined') is 0.5\n",
            "probability of ('defined', 'in') is 1.0\n",
            "probability of ('in', 'Sanfilippo') is 0.013333333333333334\n",
            "probability of ('Sanfilippo', '&') is 1.0\n",
            "probability of ('&', 'aposs') is 0.3333333333333333\n",
            "probability of ('aposs', 'type') is 1.0\n",
            "probability of ('type', 'system') is 0.6666666666666666\n",
            "probability of (',', 'by') is 0.006578947368421052\n",
            "probability of ('by', 'using') is 0.05555555555555555\n",
            "probability of ('using', 'the') is 0.18181818181818182\n",
            "probability of ('the', 'conjunction') is 0.005434782608695652\n",
            "probability of ('conjunction', 'of') is 1.0\n",
            "probability of ('information', 'available') is 0.05263157894736842\n",
            "probability of ('available', 'from') is 1.0\n",
            "probability of ('from', 'all') is 0.05555555555555555\n",
            "probability of ('all', 'three') is 0.14285714285714285\n",
            "probability of ('three', ',') is 0.3333333333333333\n",
            "probability of ('it', 'proved') is 0.07692307692307693\n",
            "probability of ('proved', 'possible') is 1.0\n",
            "probability of ('possible', 'to') is 0.5\n",
            "probability of ('to', 'effectively') is 0.013513513513513514\n",
            "probability of ('effectively', 'enrich') is 0.3333333333333333\n",
            "probability of ('enrich', 'this') is 1.0\n",
            "probability of ('this', 'information') is 0.045454545454545456\n",
            "probability of ('information', 'at') is 0.05263157894736842\n",
            "probability of ('same', 'time') is 0.3333333333333333\n",
            "probability of ('time', 'a') is 0.5\n",
            "probability of ('a', 'mapping') is 0.008547008547008548\n",
            "probability of ('mapping', 'it') is 1.0\n",
            "probability of ('it', 'into') is 0.07692307692307693\n",
            "probability of ('a', 'formal') is 0.008547008547008548\n",
            "probability of ('formal', 'representation') is 1.0\n",
            "probability of ('representation', '.') is 0.5\n",
            "probability of ('.', '4.2.5') is 0.00684931506849315\n",
            "probability of ('4.2.5', 'Towards') is 1.0\n",
            "probability of ('Towards', 'a') is 1.0\n",
            "probability of ('a', 'Multilingual') is 0.008547008547008548\n",
            "probability of ('Multilingual', 'LKB') is 1.0\n",
            "probability of ('LKB', 'A') is 0.25\n",
            "probability of ('A', 'goal') is 0.25\n",
            "probability of ('goal', 'of') is 0.3333333333333333\n",
            "probability of ('of', 'ACQUILEX') is 0.005681818181818182\n",
            "probability of ('ACQUILEX', 'is') is 1.0\n",
            "probability of ('to', 'demonstrate') is 0.013513513513513514\n",
            "probability of ('demonstrate', 'that') is 1.0\n",
            "probability of ('that', 'an') is 0.02631578947368421\n",
            "probability of ('an', 'LKB') is 0.05\n",
            "probability of ('LKB', 'can') is 0.25\n",
            "probability of ('be', 'produced') is 0.038461538461538464\n",
            "probability of ('produced', 'that') is 1.0\n",
            "probability of ('that', 'usefully') is 0.02631578947368421\n",
            "probability of ('usefully', 'exploit') is 1.0\n",
            "probability of ('exploit', 'various') is 1.0\n",
            "probability of ('various', 'MRD') is 0.125\n",
            "probability of ('MRD', 'source') is 1.0\n",
            "probability of ('source', 'and') is 0.5\n",
            "probability of ('and', 'integrates') is 0.00847457627118644\n",
            "probability of ('integrates', 'multilingual') is 1.0\n",
            "probability of ('multilingual', 'information') is 1.0\n",
            "probability of ('The', 'use') is 0.038461538461538464\n",
            "probability of ('a', 'common') is 0.017094017094017096\n",
            "probability of ('common', 'LRL') is 0.5\n",
            "probability of ('LRL', 'with') is 1.0\n",
            "probability of ('common', 'type') is 0.5\n",
            "probability of ('make', 'it') is 0.25\n",
            "probability of ('it', 'possi') is 0.07692307692307693\n",
            "probability of ('possi', '...') is 1.0\n",
            "probability of ('...', 'We') is 1.0\n",
            "probability of ('the', 'Stanford') is 0.005434782608695652\n",
            "probability of ('Stanford', 'CoreNLP') is 1.0\n",
            "probability of ('CoreNLP', 'toolkit') is 1.0\n",
            "probability of ('toolkit', ',') is 0.5\n",
            "probability of (',', 'an') is 0.006578947368421052\n",
            "probability of ('an', 'extensible') is 0.05\n",
            "probability of ('extensible', 'pipeline') is 1.0\n",
            "probability of ('pipeline', 'that') is 1.0\n",
            "probability of ('that', 'provides') is 0.02631578947368421\n",
            "probability of ('provides', 'core') is 0.5\n",
            "probability of ('core', 'natural') is 1.0\n",
            "probability of ('natural', 'lan-guage') is 0.015625\n",
            "probability of ('lan-guage', 'analysis') is 1.0\n",
            "probability of ('This', 'toolkit') is 0.034482758620689655\n",
            "probability of ('toolkit', 'is') is 0.5\n",
            "probability of ('is', 'quite') is 0.018518518518518517\n",
            "probability of ('quite', 'widely') is 1.0\n",
            "probability of ('widely', 'used') is 1.0\n",
            "probability of ('used', ',') is 0.09090909090909091\n",
            "probability of (',', 'both') is 0.006578947368421052\n",
            "probability of ('both', 'in') is 0.2\n",
            "probability of ('research', 'NLP') is 0.08333333333333333\n",
            "probability of ('NLP', 'community') is 0.022727272727272728\n",
            "probability of ('community', 'and') is 0.5\n",
            "probability of ('and', 'also') is 0.00847457627118644\n",
            "probability of ('also', 'among') is 0.2\n",
            "probability of ('among', 'commercial') is 0.3333333333333333\n",
            "probability of ('commercial', 'and') is 1.0\n",
            "probability of ('and', 'govern-ment') is 0.00847457627118644\n",
            "probability of ('govern-ment', 'user') is 1.0\n",
            "probability of ('user', 'of') is 0.5\n",
            "probability of ('of', 'open') is 0.005681818181818182\n",
            "probability of ('open', 'source') is 1.0\n",
            "probability of ('source', 'NLP') is 0.5\n",
            "probability of ('NLP', 'technol-ogy') is 0.022727272727272728\n",
            "probability of ('technol-ogy', '.') is 1.0\n",
            "probability of ('We', 'suggest') is 0.041666666666666664\n",
            "probability of ('suggest', 'Gaussian') is 1.0\n",
            "probability of ('Gaussian', 'Processes') is 1.0\n",
            "probability of ('Processes', '(') is 1.0\n",
            "probability of ('(', 'GPs') is 0.0196078431372549\n",
            "probability of ('GPs', ')') is 1.0\n",
            "probability of ('a', 'powerful') is 0.008547008547008548\n",
            "probability of ('powerful', 'mod-elling') is 0.5\n",
            "probability of ('mod-elling', 'framework') is 1.0\n",
            "probability of ('framework', 'incorporating') is 0.5\n",
            "probability of ('incorporating', 'kernel') is 1.0\n",
            "probability of ('kernel', 'and') is 1.0\n",
            "probability of ('and', 'Bayesian') is 0.00847457627118644\n",
            "probability of ('Bayesian', 'inference') is 1.0\n",
            "probability of ('inference', ',') is 1.0\n",
            "probability of ('are', 'recognised') is 0.038461538461538464\n",
            "probability of ('recognised', 'a') is 1.0\n",
            "probability of ('a', 'state-of-the-art') is 0.008547008547008548\n",
            "probability of ('state-of-the-art', 'for') is 1.0\n",
            "probability of ('for', 'many') is 0.017857142857142856\n",
            "probability of ('many', 'machine') is 0.5\n",
            "probability of ('learning', 'tasks') is 0.05\n",
            "probability of ('.', ':') is 0.00684931506849315\n",
            "probability of (':', 'A') is 0.05263157894736842\n",
            "probability of ('A', 'fundamental') is 0.25\n",
            "probability of ('fundamental', 'issue') is 0.5\n",
            "probability of ('issue', 'in') is 0.2\n",
            "probability of ('the', 'prerequisite') is 0.005434782608695652\n",
            "probability of ('prerequisite', 'of') is 1.0\n",
            "probability of ('an', 'enormous') is 0.05\n",
            "probability of ('enormous', 'quantity') is 1.0\n",
            "probability of ('quantity', 'of') is 1.0\n",
            "probability of ('of', 'preprogrammed') is 0.005681818181818182\n",
            "probability of ('preprogrammed', 'knowledge') is 1.0\n",
            "probability of ('knowledge', 'concerning') is 0.08333333333333333\n",
            "probability of ('concerning', 'both') is 0.5\n",
            "probability of ('both', 'the') is 0.2\n",
            "probability of ('the', 'language') is 0.005434782608695652\n",
            "probability of ('the', 'domain') is 0.005434782608695652\n",
            "probability of ('domain', 'under') is 0.2\n",
            "probability of ('under', 'examination') is 1.0\n",
            "probability of ('examination', '.') is 1.0\n",
            "probability of ('.', 'Manual') is 0.00684931506849315\n",
            "probability of ('Manual', 'acquisition') is 1.0\n",
            "probability of ('acquisition', 'of') is 0.3333333333333333\n",
            "probability of ('this', 'knowledge') is 0.045454545454545456\n",
            "probability of ('knowledge', 'is') is 0.08333333333333333\n",
            "probability of ('is', 'tedious') is 0.018518518518518517\n",
            "probability of ('tedious', 'and') is 1.0\n",
            "probability of ('and', 'error') is 0.00847457627118644\n",
            "probability of ('error', 'prone') is 1.0\n",
            "probability of ('prone', '.') is 1.0\n",
            "probability of ('.', 'Development') is 0.00684931506849315\n",
            "probability of ('Development', 'of') is 1.0\n",
            "probability of ('an', 'automated') is 0.05\n",
            "probability of ('automated', 'acquisition') is 0.2\n",
            "probability of ('acquisition', '``') is 0.3333333333333333\n",
            "probability of ('``', 'that') is 0.2\n",
            "probability of ('that', 'support') is 0.02631578947368421\n",
            "probability of ('support', 'sophisticated') is 0.3333333333333333\n",
            "probability of ('sophisticated', 'natural') is 0.5\n",
            "probability of ('processing', 'while') is 0.015625\n",
            "probability of ('while', 'significantly') is 0.3333333333333333\n",
            "probability of ('significantly', 'simplifying') is 1.0\n",
            "probability of ('simplifying', 'the') is 1.0\n",
            "probability of ('the', 'interface') is 0.005434782608695652\n",
            "probability of ('interface', 'between') is 0.5\n",
            "probability of ('between', 'domain-specific') is 0.125\n",
            "probability of ('domain-specific', 'knowledge') is 1.0\n",
            "probability of ('knowledge', 'and') is 0.08333333333333333\n",
            "probability of ('and', 'general') is 0.00847457627118644\n",
            "probability of ('general', 'linguis-') is 0.3333333333333333\n",
            "probability of ('linguis-', 'tic') is 1.0\n",
            "probability of ('tic', 'resources') is 1.0\n",
            "probability of ('paper', 'present') is 0.09523809523809523\n",
            "probability of ('present', 'the') is 0.14285714285714285\n",
            "probability of ('the', 'result') is 0.005434782608695652\n",
            "probability of ('result', 'of') is 0.125\n",
            "probability of ('our', 'experience') is 0.5\n",
            "probability of ('experience', 'in') is 0.3333333333333333\n",
            "probability of ('in', 'designing') is 0.013333333333333334\n",
            "probability of ('designing', 'and') is 1.0\n",
            "probability of ('and', 'using') is 0.00847457627118644\n",
            "probability of ('the', 'upper') is 0.005434782608695652\n",
            "probability of ('upper', 'model') is 1.0\n",
            "probability of ('a', 'variety') is 0.008547008547008548\n",
            "probability of ('application', 'over') is 0.09090909090909091\n",
            "probability of ('past', '5') is 0.5\n",
            "probability of ('5', 'year') is 1.0\n",
            "probability of ('year', 'into') is 0.25\n",
            "probability of ('same', 'or') is 0.3333333333333333\n",
            "probability of ('or', 'neighboring') is 0.058823529411764705\n",
            "probability of ('neighboring', 'map') is 1.0\n",
            "probability of ('map', 'nodes') is 1.0\n",
            "probability of ('nodes', '.') is 1.0\n",
            "probability of ('.', 'Nodes') is 0.00684931506849315\n",
            "probability of ('Nodes', 'may') is 1.0\n",
            "probability of ('may', 'thus') is 0.3333333333333333\n",
            "probability of ('thus', 'be') is 1.0\n",
            "probability of ('be', 'viewed') is 0.038461538461538464\n",
            "probability of ('viewed', 'a') is 1.0\n",
            "probability of ('word', 'categories') is 0.07692307692307693\n",
            "probability of ('categories', '.') is 1.0\n",
            "probability of ('.', 'Although') is 0.00684931506849315\n",
            "probability of ('Although', 'no') is 1.0\n",
            "probability of ('no', 'a') is 1.0\n",
            "probability of ('a', 'priori') is 0.008547008547008548\n",
            "probability of ('priori', 'information') is 1.0\n",
            "probability of ('about', 'class') is 0.3333333333333333\n",
            "probability of ('class', 'is') is 0.25\n",
            "probability of ('is', 'given') is 0.018518518518518517\n",
            "probability of ('given', ',') is 0.25\n",
            "probability of (',', 'during') is 0.006578947368421052\n",
            "probability of ('the', 'self-organizing') is 0.005434782608695652\n",
            "probability of ('self-organizing', 'process') is 1.0\n",
            "probability of ('process', 'a') is 0.09090909090909091\n",
            "probability of ('a', 'model') is 0.008547008547008548\n",
            "probability of ('word', 'class') is 0.07692307692307693\n",
            "probability of ('class', 'emerges') is 0.25\n",
            "probability of ('emerges', '.') is 1.0\n",
            "probability of ('The', 'central') is 0.038461538461538464\n",
            "probability of ('central', 'topic') is 1.0\n",
            "probability of ('topic', 'of') is 1.0\n",
            "probability of ('the', 'thesis') is 0.005434782608695652\n",
            "probability of ('thesis', 'is') is 0.5\n",
            "probability of ('the', 'SOM') is 0.005434782608695652\n",
            "probability of ('SOM', 'in') is 1.0\n",
            "probability of ('The', 'approach') is 0.038461538461538464\n",
            "probability of ('a', 'workbench') is 0.008547008547008548\n",
            "probability of ('workbench', 'built') is 0.5\n",
            "probability of ('built', 'by') is 0.5\n",
            "probability of ('by', 'Priberam') is 0.05555555555555555\n",
            "probability of ('Priberam', 'Informática') is 1.0\n",
            "probability of ('Informática', 'for') is 1.0\n",
            "probability of ('the', 'company') is 0.005434782608695652\n",
            "probability of ('company', '’') is 1.0\n",
            "probability of ('’', 's') is 1.0\n",
            "probability of ('s', 'natural') is 1.0\n",
            "probability of ('processing', 'technology') is 0.015625\n",
            "probability of ('technology', '.') is 0.3333333333333333\n",
            "probability of ('This', 'workbench') is 0.034482758620689655\n",
            "probability of ('workbench', 'includes') is 0.5\n",
            "probability of ('a', 'set') is 0.008547008547008548\n",
            "probability of ('linguistic', 'resource') is 0.14285714285714285\n",
            "probability of ('resource', 'and') is 0.3333333333333333\n",
            "probability of ('software', 'tool') is 0.25\n",
            "probability of ('tool', 'that') is 0.3333333333333333\n",
            "probability of ('that', 'have') is 0.02631578947368421\n",
            "probability of ('been', 'applied') is 0.07692307692307693\n",
            "probability of ('applied', 'in') is 0.25\n",
            "probability of ('a', 'considerable') is 0.008547008547008548\n",
            "probability of ('considerable', 'number') is 1.0\n",
            "probability of ('of', 'practical') is 0.005681818181818182\n",
            "probability of ('practical', 'purposes') is 0.3333333333333333\n",
            "probability of ('purposes', ',') is 1.0\n",
            "probability of (',', 'covering') is 0.006578947368421052\n",
            "probability of ('covering', 'Abstract—Natural') is 0.5\n",
            "probability of ('Abstract—Natural', 'Language') is 0.5\n",
            "probability of ('an', 'effective') is 0.05\n",
            "probability of ('effective', 'approach') is 0.5\n",
            "probability of ('for', 'bringing') is 0.017857142857142856\n",
            "probability of ('bringing', 'improvement') is 1.0\n",
            "probability of ('in', 'educational') is 0.013333333333333334\n",
            "probability of ('educational', 'setting') is 0.5\n",
            "probability of ('setting', '.') is 1.0\n",
            "probability of ('.', 'Implementing') is 0.00684931506849315\n",
            "probability of ('Implementing', 'NLP') is 1.0\n",
            "probability of ('NLP', 'involves') is 0.022727272727272728\n",
            "probability of ('involves', 'initiating') is 0.5\n",
            "probability of ('initiating', 'the') is 1.0\n",
            "probability of ('of', 'learning') is 0.005681818181818182\n",
            "probability of ('learning', 'through') is 0.05\n",
            "probability of ('through', 'the') is 0.5\n",
            "probability of ('natural', 'acquisition') is 0.015625\n",
            "probability of ('acquisition', 'in') is 0.3333333333333333\n",
            "probability of ('the', 'educational') is 0.005434782608695652\n",
            "probability of ('educational', 'systems') is 0.5\n",
            "probability of ('It', 'is') is 0.2\n",
            "probability of ('is', 'based') is 0.018518518518518517\n",
            "probability of ('on', 'effective') is 0.043478260869565216\n",
            "probability of ('for', 'providing') is 0.017857142857142856\n",
            "probability of ('a', 'solution') is 0.008547008547008548\n",
            "probability of ('solution', 'ABSTRACT') is 0.5\n",
            "probability of (':', 'After') is 0.05263157894736842\n",
            "probability of ('After', 'twenty') is 0.5\n",
            "probability of ('twenty', 'year') is 1.0\n",
            "probability of ('year', 'of') is 0.25\n",
            "probability of ('of', 'disfavor') is 0.005681818181818182\n",
            "probability of ('disfavor', ',') is 1.0\n",
            "probability of ('a', 'technology') is 0.008547008547008548\n",
            "probability of ('technology', 'ha') is 0.3333333333333333\n",
            "probability of ('ha', 'returned') is 0.08333333333333333\n",
            "probability of ('returned', 'which') is 1.0\n",
            "probability of ('which', 'imitates') is 0.1111111111111111\n",
            "probability of ('imitates', 'the') is 1.0\n",
            "probability of ('the', 'brain') is 0.005434782608695652\n",
            "probability of ('brain', '.') is 1.0\n",
            "probability of ('language', 'experiment') is 0.01098901098901099\n",
            "probability of ('experiment', '(') is 0.5\n",
            "probability of ('(', 'Sejnowski') is 0.0196078431372549\n",
            "probability of ('Sejnowski', '&') is 1.0\n",
            "probability of ('&', 'Rosenberg') is 0.3333333333333333\n",
            "probability of ('Rosenberg', ':') is 1.0\n",
            "probability of (':', '1986') is 0.05263157894736842\n",
            "probability of ('1986', ')') is 1.0\n",
            "probability of (')', 'demonstrate') is 0.02127659574468085\n",
            "probability of ('that', 'neural') is 0.02631578947368421\n",
            "probability of ('network', 'computing') is 0.2\n",
            "probability of ('computing', 'architecture') is 1.0\n",
            "probability of ('architecture', 'can') is 0.2\n",
            "probability of ('can', 'learn') is 0.0625\n",
            "probability of ('learn', 'from') is 0.5\n",
            "probability of ('from', 'actual') is 0.05555555555555555\n",
            "probability of ('actual', 'spoken') is 1.0\n",
            "probability of ('spoken', 'language') is 0.25\n",
            "probability of ('language', ',') is 0.01098901098901099\n",
            "probability of (',', 'observe') is 0.006578947368421052\n",
            "probability of ('observe', 'rule') is 1.0\n",
            "probability of ('rule', 'of') is 0.5\n",
            "probability of ('of', 'pronunciation') is 0.005681818181818182\n",
            "probability of ('pronunciation', 'Text') is 1.0\n",
            "probability of ('Text', 'statistic') is 1.0\n",
            "probability of ('statistic', 'are') is 0.3333333333333333\n",
            "probability of ('are', 'frequently') is 0.038461538461538464\n",
            "probability of ('frequently', 'used') is 1.0\n",
            "probability of ('in', 'stylometry') is 0.013333333333333334\n",
            "probability of ('stylometry', 'and') is 1.0\n",
            "probability of ('and', 'cryptography') is 0.00847457627118644\n",
            "probability of ('cryptography', 'studies') is 1.0\n",
            "probability of ('studies', '.') is 1.0\n",
            "probability of ('some', 'text') is 0.2\n",
            "probability of ('text', 'statistic') is 0.09090909090909091\n",
            "probability of ('statistic', 'tool') is 0.3333333333333333\n",
            "probability of ('tool', 'are') is 0.3333333333333333\n",
            "probability of ('are', 'developed') is 0.038461538461538464\n",
            "probability of ('developed', 'in') is 0.2\n",
            "probability of ('in', 'ISO') is 0.013333333333333334\n",
            "probability of ('ISO', 'Prolog') is 1.0\n",
            "probability of ('Prolog', 'for') is 0.5\n",
            "probability of ('.', 'Details') is 0.00684931506849315\n",
            "probability of ('Details', 'are') is 1.0\n",
            "probability of ('are', 'given') is 0.038461538461538464\n",
            "probability of ('given', 'on') is 0.25\n",
            "probability of ('the', 'usage') is 0.005434782608695652\n",
            "probability of ('usage', 'of') is 1.0\n",
            "probability of ('of', '21') is 0.005681818181818182\n",
            "probability of ('21', 'user-callable') is 1.0\n",
            "probability of ('user-callable', 'predicates') is 1.0\n",
            "probability of ('predicates', '.') is 1.0\n",
            "probability of ('.', 'Logic') is 0.00684931506849315\n",
            "probability of ('Logic', 'and') is 1.0\n",
            "probability of ('and', 'limitation') is 0.00847457627118644\n",
            "probability of ('limitation', 'of') is 1.0\n",
            "probability of ('the', 'program') is 0.005434782608695652\n",
            "probability of ('program', 'are') is 0.3333333333333333\n",
            "probability of ('are', 'also') is 0.038461538461538464\n",
            "probability of ('also', 'discussed') is 0.2\n",
            "probability of ('discussed', 'We') is 1.0\n",
            "probability of ('We', 'summarize') is 0.041666666666666664\n",
            "probability of ('summarize', 'our') is 1.0\n",
            "probability of ('experience', 'using') is 0.3333333333333333\n",
            "probability of ('using', 'FrameNet') is 0.09090909090909091\n",
            "probability of ('FrameNet', 'in') is 1.0\n",
            "probability of ('two', 'rather') is 0.3333333333333333\n",
            "probability of ('rather', 'different') is 0.3333333333333333\n",
            "probability of ('different', 'project') is 0.25\n",
            "probability of ('project', 'in') is 1.0\n",
            "probability of ('We', 'conclude') is 0.041666666666666664\n",
            "probability of ('conclude', 'that') is 1.0\n",
            "probability of ('that', 'NLP') is 0.02631578947368421\n",
            "probability of ('NLP', 'can') is 0.022727272727272728\n",
            "probability of ('can', 'benefit') is 0.0625\n",
            "probability of ('benefit', 'from') is 0.5\n",
            "probability of ('from', 'FrameNet') is 0.05555555555555555\n",
            "probability of ('different', 'ways') is 0.25\n",
            "probability of ('ways', ',') is 1.0\n",
            "probability of ('but', 'we') is 0.16666666666666666\n",
            "probability of ('we', 'sketch') is 0.058823529411764705\n",
            "probability of ('sketch', 'some') is 1.0\n",
            "probability of ('some', 'problem') is 0.2\n",
            "probability of ('problem', 'that') is 0.25\n",
            "probability of ('be', 'overcome') is 0.038461538461538464\n",
            "probability of ('overcome', '.') is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4tE7bdKulC1j",
        "outputId": "434743a3-021d-462d-94dc-86671a95cc34"
      },
      "source": [
        "!pip install benepar\r\n",
        "import benepar\r\n",
        "df1.head(5)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting benepar\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/17/c398a35d0f303a534de8ec6949aa2ee68cc6bdbf0930685d92719b97aa1e/benepar-0.2.0.tar.gz\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.2.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.8.0+cu101)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/8c/775b7e141f11d509d59d0d2d801337ff3ad0203bc1a40335ea83e1161ba7/torch_struct-0.5-py3-none-any.whl\n",
            "Collecting tokenizers>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 8.5MB/s \n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/d8/5144b0712f7f82229a8da5983a8fbb8d30cec5fbd5f8d12ffe1854dcea67/transformers-4.4.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.12.4)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.15.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (54.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (3.7.4.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[tokenizers,torch]>=4.2.2->benepar) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[tokenizers,torch]>=4.2.2->benepar) (3.4.1)\n",
            "Building wheels for collected packages: benepar, sacremoses\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-cp37-none-any.whl size=37647 sha256=25efb9c67abdc15b6c61901ea0cdfe5cc66680cff9cb84722dad97d93bdcf09d\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/90/28/513063023646df7774be9401c440a5f13b48bdbab15a67fc42\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=6a053fc72b995dbc4879e8cb96d5ef9c54d7992ede72f70a63a7e4a7f89c4777\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built benepar sacremoses\n",
            "Installing collected packages: torch-struct, tokenizers, sacremoses, transformers, sentencepiece, benepar\n",
            "Successfully installed benepar-0.2.0 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.10.1 torch-struct-0.5 transformers-4.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>Abstract not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe method statistical modeling based max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>describ a method for statist model base on max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional random fields natural lang...</td>\n",
              "      <td>scaling conditional random fields for natural ...</td>\n",
              "      <td>scale condit random field for natur languag pr...</td>\n",
              "      <td>Scaling conditional random field for natural l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses issue cooperation linguist...</td>\n",
              "      <td>the paper addresses the issue of cooperation b...</td>\n",
              "      <td>the paper address the issu of cooper between l...</td>\n",
              "      <td>The paper address the issue of cooperation bet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In natural language processing applications, D...</td>\n",
              "      <td>in most natural language processing applicatio...</td>\n",
              "      <td>In most natur languag process applications, de...</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Abstract  ...                                      Lemmitization\n",
              "0                       Abstract not found       ...  ...                                 Abstract not found\n",
              "1                        describe a method for st...  ...  describe a method for statistical modeling bas...\n",
              "2                       Scaling conditional rando...  ...  Scaling conditional random field for natural l...\n",
              "3                       The paper addresses the i...  ...  The paper address the issue of cooperation bet...\n",
              "4                       In most natural language ...  ...  In most natural language processing applicatio...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "yFcgmKjclLez",
        "outputId": "a8fbc671-b140-4149-efd3-42236ba57303"
      },
      "source": [
        "#1.3 Noun Phrase\r\n",
        "NN_counter=[]\r\n",
        "for sentence in df1['Lemmitization']:\r\n",
        "  item=nltk.pos_tag(nltk.word_tokenize(sentence))\r\n",
        "  words=[x for x in item if x[1]=='NN']\r\n",
        "  NN_counter.append(len(words))\r\n",
        "NN_freq=[x/max(NN_counter) for x in NN_counter]\r\n",
        "print(NN_freq)\r\n",
        "df1['NN_Freq']=NN_freq\r\n",
        "df1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.22727272727272727, 0.09090909090909091, 0.22727272727272727, 0.18181818181818182, 0.2727272727272727, 0.22727272727272727, 0.2727272727272727, 0.25, 0.22727272727272727, 0.2727272727272727, 0.29545454545454547, 0.3409090909090909, 0.3181818181818182, 0.4090909090909091, 0.3181818181818182, 0.1590909090909091, 0.2727272727272727, 0.2727272727272727, 0.29545454545454547, 0.36363636363636365, 0.38636363636363635, 0.22727272727272727, 0.22727272727272727, 0.2727272727272727, 0.29545454545454547, 0.3181818181818182, 0.3181818181818182, 0.045454545454545456, 0.3409090909090909, 0.3409090909090909, 0.25, 0.1590909090909091, 0.36363636363636365, 0.0, 0.20454545454545456, 0.0, 0.06818181818181818, 0.36363636363636365, 0.9772727272727273, 1.0, 0.06818181818181818, 0.022727272727272728, 0.0, 0.0, 0.0, 0.0, 0.25, 0.2727272727272727, 0.38636363636363635, 0.18181818181818182, 0.29545454545454547, 0.3181818181818182, 0.36363636363636365, 0.13636363636363635, 0.20454545454545456, 0.25, 0.25, 0.18181818181818182, 0.13636363636363635, 0.3181818181818182, 0.3181818181818182, 0.3409090909090909, 0.38636363636363635, 0.25, 0.22727272727272727, 0.4090909090909091, 0.25, 0.25, 0.2727272727272727, 0.22727272727272727, 0.3181818181818182, 0.0, 0.18181818181818182, 0.09090909090909091, 0.29545454545454547, 0.3409090909090909, 0.0, 0.29545454545454547, 0.11363636363636363, 0.0, 0.3181818181818182, 0.22727272727272727, 0.11363636363636363, 0.18181818181818182, 0.3409090909090909, 0.3181818181818182, 0.1590909090909091, 0.29545454545454547, 0.6363636363636364, 0.25, 0.06818181818181818, 0.3181818181818182, 0.2727272727272727, 0.3181818181818182, 0.29545454545454547, 0.1590909090909091, 0.3409090909090909, 0.25, 0.11363636363636363]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "      <th>NN_Freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>Abstract not found</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe method statistical modeling based max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>describ a method for statist model base on max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>0.227273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional random fields natural lang...</td>\n",
              "      <td>scaling conditional random fields for natural ...</td>\n",
              "      <td>scale condit random field for natur languag pr...</td>\n",
              "      <td>Scaling conditional random field for natural l...</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses issue cooperation linguist...</td>\n",
              "      <td>the paper addresses the issue of cooperation b...</td>\n",
              "      <td>the paper address the issu of cooper between l...</td>\n",
              "      <td>The paper address the issue of cooperation bet...</td>\n",
              "      <td>0.227273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In natural language processing applications, D...</td>\n",
              "      <td>in most natural language processing applicatio...</td>\n",
              "      <td>In most natur languag process applications, de...</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents workbench built Priberam I...</td>\n",
              "      <td>this paper presents a workbench built by pribe...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This paper present a workbench built by Priber...</td>\n",
              "      <td>0.295455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language Processing (NLP) eff...</td>\n",
              "      <td>abstract—natural language processing (nlp) is ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abstract—Natural Language Processing (NLP) is ...</td>\n",
              "      <td>0.159091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>ABSTRACT After twenty yea...</td>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>ABSTRACT: After twenty years disfavor, technol...</td>\n",
              "      <td>abstract: after twenty years of disfavor, a te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ABSTRACT: After twenty year of disfavor, a tec...</td>\n",
              "      <td>0.340909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics frequently used stylometry cry...</td>\n",
              "      <td>text statistics are frequently used in stylome...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Text statistic are frequently used in stylomet...</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize experience using FrameNet two rat...</td>\n",
              "      <td>we summarize our experience using framenet in ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We summarize our experience using FrameNet in ...</td>\n",
              "      <td>0.113636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstract  ...   NN_Freq\n",
              "0                        Abstract not found       ...  ...  0.000000\n",
              "1                         describe a method for st...  ...  0.227273\n",
              "2                        Scaling conditional rando...  ...  0.090909\n",
              "3                        The paper addresses the i...  ...  0.227273\n",
              "4                        In most natural language ...  ...  0.181818\n",
              "..                                                ...  ...       ...\n",
              "95                       This paper presents a wor...  ...  0.295455\n",
              "96                       Abstract—Natural Language...  ...  0.159091\n",
              "97                       ABSTRACT: After twenty ye...  ...  0.340909\n",
              "98                       Text statistics are frequ...  ...  0.250000\n",
              "99                       We summarize our experien...  ...  0.113636\n",
              "\n",
              "[100 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ATpuYBmoxa",
        "outputId": "7ac0d838-460b-4b6a-f215-ba72a0d6021d"
      },
      "source": [
        "corpus=[]\r\n",
        "for line in df1[\"Lemmitization\"]:\r\n",
        "  string=\"\".join(line)\r\n",
        "  corpus.append(string)\r\n",
        "vec=TfidfVectorizer()\r\n",
        "print(corpus)\r\n",
        "x=vec.fit_transform(corpus)\r\n",
        "cNames=vec.get_feature_names()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Abstract not found', 'describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy model and describe how to implement this approach efficiently, using a example several problem in natural language processing.', 'Scaling conditional random field for natural language processing Terms and Conditions: Terms and Conditions: Copyright in work deposited in Minerva Access is retained by the', 'The paper address the issue of cooperation between linguistics and natural language processing (NLP), in general, and between linguistics and machine translation (MT), in particular. It focus on just one direction of such cooperation, namely application of linguistics to NLP, virtually', 'In most natural language processing applications, Description Logics have been used to encode in a knowledge base some syntactic, semantic, and pragmatic element needed to drive the semantic interpretation and the natural language generation processes. More recently, Description Logics have been', 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing task including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task', 'Natural Language Processing The subject of Natural Language Processing can be considered in both broad and narrow senses. In the broad sense, it cover processing issue at all level of natural language understanding, including speech recognition, syntactic and semantic analysis of sentence', 'Robots that interact with human face-to-face using natural language need to be responsive to the way human use language in those situations. We propose a psychologicallyinspired natural language processing system for robot which performs incremental semantic interpretation of spoken utterance', 'Natural language are language spoken by humans. Currently we are not yet at the point where these language in all of their unprocessed form can be understood by computers. Natural language processing is the collection of technique employed to try and accomplish that goal. The field of natural', 'ABSTRACT: Ambiguity can be referred a the ability of having more than one meaning or being understood in more than one way. Natural language are ambiguous, so computer are not able to understand language the way people do. Natural Language Processing (NLP) is concerned with the development', 'Introduction Statistical natural language processing (SNLP) is a field lying in the intersection of natural language processing and machine learning. SNLP di#ers from traditional natural language processing in that instead of having a linguist manually construct some model of a given linguistic', 'text directly (rather than e.g. title and abstracts), and suggests appropriate approach to doing this, with a focus on the role of natural language processing. The paper also comment on possible connection with data and knowledge retrieval, and concludes by emphasizing the importance of rigorous', 'ABSTRACT: Language is way of communicating your word Language help in understanding the world,we get a better insight of the world. Language help speaker to be a vague or a precise a they like. NLP Stands for natural language processing.. Natural language are those language that are spoken', 'We report experiment on the use of standard natural language processing (NLP) tool for the analysis of music lyrics. A significant amount of music audio ha lyrics. Lyrics encode an important part of the semantics of a song, therefore their analysis complement that of acoustic and cultural', 'this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach ha been shown for a number of task to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging', 'This paper focus on connectionist model in natural language processing. We briefly present and discus several aspect of high level task which recently have been approached with connectionism, either with localist or parallel distributed processing models. Several interesting architecture', 'process of language understanding. This is a new approach in natural language processing based on the deterministic chaotic behavior of dynamical systems. 1', 'this paper (see [Schank 86] for a theoretical discussion and [Kass 86] and [Leake and Owens 86] for brief discussion of a program built around these .principles); the goal here is simply to point out how our interest in natural language processing ha led u naturally, and indeed inevitably', 'Objectives To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. Target audience This tutorial target the medical informatics generalist who ha limited acquaintance with the principle behind NLP and/or limited knowledge of the current state', 'This paper briefly describes the current implementation status of an intelligent information retrieval system, MARIE, that employ natural language processing techniques. Descriptive caption are used to iden- tify photographic image concerning various military projects. The caption are parsed', 'based and literature resources. We describe here a system for agent directed natural language processing to extract information from journal articles. An interface wa developed to permit curation of the NLP result and deposition of accepted result into a knowledge base. Motivation: The advent of high', 'to evaluation in speech processing. Part 2 survey significant evaluation work done so far, for instance in machine translation, and discus the particular problem of generic system evaluation. The conclusion is that evaluation strategy and technique for NLP need much more development, in particular', 'similar to the way human intuitively do in order to eliminate noisy content. In this paper, we describe a combination of HTML DOM analysis and Natural Language Processing (NLP) technique for automated extraction of main article with associated image from web pages.', 'Abstract-- Natural Language Processing is a theoretically motivated range of computational technique for analysing and representing naturally occurring text at one or more level of linguistic analysis for the purpose of achieving human-like language processing for a range of task', 'This paper review the process involved in Natural Language Processing (NLP). It then demonstrates the various kind of choice that need be taken during the execution of the word morphology, the syntactic text analysis, or text generation components. It compare the time complexity', 'This article focus on the derivation of large lexicon for natural language processing. We describe the development of a dictionary support environment linking a restructured version of the Longman Dictionary of Contemporary English to natural language processing systems. The process', 'We introduce a method for analyzing the complexity of natural language processing tasks, and for predicting the difficulty new NLP tasks. Our complexity measure are derived from the Kolmogorov complexity of a class of automaton — meaning automata, whose purpose is to extract relevant piece', ', sounds, text and motion. The technique developed from deep learning research have already been impacting the research of natural language process. This paper review the recent research on deep learning, it application and recent development in natural language processing. 1', 'This is an author-produced version of a paper published in The', 'Abstract—Natural language processing (NLP) is the application of automated parsing and machine learning technique to analyze standard text. Applications of NLP to requirement engineering include extraction of ontology from a requirement specification, and use of NLP to verify the consistency', 'statistical baseline including: the forgiving nature but broad coverage of the typical retrieval task; the lack of good weighting scheme for compound index terms; and the implicit linguistic processing inherent in the statistical methods. Natural language processing technique may be more important', 'Work in computational linguistics began very soon after the development of the first computer (Booth, Brandwood and Cleave 1958), yet in the intervening four decade there ha been a pervasive feeling that progress in computer understanding of natural language ha not been commensurate', 'the voice recognition for a natural language (Tamil) by combining the digital and mathematical knowledge using MFCC and DTW to extract and match the feature to improve the accuracy for better performance.', 'Abstract: Testing against natural language requirement is the standard approach for system and acceptance testing. This test is often performed by an independent test organization unfamiliar with the application area. The only thing the tester have to go by are the written requirements. So', 'Abstract not found', 'conversational partners. But it also provides u with information about being creative, making associations, storytelling and language use. Many more subtlety in face-to-face and multiparty interaction can be added, such a using humor to persuade and dominate, to soften or avoid a face threatening act', 'Abstract not found', 'In recent years, machine learning (ML) ha been used more and more to solve complex task in different disciplines, ranging from Data Mining to Information', 'We argue that manual and automatic thesaurus are alternative resource for the same NLP tasks. This involves the radical step of interpreting manual thesaurus a classification of word rather than word senses: the case for this is made. The range of role for thesaurus within NLP is briefly presented and the WASPS thesaurus is introduced. Thesaurus evaluation is now becoming urgent. A range of evaluation strategies, all embedded within NLP tasks, is proposed.', 'Introduction Patterns in music have been the object of intensive study in the past years. \\\\One of the purpose of analyzing musical structure and form is to discover the pattern that are explicit or implicit in musical works\" Simon [13]. Patterns comprise periodicity, make use of alphabets, can be compound (made up of subpatterns) and posse phrase structure with various form of punctuation. Traditionally, composer have employed pattern propagation intuitively, but algorithmic composition technique allow the pattern propagation to be formalized, albeit a high level. During composition, all the musical pattern evolve according to the rule and constraint specied at the design stage. In jazz improvisation, the musician invents a solo guided by a progression of chord (the changes). One approach [1] to learn improvising is to memorize pattern (short chunk of music) that t sub-progressions, and to concatenate them to form a whole solo that t a whole progression. One', 'Abstract Many information retrieval(IR) system retrieve relevant document based on exact matching of keywords between a query and documents. This method degrades precision rate. In order to solve the problem, we collected semantically related word and assigned semantic relationship used in general thesaurus and a special relationship called keyfact term(FT) manually. In addition to the semantic knowledge, we automatically constructed statistic knowledge based on the concept of mutual information. Keyfact is an extended concept of keyword represented by noun and compound noun. Keyfact can be a verb and an adjective including subject or object term. We first retrieved relevant document with original query using tf * idf weighting formula and then an expanded query including keyfacts is used in both second document ranking and word sense disambiguating. So we made an improvement in precision rate using keyfact network. 1', 'this paper we argue that questionanswering (QA) over technical domain is distinctly different from TREC-based QA or Web-based QA and it cannot benefit lom data-intensive approach', 'Universit&quot;at de Saarlandes', 'Proceedings of the Workshop on', 'uni-hamburg.de', 'Abstract not found', 'Abstract not found', 'SRI ha developed a new architecture for integrating speech and natural-language processing that applies linguistic constraint during recognition by incrementally expanding the state-transition network embodied in a unification grammar. We compare this dynamic-gralnlnar-network (DGN) approach', 'This chapter considers the revolution that ha taken place in natural language processing research over the last five years. It begin by providing a brief guide to the structure of the field and then present a caricature of two competing paradigm of 1980s NLP research and indicates the reason', 'visual development environment to support the visual assembly, execution and analysis of modular natural language processing systems. The visual model is an executable data flow program graph, automatically synthesised from data dependency declaration of language processing modules. The graph', 'In this Chapter the basic us of Description Logics for Natural Language Processing will be analysed, together with a little bit of history, and the role of Description Logics in the current state of the art in computational linguistics will be pointed out. 18.1 Introduction Since the early day', 'We applied a structure learning model, Max-Margin Structure (MMS), to natural language processing (NLP) tasks, where the aim is to capture the latent relationship within the output language domain. We formulate this model a an extension of multi–class Support Vector Machine (SVM) and present a', '-mation Infrastructure, digital libraries, networked services, digital convergence or intelligent agents. This attention is moving natural language processing along the critical path for all kind of novel applications. This article will mention a number of successful application of natural language processing (NLP', 'Over the last few years, a number of area of natural language processing have begun applying graph-based techniques. These include, among others, text summarization, syntactic parsing, word sense disambiguation, ontology construction, sentiment and subjectivity analysis, text clustering', 'In Natural Language Processing (NLP), research result from software engineering and software technology have often been neglected.', 'of kernelized sorting to increase it robustness and performance on several Natural Language Processing (NLP) tasks: document matching from parallel and comparable corpora, machine transliteration and even image processing. Empirically we show that, on these tasks, a semi-supervised variant of kernelized', 'will be structured. In the word of statistical natural language processing, we need a sophisticated statistical model of the basic elements, such a word or phrases, to be combined with the structural modeling such a syntactic parsing or dependency analysis. Since the basic property of these element', 'In this paper, we describe a framework for developing probabilistic classifier in natural language processing. Our focus is on formulating model that capture the most important interdependency among features, to avoid overfitting the data while also characterizing the data well. The class', 'Many Natural Language Processing (NLP) technique have been used in Information Retrieval. The result are not encouraging. Simple method (stopwording, porter-style stemming, etc.) usually yield significant improvements, while higher-level processing (chunking, parsing, word sense disambiguation', 'Abstract- This paper explains the information retrieval using natural language processing for Malayalam language in these basic', 'in the state of the art plan recognition systems. This paper will outline the relation between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research result in NLP and argue for their applicability to PR. 1', 'in the state of the art plan recognition systems. This paper will outline the relation between natural language processing(NLP) and plan recognition(PR), argue that each of them can effectively inform the other, and then focus on key recent research result in NLP and argue for their applicability to PR. 1', 'Information retrieval is the process of finding the document in a document collection that satisfies the information need of the user. The document are natural language constructs, and the motivation of this work is to investigate how natural language processing can be used to improve', 'of logic programming within both natural language research and machine learning, we point out opportunity for induction of linguistic knowledge within logic (programming). Keywords: inductive logic programming, natural language processing, logic programming, machine learning. 1 Introduction There is a', 'What is a statistical method and how can it be used in natural language processing (NLP)? In this paper, we start from a definition of NLP a concerned with the design and implementation of effective natural language input and output component for computational systems. We distinguish three', 'In this report, some collaborative work between the field of Machine Learning (ML) and Natural Language Processing (NLP) is presented. The document is structured in two parts. The first part includes a superficial but comprehensive survey covering the state--of--the--art of machine learning', 'Abstract. This thesis examines the use of machine learning technique in various task of natural language processing, mainly for the task of information extraction from texts. The objective are the improvement of adaptability of information extraction system to new thematic do-mains (or even', 'This chapter examines the application of natural language processing to computerassisted language learning including the history of work in this field over the last thirtyfive year but with a focus on current development and opportunities. 36.1', 'Traditional approach tointerpretation in natural language processing typically fall into one of three classes: syntax-driven, semantics-driven, or frame/task based. Syntax-driven approach use a domain-independent grammar to drive the interpretation process and produce a global parse', 'Natural Language Processing (NLP) is a very large and diverse subtopic of artificial intelligence. As a result, NLP itself ha many subtopics including optical character recognition, text to speech translators, foreign language reading and writing aids, machine translation, and speech recognition', 'Probabilistic finite-state string transducer (FSTs) are extremely popular in natural language processing, due to powerful generic method for applying, composing, and learning them. Unfortunately, FSTs are not a good fit for much of the current work on probabilistic modeling for machine', 'ABSTRACT. In this special issue of TAL, we look at the fundamental principle underlying evaluation in natural language processing. We adopt a global point of view that go beyond the horizon of a single evaluation campaign or a particular protocol. After a brief review of history and terminology', 'Abstract not found', 'Natural language processing system (NLP) that extract clinical information from textual report were shown to be effective for limited domain and for particular applications. Because an NLP system typically requires substantial resource to develop, it is beneficial if it is designed to be easily', 'fact form a link between IE, a recent development in Natural Language Processing, and logic programming with Prolog. 1', 'We describe a single convolutional neural network architecture that, given a sentence, output a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar word and the likelihood that the sentence make sense (grammatically', 'We developed a prototype information retrieval system which us advanced natural language processing technique to enhance the effectiveness of traditional key-word based document retrieval. The backbone of our system is a statistical retrieval engine which performs automated indexing', 'Abstract not found', 'In this paper we will discus several issue and requirement for enabling natural language processing system to become context-adaptive. Given the fact that emerging system feature speaker independent continuous speech recognition restricted to individual domain and are equipped with syntactic', 'In Fall 2004 I introduced a new course called Applied Natural Language Processing, in which student acquire an understanding of which text analysis technique are currently feasible for practical applications.', 'Abstract not found', 'Abstract: Natural language processing is the study of mathematical and computational modelling of various aspect of language and the improvement of a wide range of systems. Natural language is any language that arises a an innate facility for language possessed by the human intellect; it may', 'Natural Language Processing (NLP), which is a branch of artificial intelligence, includes speech synthesis, Speech recognition, and Machine translation. Natural Language Processing ha a wide range of application in the Indian context. Most of the rural Indian community is unable to make use', 'An Evaluation of LOLITA and related Natural Language Processing Systems Paul Callaghan Submitted to the University of Durham for the degree of Ph.D., August 1997 --------------------- This research address the question, \"how do we evaluate system like LOLITA?\" LOLITA is the Natural', 'Previous work demonstrated that Web count can be used to approximate bigram counts, suggesting that Web-based frequency should be useful for a wide variety of Natural Language Processing (NLP) tasks. However, only a limited number of task have so far been tested using Web-scale data set', 'This chapter examines the application of natural language processing to computerassisted language learning including the history of work in this field over the last thirtyfive year but with a focus on current development and opportunities. 16.1 Introduction This chapter focus on application', 'This paper describes a natural language system which improves it own performance through learning. The system process short English narrative and is able to acquire, from a single narrative, a new schema for a stereotypical set of actions. During the understanding process, the system attempt', 'We classify and review current approach to software infrastructure for research, development and delivery of NLP systems. The task', 'Confidence measure are a practical solution for improving the usefulness of Natural Language Processing applications. Confidence estimation is a generic machine learning approach for deriving confidence measures. We give an overview of the application of confidence estimation in various field', '! lex-sign sense-id : sense-id dictionary ? = \"LDOCE\" ! lex-sign sense-id : sense-id ldb-entry-no ? = \"12364\" ! lex-sign sense-id : sense-id sense-no ? = \"0\". When loaded into the LKB, (9) will be expanded into a fully-fledged representation for the transitive use of experience; by integrating word-specific information provided by (9) with the information encoded by the LKB type strict-trans-sign. Thus, although neither LDOCE, LLCE or the earlier subcategorised lexicon contain all the information about psychological verb defined in Sanfilippo&aposs type system, by using the conjunction of information available from all three, it proved possible to effectively enrich this information at the same time a mapping it into a formal representation. 4.2.5 Towards a Multilingual LKB A goal of ACQUILEX is to demonstrate that an LKB can be produced that usefully exploit various MRD source and integrates multilingual information. The use of a common LRL with a common type system, make it possi...', 'We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural lan-guage analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and govern-ment user of open source NLP technol-ogy. We suggest', 'Gaussian Processes (GPs) are a powerful mod-elling framework incorporating kernel and Bayesian inference, and are recognised a state-of-the-art for many machine learning tasks.', ': A fundamental issue in natural language processing is the prerequisite of an enormous quantity of preprogrammed knowledge concerning both the language and the domain under examination. Manual acquisition of this knowledge is tedious and error prone. Development of an automated acquisition', '\" that support sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguis- tic resources. This paper present the result of our experience in designing and using the upper model in a variety of application over the past 5 year', 'into the same or neighboring map nodes. Nodes may thus be viewed a word categories. Although no a priori information about class is given, during the self-organizing process a model of the word class emerges. The central topic of the thesis is the use of the SOM in natural language processing. The approach', 'This paper present a workbench built by Priberam Informática for the development of the company’s natural language processing technology. This workbench includes a set of linguistic resource and software tool that have been applied in a considerable number of practical purposes, covering', 'Abstract—Natural Language Processing (NLP) is an effective approach for bringing improvement in educational setting. Implementing NLP involves initiating the process of learning through the natural acquisition in the educational systems. It is based on effective approach for providing a solution', 'ABSTRACT: After twenty year of disfavor, a technology ha returned which imitates the process of the brain. Natural language experiment (Sejnowski & Rosenberg: 1986) demonstrate that neural network computing architecture can learn from actual spoken language, observe rule of pronunciation', 'Text statistic are frequently used in stylometry and cryptography studies. In this paper, some text statistic tool are developed in ISO Prolog for natural language processing. Details are given on the usage of 21 user-callable predicates. Logic and limitation of the program are also discussed', 'We summarize our experience using FrameNet in two rather different project in natural language processing (NLP). We conclude that NLP can benefit from FrameNet in different ways, but we sketch some problem that need to be overcome. 1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYOnqNr0my0Z",
        "outputId": "3ac32e74-bb9e-451a-98fb-6ae55cb9e079"
      },
      "source": [
        "print(type(x))\r\n",
        "tfidf=x.toarray()\r\n",
        "print(type(tfidf))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "1lMNzoMmm5qg",
        "outputId": "69b172d0-d288-425c-ff23-ababbd8dfd33"
      },
      "source": [
        "#2.1 calculation of tf-idf values\r\n",
        "tfidf=pd.DataFrame(tfidf)\r\n",
        "tfidf.columns=cNames\r\n",
        "tfidf"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>12364</th>\n",
              "      <th>13</th>\n",
              "      <th>16</th>\n",
              "      <th>18</th>\n",
              "      <th>1958</th>\n",
              "      <th>1980s</th>\n",
              "      <th>1986</th>\n",
              "      <th>1997</th>\n",
              "      <th>2004</th>\n",
              "      <th>21</th>\n",
              "      <th>36</th>\n",
              "      <th>86</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>abstract</th>\n",
              "      <th>abstracts</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>accepted</th>\n",
              "      <th>access</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>according</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>achieved</th>\n",
              "      <th>achieving</th>\n",
              "      <th>acoustic</th>\n",
              "      <th>acquaintance</th>\n",
              "      <th>acquilex</th>\n",
              "      <th>acquire</th>\n",
              "      <th>acquisition</th>\n",
              "      <th>act</th>\n",
              "      <th>actions</th>\n",
              "      <th>actual</th>\n",
              "      <th>adaptability</th>\n",
              "      <th>adaptive</th>\n",
              "      <th>added</th>\n",
              "      <th>addition</th>\n",
              "      <th>address</th>\n",
              "      <th>adjective</th>\n",
              "      <th>adopt</th>\n",
              "      <th>...</th>\n",
              "      <th>viewed</th>\n",
              "      <th>virtually</th>\n",
              "      <th>visual</th>\n",
              "      <th>voice</th>\n",
              "      <th>wa</th>\n",
              "      <th>wasps</th>\n",
              "      <th>way</th>\n",
              "      <th>ways</th>\n",
              "      <th>we</th>\n",
              "      <th>web</th>\n",
              "      <th>weighting</th>\n",
              "      <th>well</th>\n",
              "      <th>were</th>\n",
              "      <th>what</th>\n",
              "      <th>when</th>\n",
              "      <th>where</th>\n",
              "      <th>which</th>\n",
              "      <th>while</th>\n",
              "      <th>who</th>\n",
              "      <th>whole</th>\n",
              "      <th>whose</th>\n",
              "      <th>wide</th>\n",
              "      <th>widely</th>\n",
              "      <th>will</th>\n",
              "      <th>with</th>\n",
              "      <th>within</th>\n",
              "      <th>without</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workbench</th>\n",
              "      <th>works</th>\n",
              "      <th>workshop</th>\n",
              "      <th>world</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yet</th>\n",
              "      <th>yield</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.493371</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.080828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.236031</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.174025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.189648</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425299</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.107714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.205461</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.107306</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.205461</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147592</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.167211</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.178141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.20132</td>\n",
              "      <td>0.263745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1158 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    12364   13   16   18  1958  ...      year  years  yet  yield  your\n",
              "0     0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "1     0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "2     0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "3     0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "4     0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "..    ...  ...  ...  ...   ...  ...       ...    ...  ...    ...   ...\n",
              "95    0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "96    0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "97    0.0  0.0  0.0  0.0   0.0  ...  0.167211    0.0  0.0    0.0   0.0\n",
              "98    0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "99    0.0  0.0  0.0  0.0   0.0  ...  0.000000    0.0  0.0    0.0   0.0\n",
              "\n",
              "[100 rows x 1158 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bXV_nfrnMSh"
      },
      "source": [
        "#2.2 Cosine Similarity\r\n",
        "\r\n",
        "Text='Many students have sat in the tutoring center working through problem after problem on their math homework wondering why they cannot solve them. It turns out that much of classical mathematical logic is quite different from the way humans reason. In fact, without a rigorous background in mathematical logic it is difficult for humans to reason according to the norms of formal mathematics. This project assumes Husserl’s idea that people reason to an interpretation and from an interpretation and includes new insights into the way in which humans construct logical frameworks. Using Stenning and Van Lambalgen’s theory that much of human reasoning is about process planning we will apply closed-world reasoning to the construction of mathematical proofs via logic programming. The resulting model will help explicate the problems student have in creating proofs'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "-THYpAxDnZoN",
        "outputId": "e6baefd2-e918-4595-c540-014ea4cadc1c"
      },
      "source": [
        "cosine_values=[]\r\n",
        "for line in df1[\"Lemmitization\"]:\r\n",
        "  list1=[]\r\n",
        "  list2=[]\r\n",
        "  value=0\r\n",
        "  text_tokenized=nltk.tokenize.word_tokenize(Text)\r\n",
        "  line_tokenized=nltk.tokenize.word_tokenize(line)\r\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "  text_unstop={word for word in text_tokenized if word not in stop_words}\r\n",
        "  line_unstop={word for word in line_tokenized if word not in stop_words}\r\n",
        "  vector=text_unstop.union(line_unstop)\r\n",
        "\r\n",
        "  for word in vector: \r\n",
        "    if word in text_unstop: \r\n",
        "      list1.append(1)\r\n",
        "    else: \r\n",
        "      list1.append(0) \r\n",
        "    if word in line_unstop: \r\n",
        "      list2.append(1) \r\n",
        "    else: \r\n",
        "      list2.append(0) \r\n",
        "  for i in range(len(vector)): \r\n",
        "    value+= list1[i]*list2[i] \r\n",
        "  cosine_calc = value/float((sum(list1)*sum(list2))**0.5)\r\n",
        "  cosine_values.append(cosine_calc)\r\n",
        "\r\n",
        "print(cosine_values)\r\n",
        "df1['Cosine Values']=cosine_values\r\n",
        "df1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.09494253265550827, 0.0, 0.09309881282314554, 0.09690031662230184, 0.06394568344792313, 0.09135849464367089, 0.09494253265550827, 0.07590680990538862, 0.09494253265550827, 0.0712068994916312, 0.0852609112638975, 0.09494253265550827, 0.041959067914834454, 0.08141255010133056, 0.0866702780034879, 0.11867816581938533, 0.020659216918960218, 0.04263045563194875, 0.08391813582966891, 0.041318433837920436, 0.11214032600406415, 0.10489766978708615, 0.0, 0.10657613907987187, 0.09309881282314554, 0.06611394640818899, 0.1211253957778773, 0.09690031662230184, 0.045679247321835446, 0.021667569500871973, 0.022839623660917723, 0.05060453993692575, 0.06728419560243849, 0.0, 0.06500270850261593, 0.0, 0.10614897848685505, 0.07601486235852868, 0.052423243852793656, 0.09118634733669016, 0.02966954145484633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04485613040162566, 0.0852609112638975, 0.09309881282314554, 0.06728419560243849, 0.06293860187225168, 0.06500270850261593, 0.06394568344792313, 0.08900862436453899, 0.04333513900174395, 0.09494253265550827, 0.11018991068031499, 0.08263686767584087, 0.03425943549137658, 0.06728419560243849, 0.06728419560243849, 0.1012090798738515, 0.09690031662230184, 0.06611394640818899, 0.10176568762666319, 0.11018991068031499, 0.05060453993692575, 0.08971226080325131, 0.041959067914834454, 0.06500270850261593, 0.06293860187225168, 0.0, 0.044075964272126, 0.15321285325897388, 0.021315227815974374, 0.04654940641157277, 0.0, 0.06728419560243849, 0.1211253957778773, 0.0, 0.07267523746672638, 0.06500270850261593, 0.04333513900174395, 0.04070627505066528, 0.04949221641439498, 0.13703774196550633, 0.08900862436453899, 0.023735633163877067, 0.05210648419239438, 0.08391813582966891, 0.053074489243427524, 0.02474610820719749, 0.06500270850261593, 0.10833784750435987, 0.11214032600406415, 0.0712068994916312, 0.06105941257599791, 0.06500270850261593, 0.11867816581938534]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "      <th>NN_Freq</th>\n",
              "      <th>Cosine Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract not found       ...</td>\n",
              "      <td>Abstract found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>abstract not found</td>\n",
              "      <td>Abstract not found</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe a method for st...</td>\n",
              "      <td>describe method statistical modeling based max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>describ a method for statist model base on max...</td>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.094943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional rando...</td>\n",
              "      <td>Scaling conditional random fields natural lang...</td>\n",
              "      <td>scaling conditional random fields for natural ...</td>\n",
              "      <td>scale condit random field for natur languag pr...</td>\n",
              "      <td>Scaling conditional random field for natural l...</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses the i...</td>\n",
              "      <td>The paper addresses issue cooperation linguist...</td>\n",
              "      <td>the paper addresses the issue of cooperation b...</td>\n",
              "      <td>the paper address the issu of cooper between l...</td>\n",
              "      <td>The paper address the issue of cooperation bet...</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.093099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In most natural language ...</td>\n",
              "      <td>In natural language processing applications, D...</td>\n",
              "      <td>in most natural language processing applicatio...</td>\n",
              "      <td>In most natur languag process applications, de...</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.096900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents a wor...</td>\n",
              "      <td>This paper presents workbench built Priberam I...</td>\n",
              "      <td>this paper presents a workbench built by pribe...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This paper present a workbench built by Priber...</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.112140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language...</td>\n",
              "      <td>Abstract—Natural Language Processing (NLP) eff...</td>\n",
              "      <td>abstract—natural language processing (nlp) is ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Abstract—Natural Language Processing (NLP) is ...</td>\n",
              "      <td>0.159091</td>\n",
              "      <td>0.071207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>ABSTRACT After twenty yea...</td>\n",
              "      <td>ABSTRACT: After twenty ye...</td>\n",
              "      <td>ABSTRACT: After twenty years disfavor, technol...</td>\n",
              "      <td>abstract: after twenty years of disfavor, a te...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ABSTRACT: After twenty year of disfavor, a tec...</td>\n",
              "      <td>0.340909</td>\n",
              "      <td>0.061059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics are frequ...</td>\n",
              "      <td>Text statistics frequently used stylometry cry...</td>\n",
              "      <td>text statistics are frequently used in stylome...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Text statistic are frequently used in stylomet...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.065003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize our experien...</td>\n",
              "      <td>We summarize experience using FrameNet two rat...</td>\n",
              "      <td>we summarize our experience using framenet in ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>We summarize our experience using FrameNet in ...</td>\n",
              "      <td>0.113636</td>\n",
              "      <td>0.118678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstract  ... Cosine Values\n",
              "0                        Abstract not found       ...  ...      0.000000\n",
              "1                         describe a method for st...  ...      0.094943\n",
              "2                        Scaling conditional rando...  ...      0.000000\n",
              "3                        The paper addresses the i...  ...      0.093099\n",
              "4                        In most natural language ...  ...      0.096900\n",
              "..                                                ...  ...           ...\n",
              "95                       This paper presents a wor...  ...      0.112140\n",
              "96                       Abstract—Natural Language...  ...      0.071207\n",
              "97                       ABSTRACT: After twenty ye...  ...      0.061059\n",
              "98                       Text statistics are frequ...  ...      0.065003\n",
              "99                       We summarize our experien...  ...      0.118678\n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0EvXwkfnwuq"
      },
      "source": [
        "df1.to_csv('assignment4file.csv')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link:\n",
        "  https://github.com/Vamsikrishna1804/Vamsikrishnabharghava_INFO5731_Spring2021/blob/main/assignment4file1.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}