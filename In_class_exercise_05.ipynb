{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vamsikrishna1804/Vamsikrishnabharghava_INFO5731_Spring2021/blob/main/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6_kjKqYPwvx"
      },
      "source": [
        "## The fifth In-class-exercise (2/23/2021, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxabvLvPPwv3"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbF9C6Ztbstc",
        "outputId": "1d22025f-b51b-4fe2-b05b-95673ef60801"
      },
      "source": [
        "import requests\r\n",
        "import csv\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "import urllib.request\r\n",
        "import nltk\r\n",
        "from textblob import TextBlob\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('universal_tagset')\r\n",
        "nltk.download('punkt')\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def page(page_number=None):\r\n",
        "  if page_number==0:\r\n",
        "    url=\"https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc\"\r\n",
        "  elif page_number==10:\r\n",
        "    url=\"https://citeseerx.ist.psu.edu/search;jsessionid=5CBA9DFA8F9AA1564F602D9FDD277064?q=natural+language+processing&t=doc&sort=rlv&start=10\"\r\n",
        "  else:\r\n",
        "    url=f\"https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page={page_number}\"\r\n",
        "  print(url)\r\n",
        "  with urllib.request.urlopen(url) as f:\r\n",
        "   webpage=f.read()\r\n",
        "   soup=BeautifulSoup(webpage)\r\n",
        "  op=[]\r\n",
        "  table2=soup.find_all('div',{'class':'snippet'})\r\n",
        "  for item in table2:\r\n",
        "    op.append({item.text})\r\n",
        "  return op\r\n",
        "\r\n",
        "def abstracts():\r\n",
        "    final_op = []\r\n",
        "    #text_data=[]\r\n",
        "    for pages in range(0, 501,10):\r\n",
        "      print(f\"Extracting page: {pages}\")\r\n",
        "      final_op = final_op + page(page_number=pages)\r\n",
        "\r\n",
        "    df = pd.DataFrame(final_op)\r\n",
        "    df.columns=['Abstract Data']\r\n",
        "    df.to_csv(\"abstract_list.csv\")\r\n",
        "    return df\r\n",
        "\r\n",
        "df=abstracts()\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Extracting page: 0\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc\n",
            "Extracting page: 10\n",
            "https://citeseerx.ist.psu.edu/search;jsessionid=5CBA9DFA8F9AA1564F602D9FDD277064?q=natural+language+processing&t=doc&sort=rlv&start=10\n",
            "Extracting page: 20\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=20\n",
            "Extracting page: 30\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=30\n",
            "Extracting page: 40\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=40\n",
            "Extracting page: 50\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=50\n",
            "Extracting page: 60\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=60\n",
            "Extracting page: 70\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=70\n",
            "Extracting page: 80\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=80\n",
            "Extracting page: 90\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=90\n",
            "Extracting page: 100\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=100\n",
            "Extracting page: 110\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=110\n",
            "Extracting page: 120\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=120\n",
            "Extracting page: 130\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=130\n",
            "Extracting page: 140\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=140\n",
            "Extracting page: 150\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=150\n",
            "Extracting page: 160\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=160\n",
            "Extracting page: 170\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=170\n",
            "Extracting page: 180\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=180\n",
            "Extracting page: 190\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=190\n",
            "Extracting page: 200\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=200\n",
            "Extracting page: 210\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=210\n",
            "Extracting page: 220\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=220\n",
            "Extracting page: 230\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=230\n",
            "Extracting page: 240\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=240\n",
            "Extracting page: 250\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=250\n",
            "Extracting page: 260\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=260\n",
            "Extracting page: 270\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=270\n",
            "Extracting page: 280\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=280\n",
            "Extracting page: 290\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=290\n",
            "Extracting page: 300\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=300\n",
            "Extracting page: 310\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=310\n",
            "Extracting page: 320\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=320\n",
            "Extracting page: 330\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=330\n",
            "Extracting page: 340\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=340\n",
            "Extracting page: 350\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=350\n",
            "Extracting page: 360\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=360\n",
            "Extracting page: 370\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=370\n",
            "Extracting page: 380\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=380\n",
            "Extracting page: 390\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=390\n",
            "Extracting page: 400\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=400\n",
            "Extracting page: 410\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=410\n",
            "Extracting page: 420\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=420\n",
            "Extracting page: 430\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=430\n",
            "Extracting page: 440\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=440\n",
            "Extracting page: 450\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=450\n",
            "Extracting page: 460\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=460\n",
            "Extracting page: 470\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=470\n",
            "Extracting page: 480\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=480\n",
            "Extracting page: 490\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=490\n",
            "Extracting page: 500\n",
            "https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=rlv&start/?page=500\n",
            "                                         Abstract Data\n",
            "0                                          \"...   ...\"\n",
            "1    \"... The concept of maximum entropy can be tra...\n",
            "2    \"... Scaling conditional random fields for nat...\n",
            "3    \"... The paper addresses the issue of cooperat...\n",
            "4    \"... In most natural language processing appli...\n",
            "..                                                 ...\n",
            "505  \"... We propose a unified neural network archi...\n",
            "506  \"... Natural Language Processing The subject o...\n",
            "507  \"... Robots that interact with humans face-to-...\n",
            "508  \"... Natural languages are languages spoken by...\n",
            "509  \"...  ABSTRACT: Ambiguity can be referred as t...\n",
            "\n",
            "[510 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U10axjm-FKDO",
        "outputId": "d5622d8a-7e40-4b91-da78-85dd3974816a"
      },
      "source": [
        "#Remove noise, such as special characters and punctuations.\r\n",
        "df1=pd.read_csv('abstract_list.csv')\r\n",
        "df1['Special characters and Punctutation Removal'] = df1['Abstract Data'].str.replace('[,.~`\\?!@#$%^&\"*-/:;...()]','')\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Removing Numbers\r\n",
        "df1['Removal of numbers'] = df1['Abstract Data'].str.replace('\\d+', '')\r\n",
        "print(df1)\r\n",
        "\r\n",
        "# Remove stopwords by using the stopwords list.\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop=stopwords.words('english')\r\n",
        "df1['Removal of Stop words']=df1['Abstract Data'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Lowercase all texts\r\n",
        "df1['Lower_case'] = df1['Abstract Data'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "df1.to_csv(\"cleandata_list.csv\")\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Stemming.\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "st=PorterStemmer()\r\n",
        "df1['Stemming']=df1['Abstract Data'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "print(df1)\r\n",
        "\r\n",
        "#Lemmatization.\r\n",
        "from textblob import Word\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "df1['Lemmitization']=df1['Abstract Data'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "print(df1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Unnamed: 0  ...        Special characters and Punctutation Removal\n",
            "0             0  ...                                                   \n",
            "1             1  ...   The concept of maximum entropy can be traced ...\n",
            "2             2  ...   Scaling conditional random fields for natural...\n",
            "3             3  ...   The paper addresses the issue of cooperation ...\n",
            "4             4  ...   In most natural language processing applicati...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...   We propose a unified neural network architect...\n",
            "506         506  ...   Natural Language Processing The subject of Na...\n",
            "507         507  ...   Robots that interact with humans facetoface u...\n",
            "508         508  ...   Natural languages are languages spoken by hum...\n",
            "509         509  ...    ABSTRACT Ambiguity can be referred as the ab...\n",
            "\n",
            "[510 rows x 3 columns]\n",
            "     Unnamed: 0  ...                                 Removal of numbers\n",
            "0             0  ...                                        \"...   ...\"\n",
            "1             1  ...  \"... The concept of maximum entropy can be tra...\n",
            "2             2  ...  \"... Scaling conditional random fields for nat...\n",
            "3             3  ...  \"... The paper addresses the issue of cooperat...\n",
            "4             4  ...  \"... In most natural language processing appli...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...  \"... We propose a unified neural network archi...\n",
            "506         506  ...  \"... Natural Language Processing The subject o...\n",
            "507         507  ...  \"... Robots that interact with humans face-to-...\n",
            "508         508  ...  \"... Natural languages are languages spoken by...\n",
            "509         509  ...  \"...  ABSTRACT: Ambiguity can be referred as t...\n",
            "\n",
            "[510 rows x 4 columns]\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "     Unnamed: 0  ...                              Removal of Stop words\n",
            "0             0  ...                                          \"... ...\"\n",
            "1             1  ...  \"... The concept maximum entropy traced back a...\n",
            "2             2  ...  \"... Scaling conditional random fields natural...\n",
            "3             3  ...  \"... The paper addresses issue cooperation lin...\n",
            "4             4  ...  \"... In natural language processing applicatio...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...  \"... We propose unified neural network archite...\n",
            "506         506  ...  \"... Natural Language Processing The subject N...\n",
            "507         507  ...  \"... Robots interact humans face-to-face using...\n",
            "508         508  ...  \"... Natural languages languages spoken humans...\n",
            "509         509  ...  \"... ABSTRACT: Ambiguity referred ability one ...\n",
            "\n",
            "[510 rows x 5 columns]\n",
            "     Unnamed: 0  ...                                         Lower_case\n",
            "0             0  ...                                          \"... ...\"\n",
            "1             1  ...  \"... the concept of maximum entropy can be tra...\n",
            "2             2  ...  \"... scaling conditional random fields for nat...\n",
            "3             3  ...  \"... the paper addresses the issue of cooperat...\n",
            "4             4  ...  \"... in most natural language processing appli...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...  \"... we propose a unified neural network archi...\n",
            "506         506  ...  \"... natural language processing the subject o...\n",
            "507         507  ...  \"... robots that interact with humans face-to-...\n",
            "508         508  ...  \"... natural languages are languages spoken by...\n",
            "509         509  ...  \"... abstract: ambiguity can be referred as th...\n",
            "\n",
            "[510 rows x 6 columns]\n",
            "     Unnamed: 0  ...                                           Stemming\n",
            "0             0  ...                                          \"... ...\"\n",
            "1             1  ...  \"... the concept of maximum entropi can be tra...\n",
            "2             2  ...  \"... scale condit random field for natur langu...\n",
            "3             3  ...  \"... the paper address the issu of cooper betw...\n",
            "4             4  ...  \"... In most natur languag process application...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...                                                NaN\n",
            "506         506  ...                                                NaN\n",
            "507         507  ...                                                NaN\n",
            "508         508  ...                                                NaN\n",
            "509         509  ...                                                NaN\n",
            "\n",
            "[510 rows x 7 columns]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "     Unnamed: 0  ...                                      Lemmitization\n",
            "0             0  ...                                          \"... ...\"\n",
            "1             1  ...  \"... The concept of maximum entropy can be tra...\n",
            "2             2  ...  \"... Scaling conditional random field for natu...\n",
            "3             3  ...  \"... The paper address the issue of cooperatio...\n",
            "4             4  ...  \"... In most natural language processing appli...\n",
            "..          ...  ...                                                ...\n",
            "505         505  ...  \"... We propose a unified neural network archi...\n",
            "506         506  ...  \"... Natural Language Processing The subject o...\n",
            "507         507  ...  \"... Robots that interact with human face-to-f...\n",
            "508         508  ...  \"... Natural language are language spoken by h...\n",
            "509         509  ...  \"... ABSTRACT: Ambiguity can be referred a the...\n",
            "\n",
            "[510 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "-bHPVzpKFPdx",
        "outputId": "0001a07c-6508-4a60-c7c0-f63db650f783"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Abstract Data</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td></td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>The concept of maximum entropy can be traced ...</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... The concept maximum entropy traced back a...</td>\n",
              "      <td>\"... the concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... the concept of maximum entropi can be tra...</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>Scaling conditional random fields for natural...</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>\"... Scaling conditional random fields natural...</td>\n",
              "      <td>\"... scaling conditional random fields for nat...</td>\n",
              "      <td>\"... scale condit random field for natur langu...</td>\n",
              "      <td>\"... Scaling conditional random field for natu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>The paper addresses the issue of cooperation ...</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... The paper addresses issue cooperation lin...</td>\n",
              "      <td>\"... the paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... the paper address the issu of cooper betw...</td>\n",
              "      <td>\"... The paper address the issue of cooperatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>In most natural language processing applicati...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>\"... In natural language processing applicatio...</td>\n",
              "      <td>\"... in most natural language processing appli...</td>\n",
              "      <td>\"... In most natur languag process application...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>505</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>We propose a unified neural network architect...</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>\"... We propose unified neural network archite...</td>\n",
              "      <td>\"... we propose a unified neural network archi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>506</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>Natural Language Processing The subject of Na...</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>\"... Natural Language Processing The subject N...</td>\n",
              "      <td>\"... natural language processing the subject o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>507</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>Robots that interact with humans facetoface u...</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>\"... Robots interact humans face-to-face using...</td>\n",
              "      <td>\"... robots that interact with humans face-to-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"... Robots that interact with human face-to-f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>508</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>Natural languages are languages spoken by hum...</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>\"... Natural languages languages spoken humans...</td>\n",
              "      <td>\"... natural languages are languages spoken by...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"... Natural language are language spoken by h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>509</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>ABSTRACT Ambiguity can be referred as the ab...</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>\"... ABSTRACT: Ambiguity referred ability one ...</td>\n",
              "      <td>\"... abstract: ambiguity can be referred as th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"... ABSTRACT: Ambiguity can be referred a the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>510 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ...                                      Lemmitization\n",
              "0             0  ...                                          \"... ...\"\n",
              "1             1  ...  \"... The concept of maximum entropy can be tra...\n",
              "2             2  ...  \"... Scaling conditional random field for natu...\n",
              "3             3  ...  \"... The paper address the issue of cooperatio...\n",
              "4             4  ...  \"... In most natural language processing appli...\n",
              "..          ...  ...                                                ...\n",
              "505         505  ...  \"... We propose a unified neural network archi...\n",
              "506         506  ...  \"... Natural Language Processing The subject o...\n",
              "507         507  ...  \"... Robots that interact with human face-to-f...\n",
              "508         508  ...  \"... Natural language are language spoken by h...\n",
              "509         509  ...  \"... ABSTRACT: Ambiguity can be referred a the...\n",
              "\n",
              "[510 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2095uL5ZCQb"
      },
      "source": [
        "#fetching 500 rows from data frame\r\n",
        "df2=df1.iloc[1:501,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzBuXsToGiHK"
      },
      "source": [
        "#POS tagging\r\n",
        "\r\n",
        "from collections import  Counter\r\n",
        "df2['Lemmitization']= df2['Abstract Data'].apply(lambda x: TextBlob(x).words)\r\n",
        "count_dic=[]\r\n",
        "for text in df2['Lemmitization']:\r\n",
        "  td=nltk.pos_tag(text)\r\n",
        "  count_dic.append(Counter([x[1] for x in td ]))\r\n",
        "  print(count_dic)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWIHNp4ULhJW"
      },
      "source": [
        "cc_counter=[x['CC'] for x in count_dic]\r\n",
        "dt_counter=[x['DT'] for x in count_dic]\r\n",
        "md_counter=[x['MD'] for x in count_dic]\r\n",
        "in_counter=[x['IN'] for x in count_dic]\r\n",
        "jjs_counter=[x['JJS'] for x in count_dic]\r\n",
        "nnp_counter=[x['NNP'] for x in count_dic]\r\n",
        "vb_counter=[x['VB'] for x in count_dic]\r\n",
        "pos_counter=[x['POS'] for x in count_dic]\r\n",
        "prp_counter=[x['PRP$'] for  x in  count_dic]\r\n",
        "rb_counter=[x['RB'] for x in count_dic]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hz_EODbWiBsg",
        "outputId": "c80d8e97-37cc-498a-ca79-d6f0de1f0df4"
      },
      "source": [
        "df2['Coordinating Conjuctions']=cc_counter\r\n",
        "df2['Determiner']=dt_counter\r\n",
        "df2['Modal']=md_counter\r\n",
        "df2['Preposition']=in_counter\r\n",
        "df2['Adjective']=jjs_counter\r\n",
        "df2['Proper Noun']=nnp_counter\r\n",
        "df2['Verb']=vb_counter\r\n",
        "df2['Possessive Ending']=pos_counter\r\n",
        "df2['Possesive Pronoun']=prp_counter\r\n",
        "df2['Adverb']=rb_counter\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Abstract Data</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "      <th>Coordinating Conjuctions</th>\n",
              "      <th>Determiner</th>\n",
              "      <th>Modal</th>\n",
              "      <th>Preposition</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Proper Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Possessive Ending</th>\n",
              "      <th>Possesive Pronoun</th>\n",
              "      <th>Adverb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>The concept of maximum entropy can be traced ...</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... The concept maximum entropy traced back a...</td>\n",
              "      <td>\"... the concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... the concept of maximum entropi can be tra...</td>\n",
              "      <td>[The, concept, of, maximum, entropy, can, be, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>Scaling conditional random fields for natural...</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>\"... Scaling conditional random fields natural...</td>\n",
              "      <td>\"... scaling conditional random fields for nat...</td>\n",
              "      <td>\"... scale condit random field for natur langu...</td>\n",
              "      <td>[Scaling, conditional, random, fields, for, na...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>The paper addresses the issue of cooperation ...</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... The paper addresses issue cooperation lin...</td>\n",
              "      <td>\"... the paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... the paper address the issu of cooper betw...</td>\n",
              "      <td>[The, paper, addresses, the, issue, of, cooper...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>In most natural language processing applicati...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>\"... In natural language processing applicatio...</td>\n",
              "      <td>\"... in most natural language processing appli...</td>\n",
              "      <td>\"... In most natur languag process application...</td>\n",
              "      <td>[In, most, natural, language, processing, appl...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>We propose a unified neural network architect...</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>\"... We propose unified neural network archite...</td>\n",
              "      <td>\"... we propose a unified neural network archi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[We, propose, a, unified, neural, network, arc...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>496</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>Natural Language Processing The subject of Na...</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>\"... Natural Language Processing The subject N...</td>\n",
              "      <td>\"... natural language processing the subject o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, Language, Processing, The, subject, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>497</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>Robots that interact with humans facetoface u...</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>\"... Robots interact humans face-to-face using...</td>\n",
              "      <td>\"... robots that interact with humans face-to-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Robots, that, interact, with, humans, face-to...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>498</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>Natural languages are languages spoken by hum...</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>\"... Natural languages languages spoken humans...</td>\n",
              "      <td>\"... natural languages are languages spoken by...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, languages, are, languages, spoken, b...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>499</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>ABSTRACT Ambiguity can be referred as the ab...</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>\"... ABSTRACT: Ambiguity referred ability one ...</td>\n",
              "      <td>\"... abstract: ambiguity can be referred as th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ABSTRACT, Ambiguity, can, be, referred, as, t...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>500</td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td></td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ... Adverb\n",
              "1             1  ...      6\n",
              "2             2  ...      0\n",
              "3             3  ...      3\n",
              "4             4  ...      1\n",
              "5             5  ...      0\n",
              "..          ...  ...    ...\n",
              "496         496  ...      0\n",
              "497         497  ...      0\n",
              "498         498  ...      2\n",
              "499         499  ...      1\n",
              "500         500  ...      0\n",
              "\n",
              "[500 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ang2EpBvxU8n"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewE7eHxtz1rY"
      },
      "source": [
        "corpus=[]\r\n",
        "for line in df2[\"Lemmitization\"]:\r\n",
        "  string=\" \".join(line)\r\n",
        "  corpus.append(string)\r\n",
        "vec=TfidfVectorizer()\r\n",
        "x=vec.fit_transform(corpus)\r\n",
        "cNames=vec.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqzNSc389YhP",
        "outputId": "f66d163a-6ad7-43da-ccea-8b52408e651e"
      },
      "source": [
        "print(type(x))\r\n",
        "tfidf=x.toarray()\r\n",
        "print(type(tfidf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "2D9qa1Tj98Qx",
        "outputId": "e6880d2a-77d5-4667-e238-6d2f9f300099"
      },
      "source": [
        "#calculation of tf-idf values\r\n",
        "tfidf=pd.DataFrame(tfidf)\r\n",
        "tfidf.columns=cNames\r\n",
        "tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>86</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>abstract</th>\n",
              "      <th>access</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>account</th>\n",
              "      <th>achieved</th>\n",
              "      <th>acoustic</th>\n",
              "      <th>acquaintance</th>\n",
              "      <th>addresses</th>\n",
              "      <th>algorithm</th>\n",
              "      <th>all</th>\n",
              "      <th>along</th>\n",
              "      <th>ambiguity</th>\n",
              "      <th>ambiguous</th>\n",
              "      <th>amount</th>\n",
              "      <th>an</th>\n",
              "      <th>analysis</th>\n",
              "      <th>and</th>\n",
              "      <th>application</th>\n",
              "      <th>applications</th>\n",
              "      <th>applied</th>\n",
              "      <th>approach</th>\n",
              "      <th>approached</th>\n",
              "      <th>architecture</th>\n",
              "      <th>architectures</th>\n",
              "      <th>are</th>\n",
              "      <th>around</th>\n",
              "      <th>article</th>\n",
              "      <th>as</th>\n",
              "      <th>aspects</th>\n",
              "      <th>at</th>\n",
              "      <th>attractor</th>\n",
              "      <th>audience</th>\n",
              "      <th>audio</th>\n",
              "      <th>automated</th>\n",
              "      <th>avoid</th>\n",
              "      <th>back</th>\n",
              "      <th>base</th>\n",
              "      <th>...</th>\n",
              "      <th>threads</th>\n",
              "      <th>tify</th>\n",
              "      <th>times</th>\n",
              "      <th>to</th>\n",
              "      <th>tools</th>\n",
              "      <th>traced</th>\n",
              "      <th>traditional</th>\n",
              "      <th>translation</th>\n",
              "      <th>try</th>\n",
              "      <th>trying</th>\n",
              "      <th>tutorial</th>\n",
              "      <th>understand</th>\n",
              "      <th>understanding</th>\n",
              "      <th>understood</th>\n",
              "      <th>unified</th>\n",
              "      <th>unprocessed</th>\n",
              "      <th>us</th>\n",
              "      <th>use</th>\n",
              "      <th>used</th>\n",
              "      <th>using</th>\n",
              "      <th>utterances</th>\n",
              "      <th>vague</th>\n",
              "      <th>various</th>\n",
              "      <th>versatility</th>\n",
              "      <th>virtually</th>\n",
              "      <th>way</th>\n",
              "      <th>we</th>\n",
              "      <th>where</th>\n",
              "      <th>which</th>\n",
              "      <th>who</th>\n",
              "      <th>widescale</th>\n",
              "      <th>will</th>\n",
              "      <th>with</th>\n",
              "      <th>without</th>\n",
              "      <th>word</th>\n",
              "      <th>words</th>\n",
              "      <th>works</th>\n",
              "      <th>world</th>\n",
              "      <th>yet</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066943</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.201252</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.094667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.163506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.162526</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.217399</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.178017</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.217399</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.155068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190466</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.155068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137396</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.167792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.137685</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149114</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149428</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181011</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181011</td>\n",
              "      <td>0.182102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.105434</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.184634</td>\n",
              "      <td>0.152099</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147351</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.216251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174639</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175692</td>\n",
              "      <td>0.175692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138840</td>\n",
              "      <td>0.101723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174639</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.138311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133485</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068893</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.264933</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.133485</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.069038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168268</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133485</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168268</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097425</td>\n",
              "      <td>0.168268</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168268</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157743</td>\n",
              "      <td>0.158694</td>\n",
              "      <td>0.15681</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.158694</td>\n",
              "      <td>0.158694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.249859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.157743</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065110</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158694</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125890</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250814</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.124929</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 386 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      86   ability      able  abstract  ...     works     world       yet  your\n",
              "0    0.0  0.000000  0.000000   0.00000  ...  0.000000  0.162526  0.000000   0.0\n",
              "1    0.0  0.000000  0.000000   0.00000  ...  0.217399  0.000000  0.000000   0.0\n",
              "2    0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "3    0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "4    0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...   ...\n",
              "495  0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "496  0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "497  0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.168268   0.0\n",
              "498  0.0  0.157743  0.158694   0.15681  ...  0.000000  0.000000  0.000000   0.0\n",
              "499  0.0  0.000000  0.000000   0.00000  ...  0.000000  0.000000  0.000000   0.0\n",
              "\n",
              "[500 rows x 386 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pRvPjPWNS4m-",
        "outputId": "e3770a81-d219-44cb-a0bd-3d552e3c4083"
      },
      "source": [
        "#sentence length\r\n",
        "def str_len(x):\r\n",
        "  return len(x.split(\" \"))\r\n",
        "df2['sentence length']=[*map(str_len, df2['Abstract Data'])]\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Abstract Data</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "      <th>Coordinating Conjuctions</th>\n",
              "      <th>Determiner</th>\n",
              "      <th>Modal</th>\n",
              "      <th>Preposition</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Proper Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Possessive Ending</th>\n",
              "      <th>Possesive Pronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>sentence length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>The concept of maximum entropy can be traced ...</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... The concept maximum entropy traced back a...</td>\n",
              "      <td>\"... the concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... the concept of maximum entropi can be tra...</td>\n",
              "      <td>[The, concept, of, maximum, entropy, can, be, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>Scaling conditional random fields for natural...</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>\"... Scaling conditional random fields natural...</td>\n",
              "      <td>\"... scaling conditional random fields for nat...</td>\n",
              "      <td>\"... scale condit random field for natur langu...</td>\n",
              "      <td>[Scaling, conditional, random, fields, for, na...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>The paper addresses the issue of cooperation ...</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... The paper addresses issue cooperation lin...</td>\n",
              "      <td>\"... the paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... the paper address the issu of cooper betw...</td>\n",
              "      <td>[The, paper, addresses, the, issue, of, cooper...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>In most natural language processing applicati...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>\"... In natural language processing applicatio...</td>\n",
              "      <td>\"... in most natural language processing appli...</td>\n",
              "      <td>\"... In most natur languag process application...</td>\n",
              "      <td>[In, most, natural, language, processing, appl...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>We propose a unified neural network architect...</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>\"... We propose unified neural network archite...</td>\n",
              "      <td>\"... we propose a unified neural network archi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[We, propose, a, unified, neural, network, arc...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>496</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>Natural Language Processing The subject of Na...</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>\"... Natural Language Processing The subject N...</td>\n",
              "      <td>\"... natural language processing the subject o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, Language, Processing, The, subject, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>497</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>Robots that interact with humans facetoface u...</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>\"... Robots interact humans face-to-face using...</td>\n",
              "      <td>\"... robots that interact with humans face-to-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Robots, that, interact, with, humans, face-to...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>498</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>Natural languages are languages spoken by hum...</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>\"... Natural languages languages spoken humans...</td>\n",
              "      <td>\"... natural languages are languages spoken by...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, languages, are, languages, spoken, b...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>499</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>ABSTRACT Ambiguity can be referred as the ab...</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>\"... ABSTRACT: Ambiguity referred ability one ...</td>\n",
              "      <td>\"... abstract: ambiguity can be referred as th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ABSTRACT, Ambiguity, can, be, referred, as, t...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>500</td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td></td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ... sentence length\n",
              "1             1  ...              48\n",
              "2             2  ...              27\n",
              "3             3  ...              44\n",
              "4             4  ...              44\n",
              "5             5  ...              43\n",
              "..          ...  ...             ...\n",
              "496         496  ...              45\n",
              "497         497  ...              43\n",
              "498         498  ...              51\n",
              "499         499  ...              52\n",
              "500         500  ...               4\n",
              "\n",
              "[500 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jaQYZHrbVAUR",
        "outputId": "9d711d5f-ead3-4ac8-99a9-6a0e15a6313b"
      },
      "source": [
        "# Preposition Density\r\n",
        "df2['Preposition_Density']=(df2['Preposition']/df2['sentence length'])*100\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Abstract Data</th>\n",
              "      <th>Special characters and Punctutation Removal</th>\n",
              "      <th>Removal of numbers</th>\n",
              "      <th>Removal of Stop words</th>\n",
              "      <th>Lower_case</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>Lemmitization</th>\n",
              "      <th>Coordinating Conjuctions</th>\n",
              "      <th>Determiner</th>\n",
              "      <th>Modal</th>\n",
              "      <th>Preposition</th>\n",
              "      <th>Adjective</th>\n",
              "      <th>Proper Noun</th>\n",
              "      <th>Verb</th>\n",
              "      <th>Possessive Ending</th>\n",
              "      <th>Possesive Pronoun</th>\n",
              "      <th>Adverb</th>\n",
              "      <th>sentence length</th>\n",
              "      <th>Preposition_Density</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>The concept of maximum entropy can be traced ...</td>\n",
              "      <td>\"... The concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... The concept maximum entropy traced back a...</td>\n",
              "      <td>\"... the concept of maximum entropy can be tra...</td>\n",
              "      <td>\"... the concept of maximum entropi can be tra...</td>\n",
              "      <td>[The, concept, of, maximum, entropy, can, be, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>48</td>\n",
              "      <td>8.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>Scaling conditional random fields for natural...</td>\n",
              "      <td>\"... Scaling conditional random fields for nat...</td>\n",
              "      <td>\"... Scaling conditional random fields natural...</td>\n",
              "      <td>\"... scaling conditional random fields for nat...</td>\n",
              "      <td>\"... scale condit random field for natur langu...</td>\n",
              "      <td>[Scaling, conditional, random, fields, for, na...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>14.814815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>The paper addresses the issue of cooperation ...</td>\n",
              "      <td>\"... The paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... The paper addresses issue cooperation lin...</td>\n",
              "      <td>\"... the paper addresses the issue of cooperat...</td>\n",
              "      <td>\"... the paper address the issu of cooper betw...</td>\n",
              "      <td>[The, paper, addresses, the, issue, of, cooper...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>18.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>In most natural language processing applicati...</td>\n",
              "      <td>\"... In most natural language processing appli...</td>\n",
              "      <td>\"... In natural language processing applicatio...</td>\n",
              "      <td>\"... in most natural language processing appli...</td>\n",
              "      <td>\"... In most natur languag process application...</td>\n",
              "      <td>[In, most, natural, language, processing, appl...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>4.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>We propose a unified neural network architect...</td>\n",
              "      <td>\"... We propose a unified neural network archi...</td>\n",
              "      <td>\"... We propose unified neural network archite...</td>\n",
              "      <td>\"... we propose a unified neural network archi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[We, propose, a, unified, neural, network, arc...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>2.325581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>496</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>Natural Language Processing The subject of Na...</td>\n",
              "      <td>\"... Natural Language Processing The subject o...</td>\n",
              "      <td>\"... Natural Language Processing The subject N...</td>\n",
              "      <td>\"... natural language processing the subject o...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, Language, Processing, The, subject, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>13.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>497</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>Robots that interact with humans facetoface u...</td>\n",
              "      <td>\"... Robots that interact with humans face-to-...</td>\n",
              "      <td>\"... Robots interact humans face-to-face using...</td>\n",
              "      <td>\"... robots that interact with humans face-to-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Robots, that, interact, with, humans, face-to...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>9.302326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>498</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>Natural languages are languages spoken by hum...</td>\n",
              "      <td>\"... Natural languages are languages spoken by...</td>\n",
              "      <td>\"... Natural languages languages spoken humans...</td>\n",
              "      <td>\"... natural languages are languages spoken by...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Natural, languages, are, languages, spoken, b...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>13.725490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>499</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>ABSTRACT Ambiguity can be referred as the ab...</td>\n",
              "      <td>\"...  ABSTRACT: Ambiguity can be referred as t...</td>\n",
              "      <td>\"... ABSTRACT: Ambiguity referred ability one ...</td>\n",
              "      <td>\"... abstract: ambiguity can be referred as th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ABSTRACT, Ambiguity, can, be, referred, as, t...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>15.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>500</td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td></td>\n",
              "      <td>\"...   ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>\"... ...\"</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  ... Preposition_Density\n",
              "1             1  ...            8.333333\n",
              "2             2  ...           14.814815\n",
              "3             3  ...           18.181818\n",
              "4             4  ...            4.545455\n",
              "5             5  ...            2.325581\n",
              "..          ...  ...                 ...\n",
              "496         496  ...           13.333333\n",
              "497         497  ...            9.302326\n",
              "498         498  ...           13.725490\n",
              "499         499  ...           15.384615\n",
              "500         500  ...            0.000000\n",
              "\n",
              "[500 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    }
  ]
}